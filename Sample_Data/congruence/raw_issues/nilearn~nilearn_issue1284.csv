"rectype","issueid","project_owner","project_name","actor","time","text","action","title"
"issue_title","1284","nilearn","nilearn","Joaoloula","2016-09-20 08:27:12","Work needed on examples and documentation. We could also consider adding other measures of similarity (ICC, mutual information).
","start issue","[WIP] Adding Jaccard index for computing atlas similarity"
"issue_closed","1284","nilearn","nilearn","AlexandreAbraham","2016-10-31 16:04:18","","closed issue","[WIP] Adding Jaccard index for computing atlas similarity"
"pull_request_title","1284","nilearn","nilearn","Joaoloula","2016-09-20 08:27:12","Work needed on examples and documentation. We could also consider adding other measures of similarity (ICC, mutual information).
","eaa9684f12f18e2ec36e0aed1d6c07389123e8c0","[WIP] Adding Jaccard index for computing atlas similarity"
"pull_request_merged","1284","nilearn","nilearn","AlexandreAbraham","2016-10-31 16:04:18","[WIP] Adding Jaccard index for computing atlas similarity","eaa9684f12f18e2ec36e0aed1d6c07389123e8c0","Pull request merge from Joaoloula/nilearn:jaccard_index to nilearn/nilearn:master"
"issue_comment","1284","nilearn","nilearn","GaelVaroquaux","2016-09-20 08:44:51","Jaccard similarity is already in scikit-learn:
http://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_similarity_score.html

Overall, this kind of functionality, not strictly neuroimaging related, lives more in scikit-learn than nilearn.

What may be useful could be to add a function to do matching of 2 sets of maps, using sklearn.utils.linear_assignment. It should also be added to a nilearn example, for instance at the end of http://nilearn.github.io/auto_examples/03_connectivity/plot_compare_resting_state_decomposition.html, to plot the same map for both ICA and dict-learning.
","",""
"issue_comment","1284","nilearn","nilearn","GaelVaroquaux","2016-09-20 08:56:08","I won't accept this in nilearn unless it comes with clear neuroimaging
applications. Everything in nilearn should solve a brain-research
problem.

What's the problem that you are trying to solve?
","",""
"issue_comment","1284","nilearn","nilearn","GaelVaroquaux","2016-09-20 09:00:20","> ```
> What's the problem that you are trying to solve?
> ```
> 
> Having a similarity score between two sets of brain maps with
> continuous values and possible overlaps.

No, that's a formal problem. In what application setting? What the
'brain' question?
","",""
"issue_comment","1284","nilearn","nilearn","GaelVaroquaux","2016-09-20 09:21:20","You don't have to convince me it is useful, I am convinced. You have to
put it in a way that non technical users can use it, understand how and
why.

Nilearn targets people who are not computer scientists. If we cannot
demonstrate how some piece of code makes their life better, it does not
enter nilearn.
","",""
"issue_comment","1284","nilearn","nilearn","GaelVaroquaux","2016-09-20 09:40:45","Yes, but just giving them a metric won't help them solve their problem.
They need more. It's like giving people a showel when they want to build
a house.

Once again: either you can do a convincing example, or it does not go
into nilearn. Without a convincing example, it either won't be used, or
we'll keep getting questions on how to use it.

That's a clear and important rule of nilearn: anything that goes in must
produce an example that is relevant for brain imaging, even for people
who are not computer scientists.
","",""
"issue_comment","1284","nilearn","nilearn","GaelVaroquaux","2016-09-20 12:08:51","OK. Make sure that the example runs fast enough. Maybe limit yourself to
6 subjects, and make 2 different sub-groups of 5 subjects, with only 10
components.

To make an example that is easy to follow, I think that you need
something higher level than just the Jaccard index. It's too low level,
and will lead to a lot of numpy code in the example. Maybe a function
that take 2 set of maps, returns a reordered version and a list of
similarities.
","",""
"issue_comment","1284","nilearn","nilearn","GaelVaroquaux","2016-10-31 18:44:25","> Merged #1284.

Not happy
","",""
"issue_comment","1284","nilearn","nilearn","AlexandreAbraham","2016-09-20 08:54:02","> Jaccard similarity is already in scikit-learn:
> http://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_similarity_score.html

Scikit-learn implements the Jaccard Index in the specific setting of measuring clustering similarity. Here we have a fuzzy extension of the measure that will probably not be accepted in sklearn because there is no ""best"" way to make this extension. In particular, people in neuroimaging tend to use extensions that do not rely on linear assignment of the regions, see _Generalized Overlap Measures for Evaluation and
Validation in Medical Image Analysis_ (Crum 2006)
","",""
"issue_comment","1284","nilearn","nilearn","AlexandreAbraham","2016-09-20 08:58:38","> What's the problem that you are trying to solve?

Having a similarity score between two sets of brain maps with continuous values and possible overlaps.
","",""
"issue_comment","1284","nilearn","nilearn","Joaoloula","2016-09-20 09:07:08","I'd argue one of the main interests is a matter of reproducibility; we could introduce an example in which we split a dataset and build atlases from each of the splits and compare their similarity (the brain question being: are the regions we're learning serving actual stable and interesting neuroscientific purposes or are they just an artifact of the specific subset we looked at?)
","",""
"issue_comment","1284","nilearn","nilearn","AlexandreAbraham","2016-09-20 09:36:53","So! People here wants to assess the _reliability_ of methods extracting resting state networks. They define _reliability_ as the ability to extract similar brain atlases on test-retest sessions. In particular, most of them want to compare ICA to their approach. This is what I understood!
","",""
"issue_comment","1284","nilearn","nilearn","Joaoloula","2016-09-20 10:07:00","I think the following example is quite didactic: in the spirit of 8.4.7, we take ADHD and we split it in two. We then create atlases for both splits using CanICA and Dictionary Learning. Finally, we compute two Jaccard indexes: one between the CanICA atlases and one between the DictLearning atlases, to compare replicability between methods. The narrative would explain that we want our methods to be not only valid, but also replicable, and would give an intuitive explanation of the index.
","",""
"issue_comment","1284","nilearn","nilearn","Joaoloula","2016-09-20 11:53:28","A rough idea of the example: http://bit.ly/2cVX1Hh
","",""
"issue_comment","1284","nilearn","nilearn","Joaoloula","2016-09-20 14:17:25","Simplified example (with 10 subjects because the atlas using 6 subjects were just too ugly): http://bit.ly/2cVX1Hh. I think it's reasonably high-level and light on numpy code (most of the example is just _getting_ the atlases, which is quite a standard analysis, and computing the jaccard indexes is done in one line).

I really like the idea of comparing two atlases map by map and trying to find a one-to-one correspondence, and I believe a lot of people would be interested in it. However, it seems to me like a significantly different question than just measuring similarity between atlases, shouldn't we leave it to another PR?
","",""
"issue_comment","1284","nilearn","nilearn","Joaoloula","2016-11-01 11:33:51","Sorry about that, totally my fault: I got mixed up with the working branch for the other PR and it ended up pushing this one, since the changes had been reverted I didn't think it would change anything. I didn't know this would get automatically merged as well. I'll create new, clean PRs.
","",""
"pull_request_commit","1284","nilearn","nilearn","Joaoloula","2016-09-20 08:26:29","added jaccard index and jaccard distance functions to nilearn.connectome","eaa9684f12f18e2ec36e0aed1d6c07389123e8c0",""
