"rectype","issueid","project_owner","project_name","actor","time","text","action","title"
"issue_title","1372","nilearn","nilearn","darya-chyzhyk","2017-01-31 10:59:09","Fix #1319 

Substitute the previous version of the COBRE dataset by lightweight version of the COBRE, with simplified pre-processing and with comprehensive list of confounds (the format was changed from .mat to .tsv ). The structure of the data is quite similar to the previous one. The list of input parameters in `fetch_cobre` are kept without changes, in the outputs I added the description of the phenotypic and confounds variables and README.md file.

The confounds and the phynotypic data are stored as compressed file. To uncompressed them during the downloading I used `nilearn.datasets.utils._uncompress_file`. If we use an option `delete_archive = True` to delete the compressed file, in case of gz extension both files compressed and uncompressed were eliminated. To solve that, I moved the deleting of the file `os.remove(file_)` to each case of type of compressed extension.","start issue","[MRG+2] Fetch cobre"
"issue_closed","1372","nilearn","nilearn","GaelVaroquaux","2017-02-22 16:54:01","","closed issue","[MRG+2] Fetch cobre"
"pull_request_title","1372","nilearn","nilearn","darya-chyzhyk","2017-01-31 10:59:09","Fix #1319 

Substitute the previous version of the COBRE dataset by lightweight version of the COBRE, with simplified pre-processing and with comprehensive list of confounds (the format was changed from .mat to .tsv ). The structure of the data is quite similar to the previous one. The list of input parameters in `fetch_cobre` are kept without changes, in the outputs I added the description of the phenotypic and confounds variables and README.md file.

The confounds and the phynotypic data are stored as compressed file. To uncompressed them during the downloading I used `nilearn.datasets.utils._uncompress_file`. If we use an option `delete_archive = True` to delete the compressed file, in case of gz extension both files compressed and uncompressed were eliminated. To solve that, I moved the deleting of the file `os.remove(file_)` to each case of type of compressed extension.","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","[MRG+2] Fetch cobre"
"pull_request_merged","1372","nilearn","nilearn","GaelVaroquaux","2017-02-22 16:54:00","[MRG+2] Fetch cobre","e5ecaa0af59bd3f5720e0d4dc97a14e638e2b63e","Pull request merge from darya-chyzhyk/nilearn:fetch_cobre to nilearn/nilearn:master"
"issue_comment","1372","nilearn","nilearn","darya-chyzhyk","2017-02-22 14:40:44","@AlexandreAbraham could you please have a look here","",""
"issue_comment","1372","nilearn","nilearn","darya-chyzhyk","2017-02-22 20:08:25","whoopee! 
Thanks everybody! ","",""
"issue_comment","1372","nilearn","nilearn","codecov[bot]","2017-02-08 14:03:22","# [Codecov](https://codecov.io/gh/nilearn/nilearn/pull/1372?src=pr&el=h1) Report
> Merging [#1372](https://codecov.io/gh/nilearn/nilearn/pull/1372?src=pr&el=desc) into [master](https://codecov.io/gh/nilearn/nilearn/commit/5b70bd456226082b9b9bb4501e603bbcfb402e98?src=pr&el=desc) will **increase** coverage by `0.01%`.
> The diff coverage is `98.66%`.


```diff
@@            Coverage Diff            @@
##           master   #1372      +/-   ##
=========================================
+ Coverage   94.29%   94.3%   +0.01%     
=========================================
  Files         116     116              
  Lines       12870   12896      +26     
=========================================
+ Hits        12136   12162      +26     
  Misses        734     734
```


| [Impacted Files](https://codecov.io/gh/nilearn/nilearn/pull/1372?src=pr&el=tree) | Coverage Δ | |
|---|---|---|
| [nilearn/datasets/tests/test_func.py](https://codecov.io/gh/nilearn/nilearn/compare/5b70bd456226082b9b9bb4501e603bbcfb402e98...86ad951bc9545dee140f6da856492f3ca58c21af?src=pr&el=tree#diff-bmlsZWFybi9kYXRhc2V0cy90ZXN0cy90ZXN0X2Z1bmMucHk=) | `100% <100%> (ø)` | :white_check_mark: |
| [nilearn/datasets/tests/test_utils.py](https://codecov.io/gh/nilearn/nilearn/compare/5b70bd456226082b9b9bb4501e603bbcfb402e98...86ad951bc9545dee140f6da856492f3ca58c21af?src=pr&el=tree#diff-bmlsZWFybi9kYXRhc2V0cy90ZXN0cy90ZXN0X3V0aWxzLnB5) | `100% <100%> (ø)` | :white_check_mark: |
| [nilearn/datasets/utils.py](https://codecov.io/gh/nilearn/nilearn/compare/5b70bd456226082b9b9bb4501e603bbcfb402e98...86ad951bc9545dee140f6da856492f3ca58c21af?src=pr&el=tree#diff-bmlsZWFybi9kYXRhc2V0cy91dGlscy5weQ==) | `80.11% <100%> (+0.11%)` | :white_check_mark: |
| [nilearn/datasets/func.py](https://codecov.io/gh/nilearn/nilearn/compare/5b70bd456226082b9b9bb4501e603bbcfb402e98...86ad951bc9545dee140f6da856492f3ca58c21af?src=pr&el=tree#diff-bmlsZWFybi9kYXRhc2V0cy9mdW5jLnB5) | `89.13% <96.42%> (+0.21%)` | :white_check_mark: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/nilearn/nilearn/pull/1372?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/nilearn/nilearn/pull/1372?src=pr&el=footer). Last update [5b70bd4...f3f1c5c](https://codecov.io/gh/nilearn/nilearn/compare/5b70bd456226082b9b9bb4501e603bbcfb402e98...f3f1c5cacacbfce4c0a2f620271eff14252a52b4?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).","",""
"pull_request_commit_comment","1372","nilearn","nilearn","GaelVaroquaux","2017-02-08 18:09:02","We need to strive not to have commented lines of code: people in the future will wonder what they are for.

We should either create a data description for the test and uncomment this line, or give up on this test and fully remove this line.","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(None, '', u'nilearn/datasets/tests/test_func.py')"
"pull_request_commit_comment","1372","nilearn","nilearn","GaelVaroquaux","2017-02-08 18:09:33","You should delete these lines.","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(None, '', u'nilearn/datasets/utils.py')"
"pull_request_commit_comment","1372","nilearn","nilearn","KamalakerDadi","2017-02-15 14:57:41","quote missing on right side in name fmri_XXXXXXX.tsv.","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(None, '', u'nilearn/datasets/func.py')"
"pull_request_commit_comment","1372","nilearn","nilearn","KamalakerDadi","2017-02-15 14:58:05","Space missing before ""It""","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(None, '', u'nilearn/datasets/func.py')"
"pull_request_commit_comment","1372","nilearn","nilearn","KamalakerDadi","2017-02-15 14:59:45","It think it should be .. versionadded:: 0.3

Ref: http://www.sphinx-doc.org/en/stable/markup/para.html","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(None, '', u'nilearn/datasets/func.py')"
"pull_request_commit_comment","1372","nilearn","nilearn","KamalakerDadi","2017-02-15 15:10:56","minor comment: can you change to ""current"" ?","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(None, '', u'nilearn/datasets/tests/test_func.py')"
"pull_request_commit_comment","1372","nilearn","nilearn","KamalakerDadi","2017-02-15 15:12:05","Can you correct commented line ? Create the tsv file","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(None, '', u'nilearn/datasets/tests/test_func.py')"
"pull_request_commit_comment","1372","nilearn","nilearn","KamalakerDadi","2017-02-15 15:31:44","If I am not wrong. Sphinx is failing at this line.","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(None, '', u'nilearn/datasets/func.py')"
"pull_request_commit_comment","1372","nilearn","nilearn","KamalakerDadi","2017-02-20 21:57:59","This link is not from current light weight one. Light weight one is versioned as 0_17. The one which is commented below.","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(None, '', u'nilearn/datasets/func.py')"
"pull_request_commit_comment","1372","nilearn","nilearn","AlexandreAbraham","2017-02-22 15:15:21","This type conversion should serve a purpose. Could you check that the resulting CSV table has all proper types?","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(118, '', u'nilearn/datasets/func.py')"
"pull_request_commit_comment","1372","nilearn","nilearn","darya-chyzhyk","2017-02-22 16:26:18","Its not a CSV table anymore, its TSV ;)
In case of this type conversion I had an encode problem in Pyhhon 3
Looks like it identifies correctly the type

```
rec.array([(40061, 18, 'Male', 'Right', 'Control', 'None', 133, 0.25512, 0.22657),
 (40090, 18, 'Female', 'Right', 'Control', 'None', 150, 0.16963, 0.16963),
 (40046, 18, 'Male', 'Left', 'Patient', '295.70 depressed type', 76, 0.37504, 0.30042)], 
 dtype=[('id', '<i8'), ('current_age', '<i8'), ('gender', 'S6'), ('handedness', 'S5'), ('subject_type', 'S7'), ('diagnosis', 'S21'), ('frames_ok', '<i8'), ('fd', '<f8'), ('fd_scrubbed', '<f8')])
```
","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(118, '', u'nilearn/datasets/func.py')"
"pull_request_commit_comment","1372","nilearn","nilearn","AlexandreAbraham","2017-02-22 16:27:36","Cool, OK for me.","f3f1c5cacacbfce4c0a2f620271eff14252a52b4","(118, '', u'nilearn/datasets/func.py')"
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-01-27 16:05:31","change the fetch_cobre according new version of the data","7577251e55427a13a70f199d04078b28bf1a88dc",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-01-27 16:11:06","In _uncompress_file in case of delete = True, for the .gz extention the gz. and uncompressed file are deleted, add the delete of the compressed file in each option of the extension","54d8ff4adfec844e60b309e286e4782012a7bdf7",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-01-30 16:57:29","add the description of the function","5cc9273910de51b4c2c82dcd699396b4add25ea1",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-01-30 16:58:54","add the commented previous function","12a70440fd6b498ef181e44261bb6eab50db3472",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-01-31 10:06:51","clean the code","94f8eb96bb9f50ae896a24e4b03b06d0c6973325",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-07 14:39:05","rewrite the test for _uncompress_file function","5504673efd8f8163fe623689ff2de1c87498ce38",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-07 14:40:20","rewrite the test test_fetch_cobre for new release of cobre data","c839241ab83cc926c60db2673b2789afba26ad00",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-08 14:03:02","change the way to write tsv file with np.savetxt without header in the parameters","d7570d6612fa8582ef0b4c9b8d4526a406ced243",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-08 15:07:02","change tsv saving for different versions of Python","d992dce1331e7bab7881e832d7aa664a286afb23",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-08 15:51:31","create an empty gz file instead of compress the tsv","ecf57515306fae658b3809acf152347a269299ce",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-10 22:03:11","clean the code","99f953a61025185f6b9db704a9488d9099cf5aa8",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-10 22:13:00","make the format for the byte strings satisfying for Python3 and Python2","554e1baeec2ceff168f76b6be0a6ae46b6ee6e3a",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-10 22:16:34","make compressed file for tsv","bdcd2a02e51cdfd376f4d9590d9a7bf521820063",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-14 22:53:29","Fixed the bug, empty zip file uncompressed to the sub-folder instead of to the folder","a7367ad2cd61c886c337ee15b219d5c6d651bdf8",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-14 22:56:32","Update the description from the readme","aa9a64e0230e401c48d341d407595f6a897dc2e7",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-14 23:00:55","delete the readme as output","3beeec49d5963e7e85b34850f02a0d68cc00e6e8",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-14 23:06:59","fix conflicts","7582e2dbdd9b5d2a0ccf1913d7c5f12651e493a8",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-20 08:28:16","clean code","6d72f1b772bd74015570962dd264f693b4c37e65",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-20 15:04:57","clean the code","797cdd5111b1f6d966a1951f042efcc1f6f22e7d",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-21 14:19:44","clean the code","88989d706d4053514a3ecd263fa623b8ec6c7a9c",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-21 14:32:14","redo encoding","e3923474930f14dd3bdd7c820fad333ce7b58313",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-21 16:23:33","encoding","86ad951bc9545dee140f6da856492f3ca58c21af",""
"pull_request_commit","1372","nilearn","nilearn","darya-chyzhyk","2017-02-22 09:11:19","update the description","f3f1c5cacacbfce4c0a2f620271eff14252a52b4",""
