"rectype","issueid","project_owner","project_name","actor","time","text","action","title"
"issue_title","237","pycqa","astroid","pylint-bot","2015-10-30 17:06:28","Originally reported by: **BitBucket: [ceridwenv](http://bitbucket.org/ceridwenv), GitHub: @ceridwen**

---

For debugging purposes, along with the structured exceptions #189, I'd like to add information to Uninferable objects, but this could increase the memory demands of inference too much.  I need to check to see how much memory the added information consumes.

---
- Bitbucket: https://bitbucket.org/logilab/astroid/issue/237
","start issue","Memory implications of adding information to Uninferable"
"issue_comment","237","pycqa","astroid","pylint-bot","2015-11-01 15:11:06","_Original comment by_ **BitBucket: [ceridwenv](http://bitbucket.org/ceridwenv), GitHub: @ceridwen**:

---

What would be a good benchmark for determining memory consumption of inference?  I was thinking about it and realized that astroid probably uses less memory when testing inference than pylint does when using it, so the test should probably be done with pylint.  That said, what would be the right thing to run pylint on?  Do you have any memory-intensive code bases you regularly use pylint on?  My current plan involves the use of the Python 3.4 stdlib [tracemalloc](https://docs.python.org/3.4/library/tracemalloc.html) and maybe trying to count how many times inference objects are created.  I also think I'm going to split this out from the structured_exceptions patch.
","",""
"issue_comment","237","pycqa","astroid","pylint-bot","2015-11-02 06:27:41","_Original comment by_ **Claudiu Popa (BitBucket: [PCManticore](http://bitbucket.org/PCManticore), GitHub: @PCManticore)**:

---

Indeed, testing this with astroid might not provide enough statistical value in order to draw a relevant conclusion. I usually run pylint over https://github.com/openstack/nova, since it's quite big and a minimal difference in speed can be easily noticed with its magnitude. 
","",""
