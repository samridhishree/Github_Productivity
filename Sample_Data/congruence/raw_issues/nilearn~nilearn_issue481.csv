"rectype","issueid","project_owner","project_name","actor","time","text","action","title"
"issue_title","481","nilearn","nilearn","GaelVaroquaux","2015-03-04 00:50:08","Experience from the Leipzig teaching: we need smaller / faster download.

Specifically, 
1. we should have a zip file for fetch_adhd(n_subjects=1) to download only one subject.
2. fetch_habxy(n_subjects=1) should download as little as posible: no stimuli and only 1 subject.
","start issue","Faster / smaller datasets to download"
"issue_closed","481","nilearn","nilearn","AlexandreAbraham","2016-06-07 22:01:30","","closed issue","Faster / smaller datasets to download"
"issue_comment","481","nilearn","nilearn","AlexandreAbraham","2015-03-04 01:14:03","> we should have a zip file for fetch_adhd(n_subjects=1) to download only one subject.

What is doable:
- split every subject in a single file
- create another fetcher for ADHD versions preprocessed by the neurobureau. They have 200 subjects and provide directly timeseries for given atlases

> fetch_habxy(n_subjects=1) should download as little as posible: no stimuli and only 1 subject.

This is what `fetch_haxby` does by default. Some masks are also downloaded by default but they are very small.
","",""
"issue_comment","481","nilearn","nilearn","bcipolli","2015-03-04 01:36:48","Two of the examples use only one subject from the haxby dataset, but don't pass the `n_subjects=1` argument.  Seems this would be an issue as well.
","",""
"issue_comment","481","nilearn","nilearn","GaelVaroquaux","2015-03-04 06:58:50","> Two of the examples use only one subject from the haxby dataset, but
> don't pass the n_subjects=1 argument. Seems this would be an issue as
> well.

I think that n_subjects=1 should be the default, based on my experience
teaching yesterday.
","",""
"issue_comment","481","nilearn","nilearn","AlexandreAbraham","2015-04-15 12:32:46","Shouldn't we postpone this one? We'll have no time to solve it before the release.
","",""
"issue_comment","481","nilearn","nilearn","GaelVaroquaux","2015-04-15 14:51:21","> Shouldn't we postpone this one?

Yes, :-$
","",""
"issue_comment","481","nilearn","nilearn","AlexandreAbraham","2015-04-15 15:16:31","FYI, dividing ADHD in smaller chunks should not be difficult, I can at least do this one if you want.
","",""
"issue_comment","481","nilearn","nilearn","GaelVaroquaux","2015-06-09 18:12:28","This issue is important. It is screwing up our workshop again. 
","",""
"issue_comment","481","nilearn","nilearn","AlexandreAbraham","2015-06-09 19:33:40","can you be more specific ? which dataset is problematic ?
","",""
"issue_comment","481","nilearn","nilearn","GaelVaroquaux","2015-06-09 19:39:49","Haxby. We need to break it up in subjects and download only 1 subjects by default. 

Sent from my phone. Please forgive brevity and mis spelling

On Jun 9, 2015, 12:33, at 12:33, Alexandre Abraham notifications@github.com wrote:

> can you be more specific ? which dataset is problematic ?
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/nilearn/nilearn/issues/481#issuecomment-110478512
","",""
"issue_comment","481","nilearn","nilearn","AlexandreAbraham","2015-06-09 19:41:36","This is already the case.
","",""
"issue_comment","481","nilearn","nilearn","AlexandreAbraham","2015-06-09 19:45:21","If you need one dataset to be broken into pieces or moved to nitrc, I can hack it right now. But I have to be sure to target the right ones ;).
","",""
"issue_comment","481","nilearn","nilearn","GaelVaroquaux","2015-06-09 20:51:47","Don't hack anything. It not that urgent. For it to be beneficial we need to release. It won't happen today. 

Sent from my phone. Please forgive brevity and mis spelling

On Jun 9, 2015, 12:45, at 12:45, Alexandre Abraham notifications@github.com wrote:

> If you need one dataset to be broken into pieces or moved to nitrc, I
> can hack it right now. But I have to be sure to target the right ones
> ;).
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/nilearn/nilearn/issues/481#issuecomment-110482987
","",""
"issue_comment","481","nilearn","nilearn","AlexandreAbraham","2015-06-12 07:53:05","See #605. If the mirroring system proposed in this PR is validated, I will be able to move on this issue.
","",""
"issue_comment","481","nilearn","nilearn","AlexandreAbraham","2015-06-12 08:55:07","See also #608. Used with the NiftiSpheresMasker, you get an atlas at no cost.
","",""
"issue_comment","481","nilearn","nilearn","banilo","2015-06-12 09:02:10","+1 for increasing scalability of dataset fetchers
","",""
"issue_comment","481","nilearn","nilearn","AlexandreAbraham","2015-07-06 17:07:44","#637 make ADHD downloadable subject by subject.
","",""
"issue_comment","481","nilearn","nilearn","AlexandreAbraham","2016-06-07 22:01:30","We broke down the datasets to the smallest chunks possible. I close this issue. Please re-open one for a specific dataset when you encounter a similar problem.
","",""
