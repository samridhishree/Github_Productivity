"rectype","issueid","project_owner","project_name","actor","time","text","action","title"
"issue_title","219","nilearn","nilearn","dohmatob","2014-06-27 09:16:58","- Supports TV-l1 and S-LASSO priors
- Supports logistic and squared losses
- Has cross validation
- Can automatically select alpha by CV (+ automatic computation of useful alpha ranges for the CV)
- Warning: User must supply l1_ratio
","start issue","(WIP) Sparse models: S-LASSO and TV-l1"
"issue_closed","219","nilearn","nilearn","dohmatob","2015-07-14 07:45:53","","closed issue","(WIP) Sparse models: S-LASSO and TV-l1"
"pull_request_title","219","nilearn","nilearn","dohmatob","2014-06-27 09:16:58","- Supports TV-l1 and S-LASSO priors
- Supports logistic and squared losses
- Has cross validation
- Can automatically select alpha by CV (+ automatic computation of useful alpha ranges for the CV)
- Warning: User must supply l1_ratio
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(WIP) Sparse models: S-LASSO and TV-l1"
"issue_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:22:16","prox_tv_l1.py should be merged with operators.py
","",""
"issue_comment","219","nilearn","nilearn","agramfort","2014-06-27 14:14:58","no example with TV L1?
","",""
"issue_comment","219","nilearn","nilearn","agramfort","2014-06-27 14:51:22","tests pass in 5s on my box + 88% coverage. Good job.

just a note on the smooth lasso example I am not really able to see something on the score graph.

that's it for me now
","",""
"issue_comment","219","nilearn","nilearn","agramfort","2014-06-27 16:11:54","how about poldrack's data?
","",""
"issue_comment","219","nilearn","nilearn","agramfort","2014-06-27 17:00:27","can you share with me your script on PMG?
","",""
"issue_comment","219","nilearn","nilearn","agramfort","2014-06-28 13:19:31","pretty nice !

the selected alpha seems a bit weird to me. I thought that Gael wanted to
select one alpha per fold and average the weights rather than refit?
","",""
"issue_comment","219","nilearn","nilearn","agramfort","2014-06-28 18:34:34","ok let me know when I shall review again.
","",""
"issue_comment","219","nilearn","nilearn","agramfort","2014-07-03 09:53:48","@dohmatob let us now when you're done addressing all our comments
","",""
"issue_comment","219","nilearn","nilearn","agramfort","2014-07-10 08:24:26","we'll discuss tomorrow at NS
","",""
"issue_comment","219","nilearn","nilearn","agramfort","2014-09-03 10:35:06","class SpatialNet(object):

:)

besides I love the idea.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-06-30 16:04:14","As a general comment, I think that the sparse_model folder should be a sub-folder of the 'decoding' folder.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 09:12:07","With insight, and to have a path for forward migration, I suggest that we have one unique object for SmoothLasso and TV-l1. The benefit is that it would reduce the number of lines of code now, and even more in the future when we add more spatial penalties. We could call it 'SpatialNet', to be a bit tongue in cheak :).

So, the prototype of this class would be something like:

<pre>
class SpatialNet:
   def __init__(self, penalty='smooth-lasso', mask=None, ...(usual stuff)):
       ...
  def fit(self, niimgs):
     # Note that here fit takes niimgs, so you will have to use a NiftiMasker to learn a mask
     # see the MultiPCA code in nilearn. As in the MultiPCA code, mask given in fit can be a niimg or a NiftiMasker, if None, a NiftiMasker is used.
</pre>
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 09:13:48","The above change actually requires bit of work. I suggest to plan work like this:
- First address comments above on current code base
- Second, merge the TVL1 and SmoothLasso object with still fit taking arrays (always keep you testcase passing)
- Last, adapt the object to accept niimgs
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 09:46:11","The tests are failing on my box and on travis. You should fix them before doing any changes.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:34:11","By the way, with regards to the name: 'SpatialNet', or 'SpaceNet'? :)
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 14:47:44","@dohmatob : we have made a lot of comments. Before moving to the refactoring, could you please ping me when you are done with them. That way we can discuss the refactoring based on the new code.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-04 15:23:44","Yes, I still have tests failing. You need to get rid of the backports for the tests to pass, as these backports are incompatible with the current version of scikit-learn.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-08 09:01:27","> Ping @GaelVaroquaux I hope your tests are passing now.

Not quite yet, but I fixed that, and sent you a PR:
https://github.com/dohmatob/nilearn/pull/1

Could you please merge that in your branch.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-10 09:12:25","Not the fista optimizer, which is fairly generic. But I guess that the rest can probably be in the same file. Let's see how it goes, and we can judge based on the result.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-10 09:16:49","Note that, based on earlier comments, you can still remove some code from the corresponding files. For instance the plotting code should (for now) be in the example, not in the codebase, and the *CV object definitions should simply be removed.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 17:06:58","Still a lot of minor details to be changed (I had already mentioned that I didn't want to support mask=None).

Once you are done with all the changes (please go over your code fully to make sure that you haven't left anything and that the code looks clean), could you paste an image of the output of your example with the corresponding run-time.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 17:15:09","As a general remark, you are skipping a lot of tests. This is not good.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 21:11:17","I just ran the haxby example. Good job. It runs out of the box, in a pretty decent run time. And the results look good. For future reference, on my laptop, using a single core:
- TV-l1 takes 1564s = 26min to run
- Smooth lasso takes 249s = 4mn to run

The maps look very, very similar (but they are actually different):

TV-1:
![space_net_haxby](https://cloud.githubusercontent.com/assets/208217/4620210/64aa158a-531d-11e4-880e-87c2287178bf.png)

Smooth-lasso:
![space_net_haxby_smooth_lasso](https://cloud.githubusercontent.com/assets/208217/4620214/7098553c-531d-11e4-8ccf-13120b81cafb.png)
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 21:33:21","Something that remains to be done is to have two classes, one SpaceNetClassifier and the other SpaceNetRegressor. I'd also like the option to use the squared loss to be available in the classification settings.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 05:31:50","The fact that smooth-lasso and TV-l1 looked exactly similar felt very fishy to me. It took me a long time to realize what was wrong.

The problem is that you are using the mask_vt, which is a tiny mask. Thus, once you apply feature screening, there is very little left in the mask.

**First action point**

In general, we feel that SpaceNet is best designed for full brain use. Thus you need to change the definition of the masker in the example to:

<pre>
nifti_masker = NiftiMasker(mask_strategy='epi', standardize=True)
</pre>

Once this is done, the results make more sens (I'll post figures when computations are finished).

**Second action point**

Because users will fall in this trap, we need to express the percentile of voxels kept as a volumetric percentile with regards to a full brain. For this:
- I would like you to compute the volume in a standard brain (/usr/share/fsl/data/standard/MNI152_T1_1mm_brain_mask.nii.gz)
- When you are given a mask, in space_net, you should compute it's volume, and use the ratio of the two to adjust the screening percentile.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 11:57:20","I don't see the change in EarlyStopping. Did you forget to add it?
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-06-28 16:34:35","The legend about the selected alpha is just for information. The grand final weights map is still an average of the maps corresponding to the best models per fold, as intended. So we are fine. Good remark though.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-06-30 09:39:41","OK, I'll have a look after launch.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-06-30 16:04:03","@agramfort  You may resume reviewing.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-06-30 16:04:30","Ok.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-07-10 07:30:02","Hurray! ""TV-l1"" deconvolution using an appropriate primal-dual (re-)formulation. In a sense, this is the 'optimal' scheme (the 'book' scheme) for this long lasting problem... Ping @agramfort. The code will follow soon.

Remark: Smooth lasso is (it seems to me) the ultimate Occam's razor for the ""structure"" + ""sparsity"" prior.

![tvl1_weights](https://cloud.githubusercontent.com/assets/634068/3535480/908e2e64-0801-11e4-8f45-6d69ca31f135.png)
![tvl1_folds](https://cloud.githubusercontent.com/assets/634068/3535483/998d4126-0801-11e4-94d3-5795bb8e6de6.png)
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-09-03 16:45:40","OK, I'll look at the rest of the comments tomorrow. Keeping you posted.

On Wed, Sep 3, 2014 at 4:47 PM, Gael Varoquaux notifications@github.com
wrote:

> @dohmatob https://github.com/dohmatob : we have made a lot of comments.
> Before moving to the refactoring, could you please ping me when you are
> done with them. That way we can discuss the refactoring based on the new
> code.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219#issuecomment-54308254.

## 

DED
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-09-04 08:47:08","ping @GaelVaroquaux: Travis tells me all is well. Do you still have tests falling on your box ?
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-09-04 16:20:29","OK, I'll cleanup the backports tomorrow morning.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-09-05 12:01:16","On old sklearn versions, LinearClassifierMixin is absent. I've copied the ""predict_proba"" function.  I still need ""decision_function"" and ""predict"" APIs.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-09-08 08:22:56","OK, the only back-ports retained are for `center_data` and `LabelBinarizer`. Ping @GaelVaroquaux I hope your tests are passing now.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-09-08 09:28:31","OK. Merged. Thanks.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-09-10 09:10:39","In the end, you want solvers + estimators (and the like) to be in the same source file ? I was thinking of merging all solvers into the same source file (for example space_net_solvers.py) and all estimators (and the like) into the same source file space_net.py
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-09-10 09:15:18","Yes of course fista remains in fista.py. OK, i'm merging.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-10-18 17:10:35","# Baseline QA (in case things get broken in future)

Latest dev:

```
    # Univariate feature screening. Note that if we have only as few as 1000
    # features in the mask's support, then we should to use all of them to
    # learn the model i.e disable this screening)
    do_screening = screening_percentile < 100. and mask.sum() > 1000.
    if do_screening:
        # smooth the data before screening
        sX = np.empty(list(mask.shape) + [n_samples])
        for row in xrange(n_samples):
            sX[:, :, :, row] = _unmask(X[row], mask)
        sX = ndimage.gaussian_filter(sX, (2., 2., 2., 0.))
        sX = sX[mask].T

        # do feature screening proper
        selector = SelectPercentile(f_classif if is_classif else f_regression,
                                    percentile=screening_percentile).fit(sX, y)
        support = selector.get_support()

        # erode and then dilate mask, thus obtaining superset of the mask on
        # on which a spatial prior makes sense
        nice_mask = mask.copy()
        nice_mask[mask] = (support > 0)
        nice_mask = ndimage.binary_dilation(ndimage.binary_erosion(nice_mask),
                                           iterations=2).astype(np.bool)
        nice_mask[np.logical_not(mask)] = 0
        support = nice_mask[mask]
        X = X[:, support]
        mask = nice_mask

    # crop the mask to have a tighter bounding box
    mask = _crop_mask(mask)
```

Also, `standardize=True` in NiftiMasker.

`$ ipython plot_haxby_space_net.py --pdb`

![tvl1_faces_vs_houses](https://cloud.githubusercontent.com/assets/634068/4692895/a79bfd7a-577f-11e4-9b2e-4de3d8fad6d4.png)

Time elapsed                  : 169.082 seconds.
Number of train samples : 180
Number of test samples  : 36
Classification accuracy: 100%

commit hash 2a7ba74a1553e4cba5f47309147b4bed504953b4
","",""
"issue_comment","219","nilearn","nilearn","bthirion","2014-10-18 17:26:14","Great ! thanks
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-10-21 07:47:42","@GaelVaroquaux About the volume of the mask (for correcting the user-supplied percentile), you confirm we're talking about the volume (in mm^3) occupied by the voxels in the support of this mask. Right ?
","",""
"issue_comment","219","nilearn","nilearn","bthirion","2014-10-21 07:50:20","cm^3 is probably easier to figure out.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-10-21 08:28:52","I'm computing the volumes in same units; the ratio is thus a unitless constant, and its value doesn't depend on the units used for the volumes. However, When I report the volumes (in a warning, a verbose print, etc.), I report them in mm^3 and cm^3 (as you suggest). I'm creating a fresh issue so we can track this volume discussion.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-21 18:09:00","Correct, but I agree with @bthirion that cm^3 would be better.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-10-21 18:27:37","OK. I'm moving the discussion here issue #265.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-23 15:10:54","A few comments on the Haxby example:
- Could you remove the contour of the mask on the image (and remove the 'slicer' variable)
- Use 'cut_coords=(20, -34, -16)'
- Do a for loop demoing boths penalties: smooth-lasso and TV-l1

As mentionned before:
- in SpaceNet: scores_ -> cv_scores_
- cv should be 8 by default
- scores should always be bigger is better.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-10-28 13:13:28","Oasis vbm (regression + SL penalty)

![vbm_sl](https://cloud.githubusercontent.com/assets/634068/4808721/0cc606ca-5ea4-11e4-9dee-6fa49633fe18.png)
![vbm_sl_curves](https://cloud.githubusercontent.com/assets/634068/4808726/148b921c-5ea4-11e4-8a32-28c392232eac.png)
","",""
"issue_comment","219","nilearn","nilearn","eickenberg","2014-10-28 14:00:27","Just logging: It may be interesting to order subjects by age before the plot, because the estimate seems slightly compressive around the extreme values, which can be due to the penalization.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-10-28 14:06:55","On Tue, Oct 28, 2014 at 3:00 PM, eickenberg notifications@github.com
wrote:

> Just logging: It may be interesting to order subjects by age before the
> plot, because the estimate seems slightly compressive around the extreme
> values, which can be due to the penalization.
> 
> Yes definitely should reorder. Thanks for the remark. Will apply asap.
> —
> 
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219#issuecomment-60759151.

## 

DED
","",""
"issue_comment","219","nilearn","nilearn","eickenberg","2014-12-02 13:59:11","Would it be prudent to return the union of the feature masks obtained by SelectPercentile in the folds? That way one could at least know which zeros are due to screening, and which are due to the algorithm. Right now the only way to tell this on stat maps is that the ""zeros"" coming from the algorithm are not exactly 0, so are not masked away.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2014-12-02 19:11:20","On Tue, Dec 2, 2014 at 2:59 PM, eickenberg notifications@github.com wrote:

> Would it be prudent to return the union of the feature masks obtained by
> SelectPercentile in the folds? That way one could at least know which zeros
> are due to screening, and which are due to the algorithm. Right now the
> only way to tell this on stat maps is that the ""zeros"" coming from the
> algorithm are not exactly 0, so are not masked away.
> 
> On a debug branch, yes. But in prod, I don't think so. Lest the front-end
> users may (and will) try to do funky things with this feature.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219#issuecomment-65233921.

## 

DED
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2014-12-02 19:13:47","> On a debug branch, yes. But in prod, I don't think so. Lest the front-end
> users may (and will) try to do funky things with this feature.

Agreed.
","",""
"issue_comment","219","nilearn","nilearn","eickenberg","2014-12-02 19:44:12","yes, for debugging. it wouldnt be available in prod. thing is that it needs
some rearranging of outputs, which is structural

On Tuesday, December 2, 2014, Gael Varoquaux notifications@github.com
wrote:

> > On a debug branch, yes. But in prod, I don't think so. Lest the front-end
> > users may (and will) try to do funky things with this feature.
> 
> Agreed.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219#issuecomment-65286483.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2015-01-26 13:06:50","Hi,

Discusion:
Is there any plan to merge this PR anytime soon ? Concerning the doc (sphinx), there're probably one or two things to add, but otherwise the code is mergeable and usable rightaway.
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2015-01-26 13:08:49","> Is there any plan to merge this PR anytime soon ? Concerning the doc
> (sphinx), there're probably one or two things to add, but otherwise the
> code is mergeable and usable rightaway.

Can you paste the figure output by the examples so that we have a look?

Also, indeed there is some work to be done on the docs.
","",""
"issue_comment","219","nilearn","nilearn","AlexandreAbraham","2015-06-03 11:27:41","@dohmatob @schwarty @GaelVaroquaux What is the status on this PR?
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2015-06-03 13:57:58","The PR is pretty much complete. The only thing left is to make the examples
integrate into the docs (this has been started). I'll allocate some time to
address it next week..

On Wed, Jun 3, 2015 at 1:27 PM, Alexandre Abraham notifications@github.com
wrote:

> @dohmatob https://github.com/dohmatob @schwarty
> https://github.com/schwarty @GaelVaroquaux
> https://github.com/GaelVaroquaux What is the status on this PR?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219#issuecomment-108314717.

## 

DED
","",""
"issue_comment","219","nilearn","nilearn","GaelVaroquaux","2015-07-14 07:34:02","> Closed #219.

219 closed. Sniff! That was an old friend.

Looking forward to see its daugther.
","",""
"issue_comment","219","nilearn","nilearn","dohmatob","2015-07-14 07:48:23","On Tue, Jul 14, 2015 at 9:34 AM, Gael Varoquaux notifications@github.com
wrote:

> > Closed #219.
> 
> 219 closed. Sniff! That was an old friend.
> 
> [Chuckles] true true
> 
> Looking forward to see its daugther.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219#issuecomment-121153206.

## 

DED
","",""
"issue_comment","219","nilearn","nilearn","lesteve","2015-02-04 17:47:18","Before I forget, the examples will need to be moved to their relevant subfolders, like examples/decoding for example.

Also for plot_haxby_space_net.py and plot_oasis_vbm_space_net.py, the examples they were based on have changed cosmetically but significantly compared to when this work started. For obvious uniformity reasons, it would be very nice if your examples could be updated accordingly. 

Stuff I can think of off the top my head, using index_img, some uniformity with dataset naming and using variables with filename in them to make it more obvious what dataset_files.cmaps[0] is for example. It is quite easy to figure out by looking at an up-to-date version of plot_haxby_simple.py and plot_oasis_vbm.py.
","",""
"issue_comment","219","nilearn","nilearn","mrahim","2015-02-04 15:28:28","> Is there any packaged / fetchable template with / nilearn ?

You can remove `background_img` from `plot_stat_map`

![figure_1](https://cloud.githubusercontent.com/assets/8612476/6043179/cc13308c-ac8a-11e4-94e2-03471d5753a8.png)
","",""
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:39:19","how about using 

from distutils.version import LooseVersion

?

ping @GaelVaroquaux 
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:39:51","partial from functools does not pickle. I don't know if it's a pb here
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:43:10","it's a correlation of the y right? not an L2 / MSE loss
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-27 12:43:12","Yeah, i have other backward-compat problems with center_data (not present sklearn <= 0.10). I'm implementing some lightweight backports, so that travis can let me go...
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:43:29","X_std does not seem to be used
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:44:49","learned by univariate selection. ANOVA is one way to do it.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:47:01","returns ones?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:47:14","proof? I would talk about accuracy.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-27 12:48:10","Yes.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:48:13","ndim as meaning a numpy which is different here. I would call is size as np.size
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:48:36","missing space before if
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:48:58","cleanup / decide
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:49:19","Say what None does
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-27 12:49:27","But precisely, ANOVA (one-way) seems to be what's been done under the hood in the scikit (i just rushed into the code at the moment).
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:51:10","remove comment. Name is good for me.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:51:39","if you return linalg.svdvals(X)[0] *\* 2
then it corresponds to ""0.5 \* ||y - Xw||^2""
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:52:45","Compute upper bound on Lipschitz constant for the gradient of the logistic sum:
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:52:53","then remove this 
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:53:40","why norm(X, 2) here and svdvals above? make up your mind on what is faster.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:55:01","Standard formatting is:

```
    X: 2D array, shape (n_samples, n_features)
        Design matrix.
```
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:55:30","```
   y: 1D array , shape (n_samples,)
        Target.
```
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:55:40","no =
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:55:52","bad formatting
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:56:24","call it residual, r or R

aux is bad
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:57:01","shape?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:57:09","remove empty line
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:57:37","docstring not up to date
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 12:58:57","> But precisely, ANOVA (one-way) seems to be what's been done under the hood in the scikit (i just rushed into the code at the moment).
> 
> there are different ways. It depends what nilearn supports.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:07:33","update docstring
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:08:01","+1 for img.dtype
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:09:30","private function?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:10:57","use np.log1p
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:11:32","add your name + license on top of every file.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:11:52","import nilearn stuff at the end
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:12:29","end of sentence ??
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:12:50","documenetation -> documentation
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:13:17","mechanism
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:13:53","you don't cache these solver calls?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:14:14","don't you want to average rather than refit?

ping @GaelVaroquaux 
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:14:26","XXX : docstring
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:15:11","cache?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:15:38","no =
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:16:05","hum :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:17:12","???
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:19:40","remove w
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:20:30","sklearn import before common
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/estimators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-27 13:21:49","No, because the whole path_func is supposed to be cached anyway. Plus caching has an overhead, and I try to avoid it where i can.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-27 13:28:35","Averaging is done after joining all the fold workers . This re-fit here is simply to solve the best model to high precision to reduce the noise / imprecision in the maps induced by early stopping, etc. Plus this only adds worst-case complexity O(n_folds \* hardness of high precision problem), which is reasonable.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:29:34","fair enough.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:30:52","why tv stuff in common.py ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:31:22","authors + license should be on top of every file
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:31:44","logisitc -> logistic
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:31:49","these
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:32:02","this docstring does not respect pep257
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 13:32:20","type is not formatted right
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 14:09:19","if  loss not in [""mse"", ""logistic""]:

is enough
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 14:13:40","import matplotlib.pyplot as plt
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_smoothlasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-27 14:14:23","this print will not work with python3
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_smoothlasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-29 09:11:13","Indeed, there is an underlying naming problem: ""mse"" or ""squared_loss"". This problem is ubiquitous in the code base. Will fix this asap.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-29 09:13:04","rm what ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-29 09:28:51","OK, fine. Co-authors should claim ownership by appending / pre-pending their names themselves.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-29 09:42:13","Please could you be more precise ? pep257 is a whole document http://legacy.python.org/dev/peps/pep-0257/ :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-29 09:54:47","you copied code from different places. Just keep the names when you do this.

At least there is Gael, Gaspard, You, me, certainly Fabian and Michael...

license is important too
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-29 09:55:26","If you update the docstring headline you can remove this sentence.
The headline says it's an upper bound.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-29 09:57:42","docstring structures

```
""""""Headline

Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod
tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim
veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea
commodo consequat. Duis aute irure dolor in reprehenderit in voluptate
velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint
occaecat cupidatat non proident, sunt in culpa qui officia deserunt
mollit anim id est laborum.

Parameters
---------------
foo : type
    Description.

Returns
----------
out : type
    Description.
""""""
```

Headline fits on one line.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-29 09:59:09","ex of type desc for a numpy array

```
foo : ndarray, shape (n_samples, n_features)
```
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-29 11:02:42","OK, I've tried my best to build a comprehensive authors' list.

@everybody: Please let me know if you think you've been forgotten, or your code has simply been plagiarized! Better still, simply go ahead and (dis-)claim ownership by deleting / appending / prepending your name from / to the authors' list of the concerned module / script. This is an issue like any other code bug, and you shouldn't hesitate to provide fixes, etc.

I'm done with this for now. 
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-29 11:24:29","OK. Done.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-29 11:28:42","OK, makes sense. Done.

On Sun, Jun 29, 2014 at 11:57 AM, Alexandre Gramfort <
notifications@github.com> wrote:

> In nilearn/sparse_models/tv.py:
> 
> > +import numpy as np
> > +from .common import (compute_mse_lipschitz_constant,
> > -                     compute_logistic_lipschitz_
> >   constant,
> > -                     tv_l1_reg_objective, mse_loss, mse_loss_grad,
> > -                     logistic_grad as logistic_loss_grad,
> > -                     logistic as logistic_loss)
> >   +from .prox_tv_l1 import prox_tv_l1, intercepted_prox_tv_l1
> >   +from .fista import mfista
> >   +
> >   +
> >   +def tvl1_solver(X, y, alpha, l1_ratio, mask=None, loss=None,
> > -                rescale_alpha=True, lipschitz_constant=None,
> > -                prox_max_iter=5000, verbose=0, tol=1e-4, **kwargs):
> > -    """"""Minimizes empirical risk for TV-l1 penalized least-squares (
> > -    mean square error --a.k.a mse) or logisitc regression. The same solver
> > -    works for both of this losses.
> 
> docstring structures
> 
> """"""Headline
> 
> Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod
> tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim
> veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea
> commodo consequat. Duis aute irure dolor in reprehenderit in voluptate
> velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint
> occaecat cupidatat non proident, sunt in culpa qui officia deserunt
> mollit anim id est laborum.
> 
> ## Parameters
> 
> foo : type
>     Description.
> 
> ## Returns
> 
> out : type
>     Description.
> 
> Headline fits on one line.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r14328391.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-29 16:06:09","why those **kwargs? I tend to avoid kwargs especially in high level functions.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-29 16:09:27","extra =
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-29 16:09:54","missing . at the end of line. Same pb above.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-29 16:11:59","ping
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-06-29 16:12:42","ping this is for w -> 0.5 \* ||y - Xw||^2

as said already. Let me know when you're done addressing my comments.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-06-30 15:52:29","I wouldn't skip a line here.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-06-30 15:58:05","Incorrect docstring formatting: there is a space missing in front of the "":"" (the problem is repeated quite a few times in the file).
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-06-30 15:59:41","I don't think that a function should be called ""my_alpha_grid"". The name isn't quite as explicit as we would like it to be. We need to find a good term to replace the ""my_"". What is special about this alpha_grid?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-06-30 16:02:43","More control on the way the grid is created (i.e more options). will change the name asap.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-06-30 16:05:41","This file needs to be called different. 
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-06-30 16:26:11","> > -    gradient = np.zeros(shape, dtype=np.float)  # xxx: img.dtype?
> 
> +1 for img.dtype

No. It needs to be done in the following way:

<pre>
if img.dtype.kind == 'f':
   dtype = img.dtype
elif img.dtype.itemsize == 8:
   dtype = np.float64
else:
   dtype = np.float32
</pre>


this is to avoid at all cost using float64, which consume a lot of memory
and slow computations down.

We should implement a helper function in nilearn._utils.numpy_conversions
that does the above.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-06-30 16:26:43","> > +def test_grad_div_adjoint_arbitrary_ndim(size=5, max_ndim=5, random_state=42):
> 
> private function?

Should be in tests.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-06-30 16:27:48","> don't you want to average rather than refit?
> 
> ping @GaelVaroquaux

Yes, average.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-06-30 16:29:47","> > -        self.**class**.**name**.endswith(""CV"")
> 
> ???

Just don't catter for non CV stuff. In the nilearn codebase, the way to
do non-CV stuff will be to give a single value of alpha and l1_ratio to
the objects.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-06-30 16:33:37","> Indeed, there is an underlying naming problem: ""mse"" or ""squared_loss"".

Should be called ""squared_loss"".
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-07-04 06:04:55","Are you ok with loss.py ?

On Mon, Jun 30, 2014 at 6:05 PM, Gael Varoquaux notifications@github.com
wrote:

> In nilearn/sparse_models/common.py:
> 
> > @@ -0,0 +1,393 @@
> > +""""""
> 
> This file needs to be called different.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r14358354.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:11:29","Can the `**kwargs` be avoided?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:11:48","weights
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:13:57","This is never executed
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:15:18","`if callback is not None`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:16:43","naming question: Why is this a call **ER** back? It is also being called.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:17:20","can we be explicit about the kwargs?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:17:52","kwargs
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:32:33","Why not `self.debias` ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:35:00","what about `supplementary_kwargs` or being explicit?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:36:20","This creates a list of `n_problems` times the **same** instance of a list. Are you sure you want this?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-08-07 09:36:39","Short answer: No. These are arguments to the solver under-the-hood. Having a kwargs in very-very low-level APIs is not really a problem.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:40:50","OK, it seems you are overwriting those scary empty lists from above. So it will work.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:52:02","eval is evil
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-08-07 09:52:42","Indeed this an utterly dangerous potential bug:

```
In [1]: l = [[]] * 5

In [2]: l[0].append(3)

In [3]: l
Out[3]: [[3], [3], [3], [3], [3]]
```

Fixed.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:53:21","add error message or work with traditional `raise`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-08-07 09:55:23","You might want to close your parietal-python issue :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:57:48","Could you put the function instead of the string?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 09:58:15","function instead of string
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 10:01:14","Cross-validated
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 10:05:57","Which `ImportError` are you catching? nilearn depends on sklearn, so it shouldn't be necessary. If I am mistaken, then at least change `pass` to `raise SkipTest` (or something to that effect), because `except: pass` is bad practice
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/tests/test_cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-07 13:58:58","The order is wrong with respect to your outputs.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-08 10:42:38","We should make sure we are all talking about the same rescaling if/when we are talking about rescaling. The default of `rescale_alpha` is set to `True` which I fully endorse, because we do not want different numbers of samples changing the effective regularization: This parameter indicates whether or not we put `1. / (2 * n_samples)` in front of the loss instead of `1. / 2.`

We should discuss a) whether `False` for this parameter is ever useful (AFAIK, sklearn does this scaling by default) and b) whether we should talk about a much more interesting scaling which is a scaling as a fraction of `alpha_max`. The only reason not to do this is that we only have an upper bound for `alpha_max` and not the exact value. The upper bound is not unreasonable, though, as it is the `alpha_max` of the corresponding tvl1 variation lasso.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-08-10 14:13:58","error message or preferably `raise ValueError(""loss must be one of 'mse' or 'logistic', not '%s'"" % loss)`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-01 21:44:08","then there should be elastic_net somewhere in the name of the function
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 11:21:00","This is great, bit for the sake of future maintenance, please indicate whether each feature put in this module has been introduced in skl (say) 0.14 or 0.15, so that this part can be safely removed within a few years.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/_utils/fixes/sklearn_basic_backports.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:08:09","This choice should be documented
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:08:40","More complete docstrings for that guy ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:23:18","What do you mean ?  ;-)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:24:49","gradient is computed
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:25:55","close the parentheses
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:27:29","use sk's fast_dot fast_dot ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:28:48","compute_hess
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:30:53","hess_matvec
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:32:53","Don't call it gradient, this is confusing
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:35:59","grad: what are the expected dimensions etc ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:38:26","reorganize the dosctrings
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:39:40","img.dtype would make sense I guess
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:42:34","Why not use nilearn.masking._unmask_nd ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:43:34","dosctring ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-02 21:44:56","dosctrings ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 09:49:34","We do we need this backport? It seems to me that it is a class that has not changed at all lately.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/_utils/fixes/sklearn_basic_backports.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 09:50:03","Why do we need this backport.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/_utils/fixes/sklearn_basic_backports.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 09:50:07","Why do we need this backport.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/_utils/fixes/sklearn_basic_backports.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 09:51:40","There is something that makes me uneasy with the code here. You are grabbing huge portions of the codebase of scikit-learn, and slightly modifying them. There must be better ways to do things.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/_utils/fixes/sklearn_basic_backports.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 09:53:13","This sentence is imprecise and unprofessional. You should not be putting such sentences in production code but actually describing what is going on.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:26:18","I would prefer if you could inherit from sklearn.base.BaseEstimator here.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:27:34","Also, I would prefer that you implement this as a function, or maybe even in-lined in the right place, as in the SparseVariation codebase. It helps reducing the complexity of the codebase.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:31:41","Same remark here: that class can go away, as in the Sparse Variation codebase.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:32:56","In general, any function or class name that is 'my_something' is wrong, as it is not a useful name.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:36:03","In docstrings, there should always be a space between the parameter name and the colon ':', elsewhere the doc rendered doesn't render the docstring well. Please check that this applies to all your docstrings.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:39:39","Don't use lambdas.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:41:23","I'd rather not use partial either: I don't think that it pickles, thus it makes it harder to do parallel computing on this code.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:45:16","You can remove the support for the callback here, it will make the codebase lighter.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:47:47","I don't understand the logic for caching this. If another fold return the same alpha, it will be on different data.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:49:21","There is a lot of duplication in the two functions: logistic_path_scores and squared_loss_path_scores. Wouldn't it be possible to merge them?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-09-03 11:51:31","If I'm not mistaken, the tricks used in fast_dot are now defacto. No ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:51:40","We try to avoid at all cost properties. Here I believe that it is not necessary.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:52:12","> If I'm not mistaken, the tricks used in fast_dot are now defacto. No ?

What do you mean by 'defacto'?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:52:55","I guess this is the class that will evolve into a 'SpatialNet' object.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:54:18","Can you find a more explicite name than 'tricky_kwargs'?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 11:55:24","It would probably be better to use 'np.sort(alphas)[::-1]'
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-09-03 12:17:04","This is just two lines of code for doing something precise. I'd consider using nilearn.masking._unmask_nd an overkill.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:21:44","Is there a reason why you did not use np.range here?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:30:02","The function _ovr_y is a slightly tricky pattern to read. It deserves at least a comment above to explain what it does.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:31:48","What's the use of this method? Can't you just let inheritence work, and not actually define a fit?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:33:00","Same here: I am not sure that you need the method.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:33:24","Once again, inheritence does the job for you.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:36:03","I suggest that we drop the 'CV' names. The neuroimaging end user will not understand what CV means, and we'll just say in the docstring that these objects set their parameters by cross-validation.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:36:53","This shouldn't be in, I believe.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:44:00","Let's not create a subackage. We can organize the code in a small number of files with explicit names, and thus avoid a nested structure.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'setup.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:46:06","I believe that you are not dividing by n_samples.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:46:24","The docstring is thus false.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:49:35","I believe that this function should be moved to the test sub-package.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:50:32","Here you are using the global random number generator, which you should never do, as you don't control its internal state. You should use the 'random_state' local variable.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 12:52:46","I believe that we should try to get rid of 'common': it is somewhat of a generic file which has a non descriptive name. We could start by moving 'check_lipschitz_continuous' in the 'fista' file: is this function used anywhere else?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-09-03 12:55:52","""de facto"", i.e ""standard"", ""in practice""
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 13:04:03","To avoid being fragile with regard to the version of scikit-learn, and also having a huge set of backports, I suggest that you copy the logic of this function into a 'predict_proba' method for your object that implements a logistic loss. That should enable you to remove all the backports.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/_utils/fixes/sklearn_basic_backports.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 13:27:28","There is indeed a name collision between the image spatial gradient and the derivative of a function.

With Gaspar we had decided to call gradients in the sens of derivatives 'derivative'. This is not standard, but removes the confusion. In addition, here, we could use the term 'img_gradient'.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 13:30:18","What we often do is the following: dtype=(np.float32 if img.dtype == np.float32 else np.float64), to preserve float32
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 13:48:48","This should go somewhere in utils.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 13:49:02","This should go somewhere in utils.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-03 13:50:45","The file should be renamed 'objective_functions', or something like this.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-09-03 20:15:13","All right, got it.
maybe: precomputed ""spatial derivative + id"" array

In nilearn/decoding/sparse_models/common.py:

> > +
> > +
> > +def get_gradient_id_shape(img_shape):
> > -    """"""Return the shape of the gradient + id operator output.
> >   +
> > -    """"""
> > -    return [len(img_shape) + 1] + list(img_shape)
> >   +
> >   +
> >   +def tv_l1_from_gradient(gradient):
> > -    """"""Our total-variation like norm: TV + l1
> >   +
> > -    Parameters
> > -    ----------
> > -    gradient: array
> > -       precomputed ""gradient + id"" array
> 
> There is indeed a name collision between the image spatial gradient 
> and the derivative of a function.
> 
> With Gaspar we had decided to call gradients in the sens of 
> derivatives 'derivative'. This is not standard, but removes the 
> confusion. In addition, here, we could use the term 'img_gradient'.
> 
> —
> Reply to this email directly or view it on GitHub 
> https://github.com/nilearn/nilearn/pull/219/files#r17048340.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-09-04 09:06:45","Done.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/_cv_tricks.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-09-04 09:45:26","Does this have an explicit advantage. Is this best-practice-related ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/cv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-11 13:14:02","Make things simpler here: just use 'n_jobs=1'
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_smoothlasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-11 13:15:31","""bingo"" is neither very professional nor precise. Could describe exactly what you meant here?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-11 13:17:48","You can't do that! Playing with sys.path is strictly forbidden. It has crazy side effects and leads to problems hard to debug.

Why are you doing that?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-11 13:31:38","No, you are not allow to do this :). Use the rng object for that, instead of the Python random module.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-11 13:33:56","Same thing here: you cannot modify sys.path.

This might mean that the functions in simulate_smooth_lasso_data should be moved in the nilearn package, where they are importable.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-11 13:34:46","You should not use the global random number generator. Only the local one that you have created.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-11 13:34:58","You should not use the global random number generator. Only the local one that you have created.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-11 13:35:14","You shouldn't be modifying sys.path
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-09-12 11:28:38","Indeed partial pickels.

On Wed, Sep 3, 2014 at 1:41 PM, Gael Varoquaux notifications@github.com
wrote:

> In nilearn/decoding/sparse_models/common.py:
> 
> > -    z = np.dot(X, w[:-1]) + w[-1]
> > -    yz = y \* z
> > -    z = _sigmoid(yz, copy=False)
> > -    z0 = (z - 1) \* y
> > -    grad = np.empty(w.shape)
> > -    grad[:-1] = np.dot(X.T, z0)
> > -    grad[-1] = np.sum(z0)
> > -    if mask is not None:
> > -        grad = np.append(_unmask(grad[:-1], mask), grad[-1])
> > -    return grad
> >   +
> >   +# Wrappers.
> >   +# XXX div (see below) could be computed more efficienty!
> >   +gradient = lambda w: gradient_id(w, l1_ratio=0.)[:-1]  # pure nabla
> >   +div = lambda v: div_id(np.vstack((v, [np.zeros_like(v[0])])), l1_ratio=0.)
> >   +squared_loss_grad = partial(squared_loss, compute_energy=False,
> 
> I'd rather not use partial either: I don't think that it pickles, thus it
> makes it harder to do parallel computing on this code.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r17043508.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-12 11:37:23","> Indeed partial pickels.

Cool! It used not to be the case.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/sparse_models/common.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-13 18:11:53","We haven't found empirically any benefit from the backtracking. You should remove it from the codebase.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-13 18:15:33","No need to define a variable if you are using it only once and not modifying it. You should simply inline '.5' in the code. For someone who reads the code, it is easier, as he does not have to scroll between variable definitions and the lines where the variables are used.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-13 18:18:03","I don't see where the notion of mask is used in this function. Is this comment relevant.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-13 18:38:11","Let's just remove these 2 last lines: we don't need them.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-09-15 06:08:56","OK, I'll look at your comments this morning. See you.

On Sat, Sep 13, 2014 at 8:38 PM, Gael Varoquaux notifications@github.com
wrote:

> In nilearn/decoding/fista.py:
> 
> > +
> > -        if energy < best_energy:
> > -            best_energy = energy
> > -            best_w = w.copy()
> > -            best_z = z.copy()
> > -            best_t = t
> > -            best_dgap_tol = dgap_tol
> > -            best_dgap_tol = dgap_tol
> >   +
> > -    return (best_w,
> > -            objective,
> > -            dict(w=best_w.copy(), z=best_z, t=best_t, dgap_tol=best_dgap_tol,
> > -                 stepsize=stepsize))
> >   +
> >   +# pure ISTA
> >   +ista = partial(mfista, pure_ista=True)
> 
> Let's just remove these 2 last lines: we don't need them.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r17513677.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 11:32:35","Minor typo in the comment
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 11:39:00","How about we never deal with the case of no mask (ie mask becomes a mandatory argument), and write the code above as:

<pre>
w = w[mask]
</pre>


This would also remove the need for raveling.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 11:39:41","Same thing here: let's just consider that we always have a mask.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 11:40:43","That's a function of only one line, that you are using in only one place. It think that you could inline it.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 11:41:19","I am not sure what the 'otherwise' is doing here.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 11:43:13","Do we really want to deal with 'None' masks. I think that it would make the code more straightforward to only accept masks.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 11:44:23","This test function should be moved in a separate test module.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 11:57:27","Same thing here: let's always consider that we have a mask.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 11:58:47","This should probably be renamed 'projector_on_tvl1_dual'.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 11:59:26","Do we use the callback anywhere in the code? If not, let's remove it.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 12:00:14","That 'XXX' is no longer relevent.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 12:03:57","Why is there an 'l1_ratio' variable here that is not used?

Why is 'tau' called 'tau', and not 'alpha' as we do in standard?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 12:13:34","I think that you are using this function only once, and that in the tests. You should probably inline it.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 12:14:09","I don't think that you are using this function anywhere. It should probably go.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 13:37:30","You are not using locals random number generators here. You should always use RNGs that are local to the function, so that 2 calls to the function return exactly the same thing.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 15:49:12","Comment line needs to be updated
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_smoothlasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 15:49:31","Comment needs to be updated.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 15:50:28","These lines should not be commented. They should either be running, or removed.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_sklearn_basic_backports.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-22 15:52:21","It's somewhat a problem that you are skipping those tests, as they end up being false (here you are using the 'SmoothLassoClassifier' which is not defined.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 15:27:14","Please, no global rng, but a 'random_state' argument to functions, as it is done in standard with scikit-learn.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 15:29:23","I would prefer a longer name. Say 'lipschitz_const'
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 15:29:37","I would remove this comment
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:28:13","@dohmatob : I would be interested in testing the ideas in the following paper:
http://statweb.stanford.edu/~candes/papers/adap_restart_paper.pdf
in particular paragraph 3.2

Basically, it boils down to the fact that resetting the FISTA parameters (t and t0) every once in a while speeds the convergence process. We are already partly doing that by setting 'z = w' in the monotonous FISTA scheme below.

What I would like you to test is the following strategies:
1. if at line 235 we put z=w and t=1, do we get a better convergence (standard log-log plots on fMRI problems)?
2. if, at the line above (line 226) we do the following (mfista_factor being initialized above to eg 1e-2):

``` python
     if energy_delta < mfista_factor * energy:
            mfista_factor *= 1e-2
            ....
```

in other words, loosen the criteria to switch to mFISTA in the beginning of the convergence of the algorithm.
I'd be interested in empirical tests (as usual, standard log-log plots on fMRI data) to see if this speeds things up.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(203, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:30:51","@dohmatob : comment still to address: let's now always assume that mask=None is not a valid situation, and always raise an error.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:31:13","I meant not raise an error. Just not support it.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:32:46","Please rename this function to 'dual_gap_prox_tv'
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:33:13","Please rename this function to '_objective_function_prox_tv'
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:33:40","Please update the equation to show the l1 part.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:34:43","Let's just rename the input argument 'im' to 'input_img', instead of doing this in the code.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:37:02","We need a backport of sklearn.utils.extmath.squared_norm, and we need to use it in many places.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:38:35","I think that this should be a private function.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:42:49","I would phrase this with something like 'taking in account the fact that the last entry of w is the intercept'.

I am surprised that you are not (optionally) dealing with the mask in this function. I would have thought that, from a code design point of view, this would be a good place to deal with a mask.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:44:52","As we are using a spearman correlation, I would have thought that there is no need to remove the mean, as it wouldn't change anything to the spearman correlation.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:47:52","I'd like this class, and it's subclasses to be removed. The code can be made tighter by inlining the corresponding logic where these classes are used.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:49:30","What's the purpose of the call to 'np.array'? If the goals is to conver to booleans, I'd rather have 'mask.astype'.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:50:39","It would be cheaper to test whether  the percentile is 100, I would say.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:51:18","I'd call this function '_space_net_alpha_grid'
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:54:36","You need to specify the expected shape of X. I cannot tell from looking at the function's signature and docstring if it is expected to be 4D (volumetric) or 2D.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:56:10","Once https://github.com/nilearn/nilearn/pull/241
is merged, you should apply the _fast_smooth_array to X before learning the feature selection. It is always important to smooth a bit before applying univariate feature selection with fMRI data.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:57:49","Could you define a _test_score_classif and _test_score_regression outside of the function. Defining functions inside other functions isn't great as it doesn't encourage their test / reuse.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 16:58:38","This logic should be folded into the EarlyStoppingCallback object, adding an extra argument 'classif' to it.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 17:01:20","Because parameters can be changed later by user, we usually do these checks at fit time.

If you want to do them both at construction time and at fit time, you can define a _check_params method that does these checks, and run it at the end of the __init__ and at the beginning of the fit.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 17:03:01","How about renaming this function to '_binarize_y' and moving the mask stuff to the fit part.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-09-30 17:03:21","I would like to never support mask being None.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","lesteve","2014-10-01 08:46:36","In #241, we agreed that calling _smooth_array with fwhm='fast' was the way forward because it has a little bit of additional logic, like getting rid of non-finite values in the array. It may apply to this case too.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-01 12:37:40","Nop, the TVl1 denoising problem is solved on a full cuboid. Masking and unmasking are handled outside. I tend not to mix the two.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-01 12:41:49","Is it really reasonable to have a function called _space_net_something in a module named space_net.py ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-01 12:43:23","It would at least help my allergy against functions containing the particle
/my/ ... :)

On Wed, Oct 1, 2014 at 2:41 PM, dohmatob elvis dopgima <
notifications@github.com> wrote:

> In nilearn/decoding/space_net.py:
> 
> > -        """"""Unmasks the input vector `w`, according to the mask learned by
> > -        Univariate screening.
> >   +
> > -        """"""
> >   +
> > -        if self.support_.sum() < len(self.support_):
> > -            w_ = np.zeros(len(self.support_))
> > -            w_ = np.append(w_, w[-1])
> > -            w_[:-1][self.support_] = w[:-1]
> > -        else:
> > -            w_ = w
> >   +
> > -        return w_
> >   +
> >   +
> >   +def _my_alpha_grid(X, y, eps=1e-3, n_alphas=10, l1_ratio=1., alpha_min=0.,
> 
> Is it really reasonable to have a function called _space_net_\* in a named
> space_net.py ?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r18275441.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-01 18:35:11","The code snippet you've highlighted is essentially scoring logic code (at least the regression case). Are you saying you want me to embed scoring logic into early stopping logic ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-01 20:18:41","> The code snippet you've highlighted is essentially the code for the scoring
> logic (at least the regression case). Are you saying you want me to embed
> scoring logic into early stopping logic ?

I don't think that it is used elsewhere. So, for now, I don't see a
reason not to.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-02 12:31:38","SpaceNet should be importable from nilearn.decoding (and imported like this in the examples).
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_smoothlasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-02 12:35:20","Let's use 'decoder' as a variable name here, rather than 'slcv'.

Also, let's split the instantiation of the object and the call to fit in two different lines: it will be easier for the reader.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_smoothlasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-02 12:36:29","Don't seed the global RNG. Use a local one.

Could you please check that you are never doing that anymore.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-02 15:38:53","That use of a first masker to remove features feels hackish. Do we actually need this, now that we do univariate feature screening?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_smoothlasso_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-02 16:52:42","You're right we don't need this anymore (this is residual hack from the
original plot_oasis_vbm.py script). Will cleanup asap.

On Thu, Oct 2, 2014 at 5:38 PM, Gael Varoquaux notifications@github.com
wrote:

> In plot_oasis_smoothlasso_vbm.py:
> 
> > +
> > +n_subjects = 100   # more subjects requires more memory
> > +memory = Memory(""cache"")
> > +
> > +### Load Oasis dataset ########################################################
> > +dataset_files = datasets.fetch_oasis_vbm(n_subjects=n_subjects)
> > +age = dataset_files.ext_vars['age'].astype(float)
> > +
> > +### Preprocess data ###########################################################
> > +nifti_masker = NiftiMasker(
> > -    standardize=False,
> > -    smoothing_fwhm=2,
> > -    memory=memory)  # cache options
> >   +# remove features with too low between-subject variance
> >   +gm_maps_masked = nifti_masker.fit_transform(dataset_files.gray_matter_maps)
> >   +gm_maps_masked[:, gm_maps_masked.var(0) < 0.01] = 0.
> 
> That use of a first masker to remove features feels hackish. Do we
> actually need this, now that we do univariate feature screening?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r18347190.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_smoothlasso_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-02 17:04:16","> You're right we don't need this anymore (this is residual hack from the
> original plot_oasis_vbm.py script). Will cleanup asap.

Awesome! It's always a pleasure to see hacks disappear from a codebase.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_smoothlasso_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 14:02:16","You should be able to specify memory as a string, and thus not import Memory at all.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_smoothlasso_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 14:03:01","Do we still need the NiftiMasker import?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_smoothlasso_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 14:03:58","I'd like this import to be moved below, in the visualization part. 
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_smoothlasso_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 14:11:14","The naming convention is to end names of variables that are niimgs with '_img', not '_niimg'. So that would be 'coeg_img'. Please also correct all the other variable names of this script.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_smoothlasso_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-07 16:25:53","Ok will fix asap.

On Tuesday, October 7, 2014, Gael Varoquaux notifications@github.com
wrote:

> In plot_oasis_smoothlasso_vbm.py:
> 
> > +from nilearn.input_data import NiftiMasker
> > +
> > +n_subjects = 100   # more subjects requires more memory
> > +memory = Memory(""cache"")
> > +
> > +### Load Oasis dataset ########################################################
> > +dataset_files = datasets.fetch_oasis_vbm(n_subjects=n_subjects)
> > +age = dataset_files.ext_vars['age'].astype(float)
> > +
> > +
> > +### Fit and predict ###########################################################
> > +from nilearn.decoding import SpaceNet
> > +decoder = SpaceNet(memory=memory, screening_percentile=20., verbose=1,
> > -                   smoothing_fwhm=2.)
> >   +decoder.fit(dataset_files.gray_matter_maps, age)
> >   +coef_niimg = decoder.coef_img_
> 
> The naming convention is to end names of variables that are niimgs with
> '_img', not '_niimg'. So that would be 'coeg_img'. Please also correct all
> the other variable names of this script.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r18521100.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_smoothlasso_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 16:48:08","The title should state 'Space-Net' instead of 'S-LASSO prior' and the file should be renamed.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_smoothlasso_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 16:52:00","You should not import nibabel for this. You can simply pass the filename to plot_stat_map.

Also, you should rename the title of the figure. Maybe we should use the term 'Graph-net' here, as it is the term used by many people in neuroimaging.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_smoothlasso_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 16:53:35","Same remark about ""memory"": you don't need to create a memory object. You can simply pass the path to the objects.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_smoothlasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 16:55:39","You should import directly from nilearn.image and not its sub packages.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_smoothlasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 16:56:09","The filename should be rather 'tvl1' than 'slcv'.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_smoothlasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 16:59:13","You are doing TV-l1 here. You should use a title that conveys this.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_smoothlasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 17:04:19","You are doing TV-l1 here. The filename and title should be renamed.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_smoothlasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 17:06:25","The line above should be removed.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-07 17:13:42","You shouldn't be printing. You should be testing the corresponding property.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 07:45:22","I'd prefer the name 'decoder', rather than slcv.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_smoothlasso_vbm.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 10:58:01","Any update on that?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(203, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-08 11:07:28","No, will look at it asap (hopefully today).

On Wed, Oct 8, 2014 at 12:58 PM, Gael Varoquaux notifications@github.com
wrote:

> In nilearn/decoding/fista.py:
> 
> > -        # ISTA is provably monotonous
> > -        if i > 0 and check_monotonous:
> > -            assert energy <= 1.1 \* old_energy, (
> > -                ""Oops! old_energy = %g < energy = %g. This is ""
> > -                ""unacceptable since ISTA and mFISTA are provably monotonous.""
> > -                "" The must be a bug in the code. For example, maybe ""
> > -                ""you are assuming a wrong lipschitz constant or you ""
> > -                ""have bugs in the way you compute the gradient of""
> > -                "" the smooth part of the energy."") % (old_energy, energy)
> >   +
> > -        # energy house-keeping
> > -        energy_delta = old_energy - energy
> > -        old_energy = energy
> >   +
> > -        # z update
> > -        if energy_delta < 0.:
> 
> Any update on that?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r18575016.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(203, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 11:16:50","> No, will look at it asap (hopefully today).

Remember: this PR is a priority. It is blocking for several people to be
able to move on.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(203, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-10-08 11:40:12","@GaelVaroquaux it is unclear where the priority is: this particular portion of the PR looks like a new feature and may actually slow down the process. Should it be kept for a future PR ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(203, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 11:47:46","> @GaelVaroquaux it is unclear where the priority is: this particular portion of
> the PR looks like a new feature and may actually slow down the process. Should
> it be kept for a future PR ?

It's a couple of hours of work not more.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(203, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-08 11:55:21","On Wed, Oct 8, 2014 at 1:47 PM, Gael Varoquaux notifications@github.com
wrote:

> In nilearn/decoding/fista.py:
> 
> > -        # ISTA is provably monotonous
> > -        if i > 0 and check_monotonous:
> > -            assert energy <= 1.1 \* old_energy, (
> > -                ""Oops! old_energy = %g < energy = %g. This is ""
> > -                ""unacceptable since ISTA and mFISTA are provably monotonous.""
> > -                "" The must be a bug in the code. For example, maybe ""
> > -                ""you are assuming a wrong lipschitz constant or you ""
> > -                ""have bugs in the way you compute the gradient of""
> > -                "" the smooth part of the energy."") % (old_energy, energy)
> >   +
> > -        # energy house-keeping
> > -        energy_delta = old_energy - energy
> > -        old_energy = energy
> >   +
> > -        # z update
> > -        if energy_delta < 0.:
> 
>  @GaelVaroquaux https://github.com/GaelVaroquaux it is unclear where
> the priority is: this particular portion of the PR looks like a new feature
> and may actually slow down the process. Should it be kept for a future PR ?
> It's a couple of hours of work not more.
> 
> Well, it's probably much less than a couple of hours of work. The point is
> this adaptive restart stuff is at the bottom of my priority list. It's yet
> another experiment, and can only only slow down the PR. Thence, I don't
> think it's blocking the PR in any way.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r18576623.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(203, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 12:38:04","On Wed, Oct 08, 2014 at 04:55:21AM -0700, dohmatob elvis dopgima wrote:

> Well, it's probably much less than a couple of hours of work.

If things are unclear about what I would like as far as experiments go,
we can sit down together and work on it together, when you get to it, to
make sure that you spend as little work as possible on this.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(203, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 13:25:20","I think that I would like this function to be called 'spectral_norm_squared', to be very explicit about which norm it is.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 13:27:17","I think here, it should be 'n_features', and not 'n_voxels', to be consistent with the fact that there is no notion of voxels in this part of the code.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 13:28:08","I would like to use a backport of _squared_norm from scikit-learn, as already mentioned elsewhere in the PR.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(97, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 13:59:31","Nitpicking: I'd like this function to be called '_prox_tvl1_with_intercept'.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 14:02:40","I'd prefer 'classif' to be named 'is_classif'. We don't have a docstring here, which I think is fine because it is a very internal object, but we need to be careful that people can guess the function of the argument as easily as possible.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 14:04:49","Great code! It's easy to under-estimate how much fine knowledge a function like this captures.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 14:16:41","We need to smooth the data that we are passing to the SelectPercentile here, using nilearn.image.image._smooth_array with fwm='fast'. PR #258 will enable you to pass affine=None there.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 14:17:38","Actually, scrap that, you might as well use _smooth_array_fast, as we don't need the 'ensure_finite'.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 14:22:30","Please change `**params` to solver_params, and use a dict rather than a `'**'` pattern. We have a policy against `'**'`, as they make forward evolution hard. Here you can just use a dictionary, that you pass to your solver using `'**'`.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 14:23:15","Please describe this without reference to sklearn.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 14:27:12","I think that we need two subclasses: one SpaceNetClassifier and one SpaceNetRegressor. These would simply set this parameter (which you could move to a class parameter, that way you don't have to override the init).

The benefit is that it makes it more explicit for the user, and thus reduces the risk of human error.

What do you think, @agramfort ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-08 14:50:56","The fact that you are renaming when importing tells me that we should rename those functions in the file. Could you please do that (for logistic and div).
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-08 16:53:24","> What do you think, @agramfort https://github.com/agramfort ?
> 
> +1
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-09 06:20:30","(Applies to all comments about smoothing and the like): Smoothing is already done in the NifitMasker. How do we reconcile these smoothings (the one in NiftiMasker and the proposed one in path_scores) ?

On Wed, Oct 8, 2014 at 4:16 PM, Gael Varoquaux notifications@github.com
wrote:

> In nilearn/decoding/space_net.py:
> 
> > -        List of regularization parameters being considered.
> >   +
> > -    l1_ratio : float in the interval [0, 1]; optinal (default .5)
> > -        Constant that mixes L1 and TV (resp. Smooth Lasso) penalization.
> > -        l1_ratio == 0: just smooth. l1_ratio == 1: just lasso.
> >   +
> > -    solver : function handle
> > -       See for example tv.TVl1Classifier documentation.
> >   +
> > -    """"""
> >   +
> > -    alphas = sorted(alphas)[::-1]
> >   +
> > -    # univariate feature screening
> > -    mask = mask.copy()
> > -    if screening_percentile < 100.:
> 
> We need to smooth the data that we are passing to the SelectPercentile
> here, using nilearn.image.image._smooth_array with fwm='fast'. PR #258
> https://github.com/nilearn/nilearn/pull/258 will enable you to pass
> affine=None there.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r18584555.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-09 07:49:46","> Smoothing is already done in the NifitMasker.

We need to disable it: we should never smooth when there is a spatial
penalty. Thus, for SpaceNet, we don't smooth, aside in the feature
selection.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-10 03:11:35","Oops, this is a bug (smoothing data on which spatial prior is optimized).
Fixed.

On Thu, Oct 9, 2014 at 9:49 AM, Gael Varoquaux notifications@github.com
wrote:

> In nilearn/decoding/space_net.py:
> 
> > -        List of regularization parameters being considered.
> >   +
> > -    l1_ratio : float in the interval [0, 1]; optinal (default .5)
> > -        Constant that mixes L1 and TV (resp. Smooth Lasso) penalization.
> > -        l1_ratio == 0: just smooth. l1_ratio == 1: just lasso.
> >   +
> > -    solver : function handle
> > -       See for example tv.TVl1Classifier documentation.
> >   +
> > -    """"""
> >   +
> > -    alphas = sorted(alphas)[::-1]
> >   +
> > -    # univariate feature screening
> > -    mask = mask.copy()
> > -    if screening_percentile < 100.:
> 
>  Smoothing is already done in the NifitMasker.
> We need to disable it: we should never smooth when there is a spatial
> penalty. Thus, for SpaceNet, we don't smooth, aside in the feature
> selection.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r18630977.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 20:22:56","Please move the author line to a comment, rather than a docstring line, and add that this is BSD licensed.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_spacenet.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 20:23:36","Please rename this file plot_haxby_space_net.py : CamelCase gets translated to underscore separated, as in camel_case.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_spacenet.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 20:28:40","Is there any reason that you are creating a NiftiMasker here, rather than passing the filename to the mask keyword of SpaceNet?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_spacenet.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 20:29:18","This line is too long.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_spacenet.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 20:57:39","Could you also print in minutes, as in:

<pre>
total_time = time.time() - tic
print(""Time Elapsed: %g seconds, %i minutes.""  % (total_time, total_time / 60))
</pre>
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 21:00:55","Could you please entitle this 'TV-l1 weights', as we are using TV-l1 here.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_spacenet.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 21:05:59","Same remark on the filename: it should be space_net.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_vbm_spacenet.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 21:13:36","Typo: accuracy
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_spacenet.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 21:15:55","Don't we want to keep the best_init here (using a ""copy"") and pass it during the refit, below.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 21:16:48","Should be called early_stopper.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-13 21:18:00","I would prefer if you could use np.empty here.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 05:22:57","there is little logic in the namings here: there is a ""-"" between smooth and lasso, but not tv and l1. I think that I would prefer the parameter to TV-l1 to be ""tv-l1""
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 08:54:47","That test is too stringent. In practice, it pretty much never fires and models are optimized to convergence during the parameter selection period, which takes for ever. I would change it to '>= -1e-4'. This will lead to a great change in run-time for no change in results.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 11:02:05","Here, what you can do is to define an 'inner_mask', which is a cropped version of the mask. It will be used in the solver, and make the computation of the prox operator much faster.

I would do something like this:

<pre>
idx = np.where(mask)
i_min = max(idx[0].min() -1, 0)
i_max = max(idx[0].max(), mask.shape[0] - 1)
j_min = max(idx[1].min() -1, 0)
j_max = max(idx[1].max(), mask.shape[1] - 1)
k_min = max(idx[2].min() -1, 0)
k_max = max(idx[2].max(), mask.shape[2] - 1)
inner_mask = mask[i_min:i_max, j_min:j_max, k_min:k_max]
</pre>


PS: do this outside the 'if screening_percentile...' test, as is it also valid for user-supplied masks.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 11:44:13","I don't understand the purpose of the line above.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-14 11:45:41","Done.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-14 11:46:35","Definitely. Done.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-14 13:12:26","OK. Great code except for an tiny off-by-one error: i_max -> i_max + 1, etc. ;)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-14 13:16:24","For the haxby epi mask, cropping screens-away upto 37% of the original box.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 13:25:40","> OK. Great code except for an tiny off-by-one error: i_max -> i_max + 1, etc. ;)

:) Good catch!
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 13:26:10","> For the haxby epi mask, cropping screens-away upto 37% of the original box.

And that's pretty sweet: factor of 3 improvement in the proximal
operator!
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 13:47:33","The convention is scikit-learn is that a score is always 'bigger is better'. We need to change this test_score to conform to the convention. Elsewhere it's confusing when trying to figure out what is happening in the codebase (I just went through that).
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 14:20:09","I'd like this function to take an explicit verbose parameter, to replace the one that it is currently hiding in the solver_params dictionary. The way it should be handled is that it should be passed raw to the EarlyStoppingCallback, but to the solver, it should be passed as follows:

<pre>
verbose=max(verbose -1, 0)
</pre>


that patterns enables to control the amount of verbosity in a pipeline.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 14:26:09","One things that makes the haxby dataset unusual is that it has a much larger number of samples than most datasets. As a result, the regularization is less important on this example.

What I suggest is to take only a subset of the data for training, and use the rest for testing. You can do that with the following:

<pre>
condition_mask_train = np.logical_and(condition_mask, labels['sessions'] < 10)
condition_mask_test = np.logical_and(condition_mask, labels['sessions'] >= 10)
</pre>


and then define X_train, y_train, X_test, y_test below.

You can then compute the performance of the model on left-out data using the predict method of the model.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 14:49:48","After investigation, the mask returned by the strategy we have here is not sufficiently structured. Thus I suggest, first to change the default percentile to 20% of the brain, and second the following strategy, which works better:

<pre>
sX = ndimage.gaussian_filter(sX, (2, 2, 2, 0))
...
mask[mask] = (support > 0)
mask = ndimage.binary_dilation(mask).astype(np.bool)
</pre>
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-14 17:02:28","This trick imay be good for spatial prior, but then we need to unmask the
design matrix X unto the larger / dilated support (for we always require
mask.sum() == X.shape[1]).

On Tue, Oct 14, 2014 at 4:49 PM, Gael Varoquaux notifications@github.com
wrote:

> In nilearn/decoding/space_net.py:
> 
> > -    n_samples, _ = X.shape
> >   +
> > -    # make local copy of mask
> > -    mask = mask.copy()
> >   +
> > -    # misc
> > -    verbose = solver_params.get('verbose', 0)
> > -    alphas = sorted(alphas)[::-1]
> >   +
> > -    # univariate feature screening
> > -    if screening_percentile < 100.:
> > -        # smooth the data before screening
> > -        sX = np.empty(list(mask.shape) + [n_samples])
> > -        for row in xrange(n_samples):
> > -            sX[:, :, :, row] = _unmask(X[row], mask)
> > -        sX = _fast_smooth_array(sX)
> 
> After investigation, the mask returned by the strategy we have here is not
> sufficiently structured. Thus I suggest, first to change the default
> percentile to 20% of the brain, and second the following strategy, which
> works better:
> 
> sX = ndimage.gaussian_filter(sX, (2, 2, 2, 0))
> ...
> mask[mask] = (support > 0)
> mask = ndimage.binary_dilation(mask).astype(np.bool)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r18833219.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 20:51:06","Yes, indeed. Thus we need to do something like:

<pre>
new_mask = mask.copy()
new_mask[mask] = (support > 0)
new_mask = ndimage.binary_dilation(mask).astype(np.bool)
new_mask[np.logical_not(mask)] = 0
support = new_mask[mask]
</pre>


A little bit more work :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-15 07:05:50","OK. When we're done with this mask wizardry, we'll barely need any spatial
prior whatsoever: we could simply fit a vulgare ridge on the then highly
structured support :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-15 07:07:23","> OK. When we're done with this mask wizardry, we'll barely need any spatial
> prior whatsoever: we could simply fit a vulgare ridge on the then highly
> structured support :)

The important aspect is to select a super-set of the voxels that matter.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-16 09:08:59","I will be much more efficient to do something like:

<pre>
sX = sX[mask]
</pre>
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-16 09:29:07","It should be:

<pre>
      new_mask = ndimage.binary_dilation(new_mask).astype(np.bool)
</pre>
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-16 09:33:16","Could you please:

a. Not keep the return value: it is not used here, so let's not clutter the script

b. Lock the cut_coords to z=-16, y=-38, x=35 so that the images are comparable with other images in the docs.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-16 11:18:32","Thanks to the use of the label binarizer, we don't need to convert to numerical targets. Thus, I'd prefer if the code of this part of the file, was rewritten as follow:

<pre>
target = labels['labels']
condition_mask = np.logical_or(target == ""face"", target == ""house"")
condition_mask_train = np.logical_and(condition_mask, labels['chunks'] <= 9)
condition_mask_test = np.logical_and(condition_mask, labels['chunks'] > 9)


# make X (design matrix) and y (dependent variate)
import nibabel
niimgs  = nibabel.load(data_files.func[0])
X_train = nibabel.Nifti1Image(niimgs.get_data()[:, :, :, condition_mask_train],
                        niimgs.get_affine())
y_train = target[condition_mask_train]
X_test = nibabel.Nifti1Image(niimgs.get_data()[:, :, :, condition_mask_test],
                        niimgs.get_affine())
y_test = target[condition_mask_test]
</pre>
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 12:38:44","What is the question mark for?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(79, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 12:43:02","I'd prefer being very clear: ""breaking -> terminating the optimization loop"". And qualify what is meant by ""meaning"" by naming two ways it can be employed.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 12:44:50","Are we supposed to use double/single backticks etc for the docs or not? I have seen them in several places, but not consistently.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-17 12:58:31","> Are we supposed to use double/single backticks etc for the docs or not? I have
> seen them in several places, but not consistently.

Only if there is an underscore at the end of the variable name, as
without the backticks, it is interpreted as a link.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 13:03:12","descendent -> descending
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 13:03:59","it doesn't default, it is set.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-17 13:11:47","You're looking at a typo. Thanks for catching.

On Fri, Oct 17, 2014 at 2:38 PM, eickenberg notifications@github.com
wrote:

> In nilearn/decoding/fista.py:
> 
> > -            b = lipschitz_constant \* linalg.norm(x - y, 2)
> > -            assert a <= b, err_msg + (""(a = %g >= %g)"" % (a, b))
> >   +
> >   +
> >   +def mfista(f1_grad, f2_prox, total_energy, lipschitz_constant, w_size,
> > -           dgap_tol=None, init=None, max_iter=1000, tol=1e-4,
> > -           check_lipschitz=False, dgap_factor=None, callback=None,
> > -           verbose=2):
> > -    """"""
> >   +
> > -    Parameters
> > -    ----------
> > -    f1_grad : callable(w) -> np.array
> > -        Gradient of smooth part of energy
> >   +
> > -    f2_prox : callable(w, stepsize, dgap_tol, init?) -> float, dict
> 
> What is the question mark for?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r19014904.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(79, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 13:15:01","Shouldn't we prefer pre-allocating space for `w_old`, then setting it inplace using `w_old[:] = w` in order to avoid re-allocating heap memory in different places? Or is my thinking deprecated and old-school? :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 13:19:34","Why is `gradient_buffer` initialized above, if its pointer is overwritten here? Either update inplace `gradient_buffer[:] = f1_grad(z)` or don't initialize. (modulo my missing something between the initialization line and here)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-17 13:26:34","Good point. This is garbage (the initialization).

On Fri, Oct 17, 2014 at 3:19 PM, eickenberg notifications@github.com
wrote:

> In nilearn/decoding/fista.py:
> 
> > -        objective.append(old_energy)
> > -        w_old = w.copy()
> >   +
> > -        # Invoke callback.
> > -        if verbose:
> > -            print 'mFISTA: Iteration % 2i/%2i: E = %7.4e, dE % 4.4e' % (
> > -                i + 1, max_iter, old_energy, energy_delta)
> > -        if callback and callback(locals()):
> > -            break
> > -        if np.abs(energy_delta) < tol:
> > -            if verbose:
> > -                print ""\tConverged (|dE| < %g)"" % tol
> > -            break
> >   +
> > -        # Forward (gradient) step
> > -        gradient_buffer = f1_grad(z)
> 
> Why is gradient_buffer initialized above, if its pointer is overwritten
> here? Either update inplace gradient_buffer[:] = f1_grad(z) or don't
> initialize. (modulo my missing something between the initialization line
> and here)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r19016663.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-17 13:28:08","No, w.copy() is just fine.

On Fri, Oct 17, 2014 at 3:15 PM, eickenberg notifications@github.com
wrote:

> In nilearn/decoding/fista.py:
> 
> > -    energy_delta = np.inf
> > -    best_w = w.copy()
> > -    best_energy = old_energy
> > -    best_dgap_tol = dgap_tol
> > -    ista_step = False
> > -    best_z = z.copy()
> > -    best_t = t
> > -    best_dgap_tol = dgap_tol
> > -    prox_info = dict(converged=True)
> > -    stepsize = 1. / lipschitz_constant
> > -    objective = []
> >   +
> > -    # FISTA loop
> > -    for i in xrange(int(max_iter)):
> > -        objective.append(old_energy)
> > -        w_old = w.copy()
> 
> Shouldn't we prefer pre-allocating space for w_old, then setting it
> inplace using w_old[:] = w in order to avoid re-allocating heap memory in
> different places? Or is my thinking deprecated and old-school? :)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r19016430.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 13:39:03","Could we have a comment saying `Ideally this loop is broken after one single iteration. It only loops if the precision of the proximal operator is too low, as measured by a non-decrease of the energy functional after the energy-decreasing prox operator. If the maximum of 10 loops is reached, then the dual gap precision is decreased by a factor of (.2) ** 10`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 13:42:29","similarly as above: Would old-school memory house-keeping be in order?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 13:44:13","as above (memory house keeping?)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 13:46:27","Lipschitz
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 13:48:50","curious mixture of mathematical arrow notation for functions and computer notation (.5)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 13:51:45","`np.ones((X.shape[0], 1))`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:04:03","Where else is this used? Seems not so straight forward to me. A reference, if available, would be cool. Or a quick outline as to why this should be so in the docstring. (I always had the feeling that the lipschitz constant of the gradient of the logistic loss was quite an issue)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","lesteve","2014-10-17 14:07:12","I would naively agree with @eickenberg on principles. I ran a quick and dirty timing comparison which seems to indicate the same thing although it does depend of the size of the loop and the shape of the array you are copying.

To get a better idea whether it matters in your case, I can rerun it if you provide me with a typical size of max_iter and a typical shape of w.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:10:22","An `einsum` on the raveled `spatial_grad[:-1]` would save a copy
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(123, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:19:32","@dohmatob any arguments as to why? What are the time scales of the necessary operations these days?
@lesteve a `w` instance would contain a brain image, potentially masked, but I am not sure at this point. So its shape can go from around `(10, 40, 40)` to `(100, 100, 100)`. As for `max_iter`, I am guess a couple of hundreds, but @dohmatob has more experience with that.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:28:53","any decision here? If `np.float` is decided, then I'd prefer being precise about which type.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:30:44","`ndarray`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:41:14","Is ravelling necessary? Could we force a view?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:48:15","Specify that they must be in {-1, 1}
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:55:52","Sorry, it's an upper bound. OK. The difficulty was elsewhere and is not solved by calculating this exactly. But still, maybe it can be replaced by using the operator norm (\infty, 2)?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:57:50","Are static lambda expressions ever useful? You are naming them into a variable. Why should they stay anonymous?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:58:09","TV-L1
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:58:43","[Citation needed]
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-17 14:59:42","complete
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-18 14:51:50","is this tuple valid python ? ;)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-18 14:53:53","actually, in 3 lines above the thing is named in 3 ways
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-18 14:55:27","the docstring says that a 1d array must be input. so the thing to do would be to verify this condition instead of ravelling or making a 1d view
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-18 15:28:33","This is too verbose. There are already a couple of comments baring the same pedagogy.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-18 15:29:50","NO. BTW, what do you mean by ""old-school memory house-keeping""?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-18 15:36:06","Read this: http://www.csie.ntu.edu.tw/~cjlin/papers/l1_glmnet/supplement.pdf
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-18 15:58:42","Done.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-18 16:12:28","Oops! Indeed, we're in a loop, and so *.copy() is not ""just fine"", as the overhead will become apparent for large ""max_iter"". Good catch.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-18 16:14:07","OK see updated comment above, on the same issue.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:18:04","X_std[X_std == 0.] = 1.

these are floats
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(40, '', u'nilearn/_utils/fixes/sklearn_basic_backports.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:19:19","in closed-form

in is missing
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-19 16:21:08","while we're at it:

aN inner loop

closed form

On Sunday, October 19, 2014, Alexandre Gramfort notifications@github.com
wrote:

> In nilearn/decoding/fista.py:
> 
> > @@ -0,0 +1,241 @@
> > +""""""
> > +Generic FISTA for solving TV-l1, S-Lasso, etc., problems.
> > +For problems on which the prox of the nonsmooth term cannot be computed
> > +closed-form (e.g TV-l1), approximate it using a inner FISTA loop.
> 
> in closed-form
> 
> in is missing
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r19061211.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:21:46","assert is not always evaluated. Eg. some production python code.
I would raise a RuntimeError
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:22:21","Docstring headline missing

see pep257
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:31:09","I don't really like the term energy. How about cost?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(84, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:32:07","@GaelVaroquaux are ok with using cost and not energy? it sounds too much like physics for me :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:34:30","why int(max_iter) ? max_iter is supposed to be an int already.

also what is the policy towards py3k? because of xrange.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:34:45","print uses () on py3k
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:36:07","note for later. Reviewer is (almost) always right and if to make him happy you need to type 2 lines just do it :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:39:59","The objective is
or
The cost is
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:40:35","Numpy standard is:

X : ndarray, shape (n_samples, n_features)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:40:45","y : ndarray, shape (n_samples,)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:41:08","w : ndarray, shape (n_features,)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:41:56","I don't like asserts :) I prefer proper exceptions
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:43:29","what shape?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:43:41","l1 -> L1
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:44:53","please check docstring formatting
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-19 16:47:22","Fixed th max_iter stuff.
For the moment there's no py3 anticipation. There are other non py3"" stuff like `print ""Hallo!""` lurking in the code base.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:48:22","(gap *\* 2).sum() -> np.dot(gap, gap)

?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:49:25","sometimes you write l1 and sometimes L1. Please be consistent in docstring and use L1.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-19 16:49:28","Personally, I'm fine with the (equivalent) terms energy, cost, etc. If I get another +1 for this, then I'll change it.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(84, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:50:16","Some param description start with a capital some don't. Please be consistent.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(101, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:51:31","np.dot(input_img.ravel(), input_img.ravel())

is faster as it avoids a temp array.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 16:55:19","The docstring is certainly wrong. L2 does not apply to TV-L1 etc.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:00:58","this block of code is not tested. cf. coverage report
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(180, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:02:28","TVl1-> TV-L1
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:03:05","weigghts -> weights
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:04:21","tvl1 -> tv-l1
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:04:59","shape = [n_samples, n_features] -> shape (n_samples, n_features)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:05:10","idem for shape formatting
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:06:45","fix docstring
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:07:11","fix docstring
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:07:20","idem
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:08:09","no random state?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:08:47","when you compute sqrt on float use math.sqrt and not np.sqrt. Same remark applies above.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:09:53","info of what?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:10:14","params are not documented
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:10:39","params are not documented. Same remark with info below
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(304, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-19 17:10:49","gap is a 3D array.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:11:20","some functions are private some not. I vote to make as many functions private as possible.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(361, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:11:32","returns?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:11:43","docstring missing
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:12:28","don't talk about length but shape

y : 1D array, shape (n_samples,)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-19 17:12:46","But yep, i can ravel and then dot. thx
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:14:07","hum some parameters are missing

returns are not documented
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(18, '', u'nilearn/decoding/tests/simulate_smooth_lasso_data.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:14:22","docstring
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(73, '', u'nilearn/decoding/tests/simulate_smooth_lasso_data.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:15:50","invalide -> invalid
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:16:02","why trailing _?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(80, '', u'nilearn/decoding/tests/test_objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-19 17:16:03","better to force a non-copying view than to ravel
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:16:17","ok got it
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(80, '', u'nilearn/decoding/tests/test_objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:16:55","assert_equal with nottest above
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-19 17:16:55","unless we can guarantee C ordering, ravel may make copies ...
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:18:50","can you explicit what you're testing?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:19:43","XXX to be done?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(207, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:20:22","if you import assert_array_equal on top you can avoid all the np.testing.

we do this in sklearn
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(23, '', u'nilearn/decoding/tests/test_sklearn_basic_backports.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 17:20:46","don't use relative imports in tests

fix import order
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_sklearn_compatibility.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 19:37:08","we don't use relative imports in tests
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 19:39:15","use assert_true
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 19:39:53","assert_true
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 19:40:37","0.

it's a float
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 19:46:43","clarify what you're testing
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 19:47:33","what is the difference between @ notest and @ SkipTest?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 19:48:58","insert _ between words
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 19:50:46","functions above are named tvl1 and here it is called tv_l1
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-19 19:51:38","you call it smooth-lasso and below you say tv-l1 weights
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-19 20:57:41","@agramfort this typo was fixed a long time ago.

On Sun, Oct 19, 2014 at 9:51 PM, Alexandre Gramfort <
notifications@github.com> wrote:

> In plot_haxby_space_net.py:
> 
> > +_, target = np.unique(labels['labels'], return_inverse=True)
> > +
> > +# make X (design matrix) and y (response variable)
> > +import nibabel
> > +niimgs  = nibabel.load(data_files.func[0])
> > +X_train = nibabel.Nifti1Image(niimgs.get_data()[:, :, :, condition_mask_train],
> > -                        niimgs.get_affine())
> >   +y_train = target[condition_mask_train]
> >   +X_test = nibabel.Nifti1Image(niimgs.get_data()[:, :, :, condition_mask_test],
> > -                        niimgs.get_affine())
> >   +y_test = target[condition_mask_test]
> >   +
> >   +
> >   +### Fit and predict ##########################################################
> >   +from nilearn.decoding import SpaceNet
> >   +decoder = SpaceNet(memory=""cache"", is_classif=True, penalty=""smooth-lasso"",
> 
> you call it smooth-lasso and below you say tv-l1 weights
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r19062860.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 08:20:28","as discussed, but I am unsure: wouldn't you want to subtract the intercept from `self.y_test` before this dot product?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 08:26:47","`self.ymean` is supposed to hold the intercept, which is added to y_pred before the score is computed. So, yes the intercept is being subtracted.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:44:22","are the two above comments necessary?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_vbm_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:44:27","Can we have the `""%""` in the preceding string as `""%%""`? Otherwise the string interpolation in unnecessary and should also be done with `+`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:44:36","Is this copy necessary? `squared_loss` doesn't modify the data, right?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(21, '', u'nilearn/decoding/tests/test_tv.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:44:45","What is not working here yet?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(221, '', u'nilearn/decoding/tests/test_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:44:51","Comparison of coefs impossible, even with `assert_array_almost_equal`?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(215, '', u'nilearn/decoding/tests/test_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:44:57","Are these loops to be extended or removed?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:45:02","_at_ some point? (not completely understanding)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:45:24","Maybe this test can be revived using `assert_array_almost_equal` using a lax `decimal=` kwarg. But that depends on how bad the differences are when using early stopping. If the smooth lasso with `l1_ratio=1.` and the TV-L1 
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:45:38","maybe ""are equal to zero""
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(227, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:45:44","the test depend **s**

Preference on impersonal formulation - ""According to/In the paper, the authors calculate the norm of `starting_point - optimum`""
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(219, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:45:50","objective
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:45:57","can you motivate this choice? (for future understanding)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(184, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:46:01","`of of` -> `of`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:46:08","Sometimes there are comments, sometimes there are docstrings. It would be better to have the same everywhere.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(163, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:46:13","remove `np.testing`, `assert_almost_equal` is imported
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(124, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:46:19","adjointness

And end the sentence with a `.`.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(83, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:46:21","remove space btw `-` and `xdotAty`

Also end docstring with .
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(79, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:46:26","prefer ""matrix form"", end with .
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(60, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:46:30","I'm fine with creating these global variables, but it does come with risks.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(36, '', u'nilearn/decoding/tests/test_smooth_lasso.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:46:47","import `assert_array_almost_equal`.

OK, here you check equality of `coef_`, so maybe my comment below is obsolete.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(163, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:46:52","import `assert_array_almost_equal` directly
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(130, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:46:58","import `assert_array_equal`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(84, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:47:00","`np.ones(X.shape[1], dtype=np.bool)`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:47:06","it would probably be good to import `assert_array_equal` directly.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(72, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:47:12","`assert_true`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:47:15","you have imported `check_random_state`, so you could use it here, too, preferring the numpy rng.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:47:20","functions
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(3, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:47:25","cool test!! `strong` -> `firm` for most
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:47:32","probably also _base_estimator_ (CamelCase -> snake_case)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:47:38","Shouldn't this method rather have a leading underscore, so that it isn't called twice with `size=5` ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(82, '', u'nilearn/decoding/tests/test_objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:47:44","`(img.ndim + 1,) + img.shape` should work as well and save some symbols.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(36, '', u'nilearn/decoding/tests/test_objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:47:59","you can use `gaussian_filter` here once with `sigma=(0,) + (smooth_X,) * 3`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(43, '', u'nilearn/decoding/tests/simulate_smooth_lasso_data.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:48:10","whereas in smooth_lasso there are comments of the type ""better to make them slightly bigger"", there are none here. It should be the same in both algorithms.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(508, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:48:14","Lipschitz
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:48:19","You are using `lambda` here vs `def` above. Change this too?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:48:55","square -> squared
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:49:05","If, as the doctring indicates, we know that we are in 4D, a copy could be saved using `np.sum(np.sqrt(np.einsum(""ijkl, ijkl -> jkl"", gradient[:-1])))`. But this may or may not be relevant in a benchmark.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(372, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:49:16","the factor is different here from above for squared loss. It would be good if the comment could state this.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(332, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:49:25","all the lambdas ... I'd sincerely prefer `functools.partial` or good old `def`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:49:32","Remove or shorten
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(268, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:49:44","meh - if I may: This function name sounds like it kind of stops the `prox_l1` from doing something somewhere in the middle. `prox_l1_skip_intercept` ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:50:21","specify `n, p` in design matrix shape above. Maybe be slightly more explicit than {2|3} :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:50:29","how are these stacked? If I am reading correctly this is not matlab notation. In which case, specify notation.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(127, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:50:35","Mixed matlab arts notation? :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(92, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:50:57","full stop . :) (PEP 257)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 09:51:05","`g2_weight` -> `grad_weight` ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-20 09:51:32","einsum can be quite slow. I would bench first.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(372, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","agramfort","2014-10-20 09:52:12","i would say ""square""
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 10:49:07","agreed.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(372, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 10:51:20","I did too, until I looked it up in wikipedia, but they are probably both OK. (If you see error==residual, then it is ""squared"" with a d)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:03:35","Do you mean in the line below (186)? Is `self.y_test` centered when it comes into `__init__` ? My comment is not about the computation of the score, but about the debiasing itself, but I am failing to gather all the information from the context (maybe it is just fine due to prior calculations): My worry is that `np.dot(y_pred, self.y_test)` may be quite different from `np.dot(y_pred, self.y_test - m)`, where `m` is e.g. the intercept accompanying `w` -- (in short: you wouldn't want to debias against an offset variable).
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:04:14","between 

(Also I really like the idea of Spearman correlation here)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:09:29","as much as I tend to like writing things like this myself, could we have

```
end = grad.shape[0] - 1
if l1_ratio  == 0.:
    end += 1
```
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(40, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:10:13","efficiently
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:11:14","don't change (see @agramfort s comment below)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(123, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:21:06","unless C ordering guaranteed, there is a risk of a copy here (but it should mostly be C-ordered). Can we still have that the reshaping logic happens in the `total_energy` function? It feels more consistent.

Also somehow my previous comment on the legibility of this loop has disappeared. Since I have parsed it now, I cannot comment anymore on how simple or difficult this is, but my first reading would have been better guided by a short explanatory text before the loop.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:21:59","Cater?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/_utils/fixes/sklearn_basic_backports.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:22:51","Dopgima missing :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(5, '', u'nilearn/_utils/fixes/sklearn_basic_backports.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:32:10","raise?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:32:17","raise?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:33:06","what exactly is actually Chambolle?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:37:21","Ensure C ordering

or ravel only once

or

`input_img_flat = input_img.view();
input_img_flat.shape = input_img.size,;
input_img_norm = np.dot(input_img_flat, input_img_flat)`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:39:44","Careful
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:48:46","@GaelVaroquaux do we need to clip in any of this code? Seems to me like some positivity constraint stuff that was used in other contexts, but doesn't seem to be called on here. (Certain future use or out?)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(180, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 11:49:13","other part of clipping logic here
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(218, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 11:58:16","This is a legacy test case. Yes it can be greatly simplified but I won't enjoy the pleasure of rewriting it.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(43, '', u'nilearn/decoding/tests/simulate_smooth_lasso_data.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 12:01:06","OK
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(43, '', u'nilearn/decoding/tests/simulate_smooth_lasso_data.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 12:01:40","too verbose :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(40, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 12:04:48","No, there is apparent bad naming here. It ought to be something like
`prox_l1_with_intercept`.

On Mon, Oct 20, 2014 at 11:49 AM, eickenberg notifications@github.com
wrote:

> In nilearn/decoding/space_net_solvers.py:
> 
> > +
> > -    a = np.random.randn(X.shape[1])
> > -    a /= sqrt(np.dot(a, a))
> > -    grad_buffer = np.zeros(mask.shape)
> > -    for _ in xrange(n_iterations):
> > -        grad_buffer[mask] = a
> > -        a = - div(gradient(grad_buffer))[mask] / sqrt(np.dot(a, a))
> >   +
> > -    grad_buffer[mask] = a
> > -    grad_constant = (- np.dot(div(gradient(grad_buffer))[mask], a)
> > -                     / np.dot(a, a))
> >   +
> > -    return data_constant + grad_weight \* grad_constant
> >   +
> >   +
> >   +def intercepted_prox_l1(x, tau):
> 
> meh - if I may: This function name sounds like it kind of stops the
> prox_l1 from doing something somewhere in the middle.
> prox_l1_skip_intercept ?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r19075936.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 12:35:49","Nice proposal.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 12:37:33","Superfluous brackets?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 12:43:53","Commentary could be more explicit here: e.g. ""Zeroing both spatial-prior penalty weights should yield identical results""
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(148, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 12:45:48","Yes. Good catch.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 12:57:39","This might be a good place to briefly render explicit the relation between ""L1"" (refers mostly to the L1-Norm penalty, AFAIK, independent of a specific optimization problem) and ""Lasso"" (refers to the regression problem including regularization by L1-Norm, AFAIK). I feel that terms such as ""Lasso"" and ""Ridge"" are jargon terms that are difficult to investigate for non-machine-learning folks. More concretely, on could at least mention that ""TV-L1 and Smooth-Lasso have the L1 penalty in common but differ in the type of spatial prior."" or something like that
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 13:43:45","Perhaps give complete citation by Beck & Teboulle here (including page numbers, journal and year of publication)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 13:45:00","the ref sounds colloquial this way
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 13:48:56","This explanation is self-referential: Blah_intercept ...takes into the intercept.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(266, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 13:49:22","typo: i.e.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 13:51:30","Again, pretty self-referential explanation. In philosophy, this would not qualify as a definition.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 13:59:54","This is statement is void as such. Which type of discourse are you referring to?

(Also, I think the principle is to have a full description of the function in the docstring, without implicating the function name)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 14:10:45","It's not necessary to nest ndimage.binary_dilation/erosion because it could be in one step by ndimage.binary_closing/fill_holes
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 14:10:57","Yes, they're like pedagogic subtiles (what's happening in each portion of the script) to the user.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_vbm_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 14:14:47","sure that you did not mean ""verbose=0""?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 14:18:41","OK - yes, better more than less. Although for once I thought it was clear ;) since those exact words are the actual names of the commands used. (I am talking about lines 21 and 23)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_vbm_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 14:26:38","""structured (i.e., respective spatial prior)"" and ""sparse (i.e., enforced by L1 norm)""
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 14:27:11","final point missing
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","lesteve","2014-10-20 14:27:13","Also for completeness, these comments are used as targets to include excerpts from example scripts in the documentation (probably not in this case I am guessing since this is a new example script).

For example here is the [html doc](http://nilearn.github.io/data_analysis/decoding.html#loading-the-data-into-python) and the [rst](https://raw.githubusercontent.com/nilearn/nilearn/master/doc/data_analysis/decoding.rst). You'll see that the comments are used as start-after and end-before targets for the literalinclude.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_oasis_vbm_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-20 14:29:12","Again, TV (a penalty) is mixed up by juxtaposition with S-Lasso (more than a penalty) -> Adds to confusion for new people in the field
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 14:33:08","Quality comment (cf. other comments thus far ;) )
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-10-20 14:34:35","True. It should be S-LASSO -> L2-smoothness (or something to that effect)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 16:59:50","@eickenberg  @banilo : Gentlemen, could you be less superficial and impertinent in your comments ? This is a headliner saying in the most succint words, what the (private low-level) function is doing. It's by no means an _ouroboros_. It's a pity, such an improbably heavy bail of poor reviews drown a few interesting comments which actually have a point to make. I know it's a delicate art to make good reviews, but if you can make an effort,  then we can see some better reviews here. Looking forward to this.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 17:47:11","Lambda is just fine, and is an incredible convenience in such matters. Also, Lambda reminds us of the Lispy origins of python, which most people seem to have forgotten, or completely ignore. Lambda is your f(r)iend. AC would agree.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-20 19:01:20","Well, actually lambda is discouraged. It gives code that is hard to read.

I would strongly prefer if the use of lambda could be minimized.

And nobody care about lisp anymore :P
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 19:14:35","OK, it's sad to see _Lisp_ end this way. I'll hereby promise to make a tremendous effort to mitigate my use of the _Lambda_ in the codebase henceforth :)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-20 19:17:03","Good point. I'll look at this asap.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(508, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-21 06:46:48","Good point. Fixed.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","lesteve","2014-10-22 09:14:35","li_p_schitz
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","lesteve","2014-10-22 09:20:00","I thought the right way to do it was to pass a  `random_state` argument to the function
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-22 09:26:47","It's all about making the test reproducible using a local
random-number-generator, without unnecessary sophistications :)

On Wed, Oct 22, 2014 at 11:20 AM, Loïc Estève notifications@github.com
wrote:

> In nilearn/decoding/fista.py:
> 
> > +
> > -    n_trials : int,
> > -      Number of tests performed when assessing the Lipschitz continuity of
> > -      function `f`. The more tests, the more confident we are in the
> > -      Lipschitz continuity of `f` if the test passes.
> >   +
> > -    err_msg : {str, or None},
> > -      String used to tune the output message when the test fails.
> > -      If `None`, we'll generate our own.
> >   +
> > -    Raises
> > -    ------
> > -    RuntimeError
> > -    """"""
> >   +
> > -    rng = check_random_state(42)
> 
> I thought the right way to do it was to pass a random_state argument to
> the function
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r19202162.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-22 09:27:11","Thanks. Fixing right away.

On Wed, Oct 22, 2014 at 11:14 AM, Loïc Estève notifications@github.com
wrote:

> In nilearn/decoding/fista.py:
> 
> > -    Lipschitz continuity of the function with respect to the given
> > -    constant `L`. This confidence increases with the `n_trials` parameter.
> >   +
> > -    Parameters
> > -    ----------
> > -    f : callable,
> > -      The function to be checked for Lipschitz continuity.
> > -      `f` takes a vector of float as unique argument.
> > -      The size of the input vector is determined by `ndim`.
> >   +
> > -    ndim : int,
> > -      Dimension of the input of the function to be checked for Lipschitz
> > -      continuity (i.e. it corresponds to the size of the vector that `f`
> > -      takes as an argument).
> >   +
> > -    lischitz_constant : float,
> 
> li**p**schitz
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r19201962.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","lesteve","2014-10-22 09:32:25","I don't understand the logic at play here.
1. If no `err_msg` argument is provided, it seems to me that in err_msg the `x` and `y` will be set to the first x and y in the loop rather to the ones that don't satisfy the constraint, potentially yielding to a misleading error message. In other words, I would move this statement in the `if a > b:` block below.
2. I don't really get the point of being able to provide an err_msg as an argument to be honest.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","lesteve","2014-10-22 09:34:28","I would use Lipschitz Continuity fully spelled out (or whatever LC stands for) for clarity reasons.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-22 09:44:41","This is the fingerprint of a piece of code that once lived on planet _parietal-python_. Good catch. Fixed.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","lesteve","2014-10-22 09:44:43","But this function is not a test or am I missing something? Basically I am just advocating following http://scikit-learn.org/stable/developers/#random-numbers.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-22 11:03:27","It's not a test, it's a check, but my remark still applies I believe.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-22 12:57:47","Loic's remark was a good one. His suggestion is a tiny modification, it's standard practice, and it enables varying a parameter that should really be independent of the test.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-22 13:01:41","I don't mind implementing this. My point was I don't see what added value
this modification has, appart from being a ""'good"" practice.

On Wed, Oct 22, 2014 at 2:57 PM, Gael Varoquaux notifications@github.com
wrote:

> In nilearn/decoding/fista.py:
> 
> > +
> > -    n_trials : int,
> > -      Number of tests performed when assessing the Lipschitz continuity of
> > -      function `f`. The more tests, the more confident we are in the
> > -      Lipschitz continuity of `f` if the test passes.
> >   +
> > -    err_msg : {str, or None},
> > -      String used to tune the output message when the test fails.
> > -      If `None`, we'll generate our own.
> >   +
> > -    Raises
> > -    ------
> > -    RuntimeError
> > -    """"""
> >   +
> > -    rng = check_random_state(42)
> 
> Loic's remark was a good one. His suggestion is a tiny modification, it's
> standard practice, and it enables varying a parameter that should really be
> independent of the test.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r19211055.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-22 13:11:26","> I don't mind implementing this. My point was I don't see what added value this
> modification has, appart from being a ""'good"" practice.

The choice of 42 is arbitrary. To make a check more robust, one might
want to marginalize it.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-22 13:22:50","These are far-fetched considerations and I'm still not convinced :) In any
case, I've implemented the _random_state_ arg so we can move on.

On Wed, Oct 22, 2014 at 3:11 PM, Gael Varoquaux notifications@github.com
wrote:

> In nilearn/decoding/fista.py:
> 
> > +
> > -    n_trials : int,
> > -      Number of tests performed when assessing the Lipschitz continuity of
> > -      function `f`. The more tests, the more confident we are in the
> > -      Lipschitz continuity of `f` if the test passes.
> >   +
> > -    err_msg : {str, or None},
> > -      String used to tune the output message when the test fails.
> > -      If `None`, we'll generate our own.
> >   +
> > -    Raises
> > -    ------
> > -    RuntimeError
> > -    """"""
> >   +
> > -    rng = check_random_state(42)
> 
>  I don't mind implementing this. My point was I don't see what added value
> this modification has, appart from being a ""'good"" practice.
> The choice of 42 is arbitrary. To make a check more robust, one might want
> to marginalize it.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r19211812.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-22 13:28:51","Please remove the word novel here. Some people find it annoying to use that word too often.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-10-29 12:57:16","Sorry, but I can barely see any difference between smooth-lasso and TV-l1. Is there something wrong ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 21:02:24","Please move to cost.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(84, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 21:27:21","repeats line 153
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 21:27:40","repeated line ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 21:30:45","ndarray, shape (w_size,)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 21:31:25","maybe: solution of the optimization problem
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/fista.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 21:33:42","compute_cost
(in the doc above, you called this curve). A discussed by @agramfort energy as a peculiar connotation that is not useful here.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(74, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 21:34:03","cost
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(82, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 21:35:17","ndarray, shape (4, nx, ny, nx)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 21:48:49","To me, such simplified docstrings are OK for private functions. Should we have those private ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-11-02 21:51:45","> To me, such simplified docstrings are OK for private functions. Should we have
> those private ?

I think that this is a good idea.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:00:50","ndarray, shape (w_size,)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:01:15","same here
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/proximal_operators.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:03:21","This is a bit large... We will probably have to update it. Never mind, this is not essential obviously.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(37, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:09:18","This is a subset of the mask
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:16:33","Is the use of Spearman standard here ? Or simply common sense, to accomodate losses more general than least squares ? 
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(205, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:19:43","we should use
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:20:29","100, not 50
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:28:42","If True, the data (X,y) are centered in order to have mean zero along axis 0.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(1160, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:30:28","Could this be a bit more explicit: you're talking about change across successive estimates ?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(1171, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:32:04","remove  'the' at the end of the line
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:33:10","ndarray of shape (nclasses - 1)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:36:27","should be relative to the provided mask. The volume of standard brain should only be indicative. 
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(594, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:38:37","/ 1000. (same below)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:40:56","range(n_problems)
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:47:10","I don't like this. One aspect is that ""the volume of the MNI 152 brain"" is very accurately defined. Second,  the user may have a hard time guessing what is actually done (some datasets are not full-brain, henc ethe user will have to figure out what fraction of MNI space their mask represents). I think that the setting is clearer is the percentile is wrt the provided mask.  
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(995, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:54:55","Same comment here on private functions and non-developed dosctrings.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(157, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 22:57:28","to solve
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-02 23:02:18","to solve
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(407, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-03 08:43:14","Please describe in a couple of sentences what the simulated data look like.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(2, '', u'nilearn/decoding/tests/simulate_smooth_lasso_data.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-03 08:55:04","_loss_
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-03 08:55:35","_loss_
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/tests/test_same_api.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-11-03 09:25:19","> I don't like this. One aspect is that ""the volume of the MNI 152 brain""
> is very accurately defined. Second, the user may have a hard time
> guessing what is actually done (some datasets are not full-brain, henc
> ethe user will have to figure out what fraction of MNI space their mask
> represents). I think that the setting is clearer is the percentile is
> wrt the provided mask.

I disagree. I want the measure to be absolute so that we can set a
reasonable default. Things should work out of the box.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(995, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-11-03 11:52:28","Maybe I'm wrong, but that basically excludes all ROI based analyses from benefitting from sensible out-of-the-box settings, right? (I am not mentioning single subject analyses as an argument, but one may want to accommodate those as well in the future.) As @bthirion says, a _volume_ percentile of a specified mask seems as sensible to me as selecting a percentile can get in the context of this algorithm.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(995, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-11-03 12:19:03","Both views are certainly reasonable and just as _ad hoc_ as one another. I
think there is room for this to evolve in future, and we should leave it at
that for now (and see how it goes). BTW,  issue #265 (
https://github.com/nilearn/nilearn/issues/265) was created to track this
discussion.

On Mon, Nov 3, 2014 at 12:52 PM, eickenberg notifications@github.com
wrote:

> In nilearn/decoding/space_net.py:
> 
> > -        documentation for details
> >   +
> > -    high_pass : False or float, optional (default None)
> > -        This parameter is passed to signal. Clean. Please see the related
> > -        documentation for details
> >   +
> > -    t_r : float, optional (default None)
> > -        This parameter is passed to signal.clean. Please see the related
> > -        documentation for details
> >   +
> > -    screening_percentile : float in the interval [0, 100]; Optional (
> > -    default 20)
> > -        Percentile value for ANOVA univariate feature selection. A value of
> > -        100 means 'keep all features'. This percentile is is expressed
> > -        w.r.t the volume of a standard (MNI152) brain, and so is corrected
> > -        at runtime by premultiplying it with the ratio of the volume of the
> 
> Maybe I'm wrong, but that basically excludes all ROI based analyses from
> benefitting from sensible out-of-the-box settings, right? (I am not
> mentioning single subject analyses as an argument, but one may want to
> accommodate those as well in the future.) As @bthirion
> https://github.com/bthirion says, a _volume_ percentile of a specified
> mask seems as sensible to me as selecting a percentile can get in the
> context of this algorithm.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/pull/219/files#r19728797.

## 

DED
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(995, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-03 12:37:44","You're right that this is not relevant for SpaceNet. Let's handle this 
later.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(995, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-11-03 17:52:00","redundant: default parameter for smooth is given here and a few lines down under ""Parameters""
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-11-03 17:52:17","""on"" is repeated here
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-11-03 17:52:22","superfluous bracket
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-11-03 19:00:04","> Maybe I'm wrong, but that basically excludes all ROI based analyses from
> benefitting from sensible out-of-the-box settings, right? (I am not mentioning
> single subject analyses as an argument, but one may want to accommodate those
> as well in the future.)

Quite on the contrary. With the settings being a percentile of the voxels
(which is basically what you are advocating), when doing ROI-based
analysis, the default give meaningless selections. For instance selecting
40 voxels. This is exactly what happened earlier, and what prompted me to
ask for this specific implementation.

> As @bthirion says, a volume percentile of a specified mask seems as
> sensible to me as selecting a percentile can get in the context of this
> algorithm.

Honestly, please no. Think about it. If people specify a small mask, as
in the Haxby dataset, and leave the default settings (which 90% of the
users will), they will get nonsense. And it's not like it's nonsense that
is easy to diagnose.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(995, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-11-03 19:00:45","> redundant: default parameter for smooth is given here and a few lines down
> under ""Parameters""

Yes, that's the numpy doc standard.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-11-04 09:16:31","Yes it is of great convenience. There are probably other ways to go about this, but this is clean.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(205, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-11-04 09:27:10","See on-going discussion about volume-corrected percentile, etc.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(594, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-11-04 09:50:31","I guess this is related to python3, etc. OK changing this. BTW, issue #274 has is open for discussing py3 stuff.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-11 11:28:15","There is an issue with the default alpah_min (1.e-6): in both cases, the decoder selects the highest penalty. I have replayed it with a higher alpha_min: 1.e-4 and obtain clear differences between the two decoders.
The heuristic to set the alpha grid/ choose alpha_min should be revisited ? 
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'plot_haxby_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","bthirion","2014-11-14 13:59:02","The TV+L1 term is bit surprising here, given that this function computes image gradients that are not necessarily related to spatial penalties but can be used in other context. I think that this holds for all functions in this module.
 To some extent, this also holds for the 'l1_ratio' term, but I have no great idea on how to replace it.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-11-14 14:30:56","Indeed. Fixed. commit e91a66d1535e67c0a819b88bdde226081267be70
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/objective_functions.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-12-02 13:50:41","Could we please have something different from `c` in either one of these two loops?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-12-03 08:48:41","in the three above blocks, the `def` statements would be better inside the `if` blocks (they don't need to check `loss` every time that way). Some editors will claim redefinition of function names, but that is not the case.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(497, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","eickenberg","2014-12-03 08:52:58","I don't understand why `loss` gets a default value here.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(399, '', u'nilearn/decoding/space_net_solvers.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 13:28:27","There is a problem here: `standardize` is given to a NiftiMasker which centers AND norms the data. Is the `normalize` parameter supposed to do something else?
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(534, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 13:37:47","Really useful function. A few suggestions:
- we should maybe put it into _utils, I would use it for the `Decoder` class
- should it be called check_univariate_screening instead?
- should we create a _utils/checkers.py files for this?
- could the `screening_percentile` argument just be called `screening`? In the classes constructors as well?
- this would integrate nicely with a `compressing` argument for clustering methods in future averaging models for example
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 13:40:02","`smooth` is called `smoothing_fwhm` in the NiftiMaskers
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2015-02-05 13:42:49","The doc for `normalize` is zombie code. This parameter is not used anymore. Removed.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(534, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 13:47:21","`mask` should be `mask_img`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 13:48:19","`mask` should be `mask_img`
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(500, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 13:52:22","pep8 nitpicking: Xmean to X_mean. This exception to pep8 is Xmas.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(449, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 13:55:15","It's advised to only
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(648, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 13:55:50","Set the loadings vector
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(674, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 14:02:29","Does it really work with np arrays? Doesn't TV require an affine (and therefore) a Niimage? Doesn't the
 _univariate_feature_screening also require an affine to estimate the brain volume? Anyway the fit method appears to require a NiftiMasker that fit_transforms the X argument. Which would fail if X is an array.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 14:02:48","pep8
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(732, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 14:07:23","I think it is better to use range rather than xrange
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 14:08:02","mask_img
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(965, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 14:09:17","mask_img
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(965, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2015-02-05 14:17:38","Yes indeed, xrange will become range in py3, blablabla
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","schwarty","2015-02-05 14:19:34","@dohmatob @GaelVaroquaux 

Really useful function. A few suggestions:
- we should maybe put it into _utils, I would use it for the `Decoder` class
- should it be called check_univariate_screening instead?
- should we create a _utils/checkers.py files for this?
- could the `screening_percentile` argument just be called `screening`? In the classes constructors as well?
- this would integrate nicely with a `compressing` argument for clustering methods in future averaging models for example
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(71, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2015-02-05 14:19:45","Deceptive docstring, fixing...
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2015-02-05 14:29:25","The function is not meant to become a tool. It's just a piece of inline code which got factored out for software-engineering reasons.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(71, '', u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2015-07-07 09:02:48","You cannot have changes to gen_rst in this PR: they need to go to sphinx-gallery.
","99b9c5534d1318f2e5f64f69ba8620c73580f0f5","(None, '', u'doc/sphinxext/gen_rst.py')"
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-10 03:51:45","BF: broken demo script","0d4b43407e2f84cbb84abc1146190db8795eb9ce",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-11 09:16:31","CLEANUP: removed support for pure ISTA
CLEANUP: unused f1 in mfista prototype","349e493522ed641387168ef6f85e5dee7f5a7c7e",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-11 09:17:08","Merge branch 'sparse-models' of github.com:dohmatob/nilearn into sparse-models","9981cc3a03a6fed67681c46695eaee5596aeeb60",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-13 14:08:11","- REFACTOR: no longer using nifti_masker in path_scores(...) + usign 3D booleans mask instead
- REFACTOR + CLEANUP: rmed dead code chunks from space_net_solvers.py","476174ce885333eadd0c4195fae31ce388c8540e",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-14 13:31:38","ENH: ndarray -> empty in path_scores(...)","9e5a85e5c5655b848dea7eb2ac8dbb772316bafd",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-14 13:44:51"," - ENH: using best_init in re-fit in path_scores(...)
- earlystopping -> early_stopping","d2db10c44f6b99d34bf111f835fb686c2799b0c2",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-14 13:47:56","introduced bug","00329c7fca1ae8655302db58604213f3cf31352b",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-14 13:49:55","cosmetics","5c9b0a9488073252bf2f95fb419898a3b6b5ec54",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-14 13:53:23","RENAME: tvl1 -> tv-l1","dc0995b90c67507f7a24424073ea3e22e185ed7f",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-14 14:05:27","ENH: renamed *spacenet.py -> *space_net.py","fca7a98921b1d2738e9dcf3ebf15522e74b74869",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-14 15:22:37","ENH: _crop_mask(...): new function to crop mask","7cba5e781200335c1e48ac218a265d56de755902",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-14 15:49:36","ENH: tiny fixes on _crop_mask + test cases for this new function","f1bc3c4d63fe6833bd28a24992c40559ce177794",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-14 15:58:06","TYPO: self.i_alpha_ = self.i_alpha_ -> self.i_alpha_ = self.i_alpha_[0]","5fe88b48396f53abf6ee0d3130330a3e3dc1f018",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-15 11:00:21","- ENH: explicit verbose param in path_scores(...) function
- ENH: dilating mask after screening (WIP)","cc1c2be6be3365f29da94f2d2bfffe2f6c8d162e",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-15 11:57:41","ENH: _fast_smooth_array -> scipy.ndimage.gaussian_filter","5a0a9fc8280f195606bc2fb43815ab6bb8d82215",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-15 12:03:30","unused import","4e3aaf05637554f1794e0790d66ef6835ba61188",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-15 12:07:06","unused import","ae765470421494c8855070191dc4ffb493da20ee",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-15 13:07:11","mergeconfilict","9adf2c82487fe5f48d56702931ec55f9b574e848",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-15 13:25:38","ENH: in haxby demo: splitting data into train (10 sessions)  and test (4 sessions)","bf20ef34b7febffb5c9396f9e4fbc6e898328896",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-15 13:32:17","ENH: changed screening_percentile from 10 to 20 (percentage full-brain)","3be57488a63443bc5749672e0c5c7d49a735f51c",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-18 13:00:52","BF: horrible classification scores due to unstandardized data; standardize = True in NiftiMasker henceforth
CLEANUP: removed junk initialization of gradient_buffer
ENH: cv=8 is new default in SpaceNet
ENH: new default screening_percentile=20","ac665520e0efef30e32c4b86d92e75c0eb5e86d2",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-18 13:10:30","CLEANUP: confusing docstring about tol in mfista","821e9af32325346bc82f243ca9ed723347416832",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-18 13:29:40","CLEANUP: some errors in docstrings","87bf5f64157a8717b94a8fa410e9a07ede0e9d25",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-18 13:42:48","BF: couple of tiny bugs","e246b4aa4da843d3101797273493a5d48d55c3ad",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-18 13:47:14","CLEANUP: minor troubles","acc64ccc429b317141d4f05368e8b51d20ab312e",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-18 13:59:41","more authors","c6a1226d00d30b29031ca62c77f85e82c132c765",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-18 14:18:52","printint more results in haxby demo","b82efa0046d090914f8a20a2b82633236347d761",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-18 14:20:12","cosmetic change","2a7ba74a1553e4cba5f47309147b4bed504953b4",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-18 14:48:33","ENH: not screening if mask has as few as 1000 features in its support","604c2214a331bc13fe215f2a33f4092f6a6e79a7",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-18 15:14:40","ENH: better var names + tiight_mask -> mask","02774b4882eaeb93c29a794b2169692973cceb54",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-18 15:40:28","CLEANUP: junk _ovr_y function in SpaceNet","090da52753b47f6aa8910c27ad9e656b7680dbf4",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-19 14:39:55","CLEANUP: fex typos","fe8a47174b2d4c931ce2527b32c0ed104fc3c6eb",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-19 14:44:33","CLEANUP: usless int casting","502e9465803ddeb542fee75da63cc9a4a56bfa7a",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-19 14:49:20","CLEANUP: tiny fixups about objective, cost, energy stuff","8240ebfae4d4690bbce33893bc6ebd33732cdbaf",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-19 14:58:24","CLEANUP: corrected some docstrings about numpy ndarray shapes","bc3ae9cf19244351e5476f804234d49aa5eba7b8",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-19 15:20:19","* ENH: (gap ** 2).sum() -> np.dot(gap, gap)
* CLEANUP: l1 -> L1","66961984a9286f0c6b5edc6554a4144c72e5072e",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-19 15:25:01","ENH: input_img_norm = (input_img.ravel() * input_img.ravel()).sum() -> input_img_norm = np.dot(input_img.ravel(), input_img.ravel())","e3eff16e00aeb37da0ecef8df7d12b63e22038e9",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-19 15:29:18","ENH: np.sqrt(float) -> math.sqrt(float)","b809a3fd39c43d953e42086116a413116f2b0e07",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-19 15:31:28","cosmetic fixups about docstrings etc.","3d59f9b61fe3b76a63fef56b9835353e19333590",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-19 15:33:11","CLEANUP: more corrections on docstrings about numpy ndarray shapes","ddea1bfe6d2196f8d58d50758062ec17f71d54f4",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-19 15:40:32","ENH: printing elapsed time in minutes too","0fd87f5e3a649a0484f469ad1ed4d4b200aad9b4",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-19 18:52:20","BF: broken commit","22f3471fd9927163bfda208df039ebd90b131302",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 09:49:15","addressing @agramfort's comments","ec0cea3a02284b118aebf4d1d8a5366bf98fc251",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 10:29:09","cosmetic changes to address comments","8542ff1511a953f52aeeb4e0a41c56b100c0a1b6",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 10:30:40","inserted _ in name of test case","0e3ab788f6a1030f74bdc451eac9115bc91aa307",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 10:46:13","explict testcase name","ea4367ecd48ac3460a0598730408086ff3f8e48d",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 10:56:50","ENH: not using relative imports in tests","546718b07c9f632faa0c4d3d429bdc951e4ad520",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 10:58:52","invalide -> invalid","1b867ef6bf1a0900500abfa8ac49e82aa258da68",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 11:03:29","more docstrings fixed","02679fead5f446ea5d31c8e8d8e11a17bc79e6e0",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 11:11:51","ENH: headline in mfista docstring","a4521147084cca1235217e9635b0112230750979",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 13:40:29","REFACTOR: factored out code for univariate screening into separate function","3cba6a9bca2b9596735e8d301b4cfc5780ba9e69",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 14:08:55","intercepted_prox_l1 -> prox_l1_with_intercept","2ee231e505b07857de3bd2fb93478299ba9ec763",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 14:23:20","still fixing minor issues","b5b2cb7394df9eb0f469f2abbace7ee24825a2cb",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 14:28:54","ENH: binary_dilation(binary_erosion) -> binar_fill_holes","ba74b7f35d049bb34663ed109f08d760ca9dc069",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 14:37:58","avoiding ravelling twice by using a view instead","5f774b8739ec595bc2287d62e99d3de7b3f41c6a",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 14:43:12","repairing broken testcase","e5297398c9878e82ec6812d8b05c629ca1dd1cbd",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 15:01:48","removed superfluous ravelling in mfista","ca9cb70d84cb0906281fd277cd6025872d93c0b7",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 15:13:58","DOC: improving SpaceNet doc","76256cee6a97121521f3b19c0ecb3286edd0deed",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 15:15:36","relaxed very string test","5f81cf875ca24a803864bf2dec80cb892c535967",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-20 15:44:53","typo","322e7d315e9668f8d60773772a08647dffea6a34",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 06:38:21","ENH: improving docstring of SpaceNet","6c82f183d3736dcd4f4b136c28546c95d0efc82f",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 06:46:35","ENH: still enhancing docstrings","0ac2c166b09f6d1dc67a25e84820a10d999b72fa",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 07:51:26","ENH: volume-corrected screening percentile","a126ce347db3721ba7f45f2418a795c85a138d3c",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 08:40:05","cosmetics","dae019e69a24a1c75559be331ce9786dd2963550",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 09:12:44","NEW TEST: testing code computing mask volume","49de576cc3b3853026c1c4746240c93422a85079",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 09:16:21","Merge branch 'master' into sparse-models","b97f8409a971d60d0d1183e7976a77c341ee4503",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 10:43:00","Merge branch 'master' into sparse-models","4de277ed6a5d1ae685194a4ba6df8c70b30d79c6",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 11:16:27","Merge branch 'sparse-models' of github.com:dohmatob/nilearn into sparse-models","d2f0b15b71b4cd6b91c98068bed437d284902aa9",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 11:22:49","doc","dacedb7d9027140c105c135906385842d050d6c9",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 11:42:14","typos + more references","01ccbae79f4f3338cee22b1efb18ae4025570e98",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 14:27:06","Merge branch 'sparse-models' of github.com:dohmatob/nilearn into sparse-models","8d34bb49f104161a9b1ab50028b113ad99a1f28e",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 14:36:14","implemented SpaceNetRegressor subclass","80d938dbeea45ec393db3ff180e5b411ee2edafd",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 14:58:20","typos + more coverage","2ecded3805b8c1abb9e39c19f7571d1acb8a7110",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 15:14:28","cosmetics","18f62f898050f0d2d0cf4eb264841326c1c3027e",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 15:54:29","typo","65e7a2b3434656c92982bd7ff65e69fd1dd2723d",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 16:16:32","* NEW FEATURE: SpaceNetClassifier (subclass of SpaceNet, specialized in classification problems)
* DOC: documented is_classif parameter of SpaceNet class","5d150b67e6e24fb7c5e81c36da0fd04fe863a00e",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-21 17:20:06","reporting mask volume in cm^3 + plus other cosmetics","7c1a7c8d8b67ac644e972aaf014398936d3ab9de",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 02:20:58","REFACTOR: refactored classification-specific APIs from SpaceNet base class","18ed48c9533bd49fdd0a5015af6b5f7da4a38859",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 02:30:50","ENH: disabling screening if only 100 features or fewer","e2f60bba6186d1bfd2b511863f35ebf7c91e2263",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 02:50:05","small corrections","5feb7bf572fd037e8d282f063abad43918b40da3",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 02:57:13","corrected some errors in param docstrings","41e3c9a5e4912c6b5f0b5279863ba4e411047b1b",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 02:58:58","corrected more errors in param docstrings","ea876c61820a0359e398a34b985e15f89044ba5c",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 09:14:50","NEWFEATURE: SpaceClassification works with mse loss too","be5fa423f60599afbf2b6fd8a7d6f819c024577b",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 11:30:59","fixed typo pointed-out by Loic","56b913200f790e0c998b30f5400c09a70df73c13",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 11:43:18","clean err_msg logic","5ea9ec3c5c14c84f7188420fdf0fd3642db49e10",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 11:56:56","ENH: better computation of intercept form mse loss","2d158cebce567815a93a50e294e8142258b0110e",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 13:28:34","restored default loss in demo","087eb327269b04d05151f64e7227b97aa4cad47e",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 13:37:16","cleanup demo","33d7a453f621615be3e55b49299be6b3c9075bba",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 14:40:26","ENH: not centralizing y for classif problems (recently introduced bug)","6deb9adf81686ff1c44332f9874650976ff24590",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 15:17:38","random_state arg in _check_lipschitz_continuous","1d2487bc5d4b8e227355c27b7dcaf38bcf3004a1",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 15:23:41","BF: warnings.Warn -> warnings.Warn","8c3a27e7825887cc13c6bb374ee40d065c45f7d1",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-22 15:34:15","removed abnoxious 'novel' word from mfista docstring","ccf1400e978c3761aef9be787d5bb4e31f5a5131",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-24 17:12:48","WIP: Turning off smooth done before screening","b493b97beff3a68b016e3f9c15bbb5b43c86b23b",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-24 17:15:00","NEW DEMO: PMG","c0eff5a12502c7521c767ade9c01ac7275c99afa",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-24 17:28:53","more","60181fd15ca2ed25737a6d09206c04b5933ce5c7",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-24 17:35:01","more","0733e5a00766baf4a565e443ce5d63a082392e7f",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 07:21:32","l1_ratio=.3 for pmg","6a06e7fa4931b4e5cc883de60fab6ec15d516c8e",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 11:26:51","full brain sl (ok)","4243e531519d27b5fbbccd4e21105379803324ea",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 12:45:55","l1_ratio=.3 in poldrack tvl1 demo","cab64c7d67576f3e2fc943b637c8089d35d036c2",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 13:21:44","CLEANUP + ENH: new option smooth=0 in _univariate_feature_screening","838f14a51742b287053044bac8a5d2ee749b5a9d",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 14:19:15","morphing strategy = dilation o erosion","46f1fc1abce0d0fad65710f2497605b1570f38d5",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 14:20:12","cleanup demo","b830c1caed9cb42a53f1eac06aa038dac6330499",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 14:35:33","WIP: demoing sl and tvl1 on haxby","6018695ef07095f1cb46cdfd5faed2614e9b9c60",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 15:26:33","BF: restored standardize=self.standardize in SpaceNet's NiftiMasking","2595152d32cb7fe19bd76c22b1cc2ab8f3db3e18",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 15:47:09","defaut cv=8 in SpaceNet","bf1bde83218a2bde192c38f65c0776c02e7e7568",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 15:49:51","scores_ -> cv_scores_ in SpaceNet","36aa5c120e9ea10e6dfd6daa51db8b7c342c2230",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 16:10:58","misc cleanups and cosmetics","fe97195a2968f8533a16d3fffa49e199895c4b15",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-25 17:44:43","cleanup","bf6e8f62eb188b854db82c5fc382abbaf07d86a9",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-28 08:43:53","removed unused slicer var in plot_haxb_space_net.py","4200fffef35f85cc5cab46a1f43f5125ecf084ae",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-28 12:40:16","- BF: proper standardization of input data (in fit and predict)
- BF: restored broken oasis spacenet demo","699c23b465dbab55be25a9e1191a8657b8c3bffc",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-28 13:09:58","ENH: properly setting intercept after fitting","2737be9fce689bb88117c0ca52b02d8c68a9981c",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-10-28 13:26:32","tiny bug fixes","9d17d8b05de876510d9146b03e359031ed0bdb7d",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-04 09:17:37","addressing @bthirion's recent comments","af4591b36c1caa5323d0f26be43bebcbaa4db86d",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-04 09:51:46","still addressing @bthirion's comments","c72471949898276adc4761f2f2950ca667adf3c4",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-04 10:13:13","still addressing recent comments","2366907140c42682602ec3f0fb427208d2bfc626",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-12 13:39:07","ENH: alpha_min=None by default, so that eps is used (instead) to construct alpha grid","a4275a3bdc646e00df887a7cb191d4e484670fda",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-11 17:17:10","updated ROI extraction","70119ef3c29ee68d3a12c30cb26a3b46dc5dd2d5",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-13 15:33:56","ROI example: second version","a3fc5d96454b1c4d1a7aad263693a9e18e6eec9a",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-15 17:07:37","Now Subplots and boxplots","16212e47cfb806c5cbfe7c70662624adfe5721d4",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-21 14:14:02","PEP8ized + more intuitive variable names","c4cb038dee78d1f2c669a2c44446fe1d5edae492",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-25 06:55:11","Fit-Check added to NiftiLabelsMasker","4c05eebe8cfbd67616cb610302aadd3e64e84b9a",""
"pull_request_commit","219","nilearn","nilearn","lesteve","2014-10-27 14:49:54","Remove explicit mention of NiftiMasker from BaseMasker","96947c798ef5756a56bfd9ce888e8c5b9d31b130",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-27 20:48:41","changed exception type + unit tests pass now","49038c290430d3a2470ef279cf57749a4a448371",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-28 23:20:23","variable renaming + cosmetics","7acf6feaa52d3014b03a7fe8d18aa03f31bcf011",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-29 14:39:35","fit-check for Nifti([Labels, Maps])Masker + type in BaseMasker corrected","db484d2331d0a5d6de9b7ad78ea1e88229ff1473",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-29 15:20:34","Adapted .rst files for Sphinx","7475a0770276c19e05aa75844d4d8fe7374abe93",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-29 15:27:58","de-sniggle-ized","5f8c6dc10208f39ef7d1869a3bd7616fa80d4548",""
"pull_request_commit","219","nilearn","nilearn","Titan-C","2014-10-14 09:11:38","Create nilearn_data as a working dir for _get_dataset_dir","1ff59a0bad3c74ac0ed0d239133075841e417414",""
"pull_request_commit","219","nilearn","nilearn","Titan-C","2014-10-14 17:23:28","Change logic for searching dataset dir
Catching all exceptions and reporting at the end","3d1da5ae5b11493b098a5436cae9e0820041b7bd",""
"pull_request_commit","219","nilearn","nilearn","Titan-C","2014-10-18 17:11:35","Test clause on non-directories","e93e71c49a8f0c49abb2d6ec7633a3ac60ee56e6",""
"pull_request_commit","219","nilearn","nilearn","Titan-C","2014-10-24 08:53:26","change work inside temporary directory in get dataset dir test","857f4cc608721562b6b630fed0c937c248918439",""
"pull_request_commit","219","nilearn","nilearn","Titan-C","2014-10-24 23:22:30","Regexp control on raised errors","f4ee636e39ac0694755a132c5dfd85a83e7c72bb",""
"pull_request_commit","219","nilearn","nilearn","lesteve","2014-10-28 10:23:53","reorder_img 'resample' argument sets interpolation resampling

Also:
* niimg variable names should end with _img in test_resampling.py
* fix PEP8 and typo axis -> axes
* add value of interpolation in resample_img error message","cbbbff84519a594e13e1fff9c5dd263aa7ad406a",""
"pull_request_commit","219","nilearn","nilearn","Titan-C","2014-10-27 23:20:49","Different environment setup testing","6e50cc1a24c2d780f40595be52751424ad194aad",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-29 14:39:35","fit-check for Nifti([Labels, Maps])Masker + type in BaseMasker corrected","897e5b311e27612841d6504c076c49a017af14ed",""
"pull_request_commit","219","nilearn","nilearn","Titan-C","2014-10-31 10:00:13","_get_dataset_dir error message string","70d4ce4c4b0a517198aa6f78b870aa92f99499af",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-11-02 15:18:23","Uniform use of float literals","fc33229896a47c210670138be42c718142aee70d",""
"pull_request_commit","219","nilearn","nilearn","lesteve","2014-11-04 12:58:15","Example uses nomcsv rather than loadtxt","fbd5f294b066b5f8a976a96bcc786145c93c6994",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-12 14:23:35","Merge branch 'master' into sparse-models","a57f6f43e415e577ed85c4db95d785d963540056",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-13 07:56:25","REFACTOR: adressing @bthirion's oral remarks about the lambda (no more lambdas in modules)","46deadd9802514b20b6ac5cc946c689f8316f2ed",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-14 08:44:37","typo","370cb31a2826b54ab7dbb78bd4f5f088623ab763",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-29 14:39:35","fit-check for Nifti([Labels, Maps])Masker + type in BaseMasker corrected","2aa8ebca4b9aa7295459554606a3a20a940afe46",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-14 08:51:11","Merge branch 'master' of github.com:dohmatob/nilearn","388d5af8f31c557def57d73d558e7209bf411f8a",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-14 08:51:22","Merge branch 'master' into sparse-models","5cc7784af8c96f2f8986b995cef173aca7a9b812",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-14 14:24:38","ENH: cleanup for poldrack demo script","22b7b76d26d2d4b35b63ced3422430d2e7b22c8c",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-14 14:29:17","DOC: rmed ref to TV-L1 in l1_ratio doc of gradient_id(...) function","e91a66d1535e67c0a819b88bdde226081267be70",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-10 16:18:43","cooler","493a021d9f39c9ceeff02f96b113d9dce43da748",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-14 19:00:29","n_jobs=1 restored in demo","ec3343668fecf82c1812b35a828c2f5071750c76",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-14 20:32:51","ENH: fixed MemoryError in _univariate_feature_screening(...) function","1ba60ee38ed15156e9707e04c838625be7b7f784",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-14 22:52:31","BF: restored face vs house in haxby spacenet demo","c70cc68797885246c38aed1a86b94fff151c890f",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-15 15:11:15","ENH: demoing all penalties in poldrack demo","5e15c6c79177435cb9f2ede872695921f5a704e2",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-17 13:53:45","Merge branch 'master' of https://github.com/dohmatob/nilearn","143da80df56fcb246f1abeff3782610445b0e94c",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-19 17:36:45","Merge branch 'master' of github.com:dohmatob/nilearn","0d5dcbaa18f83e552c8af267ce2a9482777c8b18",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-19 17:37:22","Merge branch 'master' into sparse-models","fe308e5bf99f9af178969d4830ffa976056b56cc",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-20 10:26:17","REFACTOR: in space_net.py cv scores are 'bigger is better'","c032f241cf8d0315baadb68ded8b2b193dde511b",""
"pull_request_commit","219","nilearn","nilearn","banilo","2014-10-29 14:39:35","fit-check for Nifti([Labels, Maps])Masker + type in BaseMasker corrected","4f072c0c2a90f765b3ebe5381d78d213d76dc04c",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-20 10:30:04","Merge branch 'master' of github.com:dohmatob/nilearn","8d13881b009f3abfc6d7037ba2f45ee53fef1ec5",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-20 10:30:40","Merge branch 'master' into sparse-models","befefa9151f02e4996c08645fec12007edc4efc0",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-20 10:32:14","BF: trailing X_ in space_net.py","dc88ffea9da53a5172ae2d44833a95baa9a56c26",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-20 13:28:39","closes issue #286 (about nans when l1_ratio = 0)","0f4a9f2aa6ef090083f1e161ad1a9bd76b9ef636",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-20 13:54:28","ENH: tests for issue #286","933e5552d6678e76f151d92dbf82d6b5ba7b6bda",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-21 12:59:52","ENH: returning lists of pairs of lists of indices used in cv","002f29accc7185a7ab1a0baf1376c9d93cd2243d",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-21 13:16:53","typo","705d93abfd976728ace365b42798a13b7bc7e672",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-23 10:28:05","Merge branch 'master' into sparse-models","9f61c7e4c3cfbe53702a23c63a4ceff47724df00",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-11-23 10:31:17","BF: wrong docstring about ventral mask in plot_haxby_space_net.py demo","435d77ed0982e298e1e33b44f62b4547988a5462",""
"pull_request_commit","219","nilearn","nilearn","GaelVaroquaux","2014-12-02 09:52:56","Clean unused imports","07b96d634d917d6487f4affb5842fed8668a9774",""
"pull_request_commit","219","nilearn","nilearn","GaelVaroquaux","2014-12-02 10:02:00","MISC: Cosmetics in the example","f613ce721e6e836a711dd9fe2221dc8034270204",""
"pull_request_commit","219","nilearn","nilearn","GaelVaroquaux","2014-12-02 10:02:14","API: SpaceNet -> BaseSpaceNet

We want to make it really clear that it is a base class, and is not
intended for the end user.","d03733af215d14ea0e9d3a0e53324b013d7f4abb",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-02 11:25:17","Merge branch 'master' into sparse-models","ab01ddfe8aa262b17e95f646f52b0d8a6741ebcf",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-02 11:28:09","Merge remote-tracking branch 'gael/pr_219' into sparse-models","26b11e059d42b40b573d8048f47f5947bfefc612",""
"pull_request_commit","219","nilearn","nilearn","GaelVaroquaux","2014-12-02 11:47:32","WIP: try to fix the alpha grid","6f32f778e42aca6d8d42718c6a76a8c182ff8cd9",""
"pull_request_commit","219","nilearn","nilearn","GaelVaroquaux","2014-12-02 18:03:59","WIP: add a second score to disambiguate","ec32c62e6548df0ac84df20974e670aa8eb82664",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-02 18:49:21","Merge branch 'master' into sparse-models","c6d80dbbaa2d8805d9a6368103a974617a11b141",""
"pull_request_commit","219","nilearn","nilearn","GaelVaroquaux","2014-12-02 19:16:51","ENH: better verbosity

verbose=1 should simply give us an idea of how the progress is doing","1cae9a42af74663950c0d5ab5935c5d6fe9b0768",""
"pull_request_commit","219","nilearn","nilearn","GaelVaroquaux","2014-12-02 19:17:22","DOC: complete the docstring","8aae3c707907630f0a9f6e5b52307e1ae434a6f2",""
"pull_request_commit","219","nilearn","nilearn","GaelVaroquaux","2014-12-02 19:17:35","MISC: avoid deprecation with new numpy","4607c5fc69adb0a78ed6092f2b8109c821cdb630",""
"pull_request_commit","219","nilearn","nilearn","GaelVaroquaux","2014-12-02 19:18:47","MISC: minor improvement in example","96f763c1b28af501057eac9ec4b80cc5696c4ce7",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-02 19:39:18","fixed tests broken by the other other pr_219","3cccb1db763eb552a722eec3e0ec06bfe4418f21",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-03 09:13:33","BF: trailing assert ...","9ae4f0a8ff3d453c6528ec0a3d0f93ef10be1544",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-03 14:56:11","-BF: confusing rescale_alpha param removed from code base
-BF: multiple tiny bugs
-BF: restored broken oasis demo","6cbd4df470b864840e37da04d4679719d197518d",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-04 14:32:48","Merge branch 'master' into sparse-models","73037a3889f78b9e990f2fb28616f7d336a188dc",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-04 14:34:41","BF: still fixing alpha grid computation","cf493afadae81e0f15df143e5ed6fb88d1542bf8",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-06 07:32:54","cleanups","a1e5940e82788636492adfb0f7bc5a55f9385364",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-06 07:33:35","Merge branch 'master' into sparse-models","b3a95249993e56563b7d49a35c33e5abd64d235a",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-06 09:09:49","NF: poldrack loader (local)","d1cb877785a238def010661aca816ce736efa95a",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-06 09:49:31","SL & TVL1 ok on poldrack :)","2d538009d7c132eefd86d251e3db101044c1cf68",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-06 18:28:49","NF: cv on l1_ratio","e22195a4007e124ce2a7f5f52e285b4325be303d",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-07 08:39:46","cleanup","fa977bae28bd4f5cb6ff02fa1fbd781c5f29f9ea",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-07 15:55:09","cv on l1_ratio: ok","875a9a9e3044b36e2847dc15d3166ea514df147b",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-08 14:27:47","NF: fetcher for PMG (code stubb, only works locally for now)","b076fa8a1d989dcf7c3a748dcc1d50620b62a9e2",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-08 14:28:17","remove obselet load_poldrack.py module","e1ffc24742b25017ce3ad764beda7c381b5dd3a2",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-08 14:40:33","typo","508a9680bfd9af981d0b7b08e31693fbd4fb6a9c",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-09 07:00:18","fixing some underground bugs","273f394a5661e4504d250f83d9f83af5d8d82850",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-09 07:03:51","NF: fetch_mixed_gambles","99201266bbd8b8eb90f3173baac545f651cff3b4",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-09 07:08:53","Merge branch 'master' into sparse-models","ff6cd3fe34102d3abf39676d17fd95bf9f88f97a",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-09 08:51:49","wip","857c04e9506b72cc07a2b5356883fa70c7d67d47",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-09 13:02:37","wip","4ab3c6e15068227660924662725f169b4a764fcc",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-11 06:57:55","Merge branch 'master' into sparse-models","513ac7b4a80a28d6bb93199b16aff98cb0e798ce",""
"pull_request_commit","219","nilearn","nilearn","GaelVaroquaux","2014-12-11 16:31:06","Favor small l1_ratios","ffa5fd1ac19f45058ba29226ba22c1cc669f60f7",""
"pull_request_commit","219","nilearn","nilearn","GaelVaroquaux","2014-12-11 18:32:04","Fix path on l1_ratio","62366c9385d6afcc1dce84f5bc38df66fdc8763a",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-12 08:55:05","Merge branch 'pr_219' into sparse-models","65e149fa27ee6de3538e2aa544b783077f9371ae",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-12 09:43:10","BF: fixed couple of testcases broken by pr_219 :)","035982da68942e50e0d1b00724c25aa38eee907a",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-12 09:45:25","restored default l1_ratios to .5 (from .75)","4e14d9800755e3bee0d3b09255e4d7ead8b632c4",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-12 10:02:07","ENH: addressing @gael's comments about spurous start(...) method in EarlyStoppingCallback","c1a4b020467afd76e5303ca7df26c004b958b7eb",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-12 13:32:36","""""""
I would prefer to avoid casting the ""penalty"" string to all lower
case. I would prefer if you did an actual check to ""tv-l1""...  The
reason is that other Python objects (eg dicts) behave with an exact
check, and thus you can confuse people and give them bad habits.
""""""","0b1e9d22b65582f605adc8048f25c3a1449b8746",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-12 13:35:10","BF: broken demos","525e4bdc5ba2a7222005177dfb8e82e74fa69589",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-12 13:56:59","cosmetics","cb4dcb45ffe8c8dd41eab57ff85ae8d0ca63ae48",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-15 12:54:31","Merge branch 'master' into sparse-models","7af81f80655b0237eb83a741c72e912c397a89ad",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-17 15:16:51","EXPERIMENTS: debugging why good alphas don't mean good maps","383e0eeac9aaa0d08ecde59fc8c8bcbd6c8512bc",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-17 15:17:14","typo","99d999a74e9b773f62bfd428e868b94973623fab",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-18 12:34:53","wip","6bc1b4cfd58a970a4a1ade59717b24405d4b7e52",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-18 17:02:11","wip","361b1d982d4d825d29813762dd5eb01a5acfb3cc",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-18 21:04:50","ENH: using sk.cv.train_test_split in oasis","0ad1e9a67b5bc31b0f470c9f766c0f6edd3d5b16",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-18 21:15:37","BF: train_size=.8 in oasis space net demo","d86c23e212c3bbc64b96c225c43c7f5606e746e8",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-18 21:18:19","Merge branch 'master' into sparse-models","cc747215a114f11879e076a56a6f7d8aad0dc3a9",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-18 21:47:58","ENH+BF+CONFUSION: (on utterly bad prediction scores on oasis dataset) now using all oasis subjects","9f980a2a3865403cff7ad0fe0fe14a4a3656c105",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-18 21:49:20","rmed n_jobs=24 in demo","2652d64f94ca959c7e2abd4ab8698e7ad7eb63b2",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2014-12-18 22:10:37","BF: weirdo","ea6a440e059b94934751ebecc141b36da619f3e0",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-01-13 10:29:08","Merge branch 'master' into sparse-models","1012c0e43c04297e6413c023c1a447c3ca8aa3ce",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-01-13 13:04:33","Merge branch 'master' into sparse-models","2794dfe242b19ea62e24804287738f9705194337",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-01-16 21:46:02","Merge branch 'master' into sparse-models","ae08b93f931b61631ede8a98d2688d97a47c8760",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-01-26 12:12:39","Merge branch 'master' into sparse-models","9928cdef3d3db5f3dd10443b182360c86f809f31",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-04 17:02:00","- ENH: putting Mean Abs Error in titles of Oasis prediction curves
- ENH: using MNI template (default) in poldrack demos
- ENH: various tiny things","7ceb460382e5c618b1d04069a2af93fbd7ffa5a7",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-04 17:03:05","Merge branch 'master' into sparse-models","4b35b7e0121e66e83f4ccc8d17d8225783974964",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-05 08:11:33","REFACTOR: moved space net demos to examples/decoding sub-dir","493ac104168fdff2296fbe9a2a33485f3745a334",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-05 11:44:16","- ENH: added space net entry in doc (decoding.rst)
- ENH: in haxby example, using index_img","d6ffdbced61b47993576e8e010054765d31aaffc",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-05 11:45:08","Merge branch 'master' into sparse-models","867d93b375f4d32c07de3038ecf0313528fa67a9",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-05 12:39:02","- ENH: removed zombie doc for inexistend normalize param
- ENH: simplified space nets by removing superfluous for loops","56c2e3adb4ecdcfae23ba6b84ba6c601c841169d",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-05 12:39:20","Merge branch 'master' into sparse-models","6f0b059dd8c56ba22306254d2a0e75939cdcdaff",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-05 12:43:30","smooth -> smoothing_fwhm","94d89bf347d39501449c453fa008a4ba483a95aa",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-05 12:48:58","mask -> mask_img in _get_mask_volume","d00eefd36147cdaf1b9e432eadf073cf4f2d86eb",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-05 13:10:59","tiny fixes","9a426fd66b01e25f7be71b0830d6a003c77dd1dc",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-05 13:21:57","BF+DOC: fixed bug in docstring about type / shape of X","34ce15d94e6006d4912a5f0504e3721919978aba",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-05 15:50:38","ENH: adding oasis vbm in doc","252dd4dd9a07faf861261a3e3c40e4031375d808",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-02-05 16:17:40","Merge branch 'master' into sparse-models","61c36711add36af8e9b86e72166f5048e0dde8ef",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-12 17:34:23","mergeconflict","4fdb5e56fbcc12fb8e0bd6f8fa948b97217fa96f",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 12:13:05","BF: fixing py3 issues + 0400 ==> 400 (error = invalid token)","37331549e8ed2a5ebbd7d46241e7bb02862f210c",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 15:42:06","BF: still fixing py3 stuff","3aaea8dab666ca34ecb88376e6cd81c600b78741",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 15:44:26","Merge branch 'master' into sparse-models","be52601e0585a0d43026c0fc09822c885edf49cb",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 15:53:20","BF: still fixing py3 stuff","0a9540f72e6fd9a7b633523e21f80b4b5049b57f",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 16:02:36","xrange ==> range","b9a179a4957e1c83a64e9acea83dfa113a388b89",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 16:14:01","random ==> check_random_state","8681ce5ea7b183463290f6d9a895cb7d3c18e2d2",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 16:35:38","rng.sample ==> rng.choice","bcb446254acc472f01c4299003c4e51719e64d92",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 16:35:59","Merge branch 'master' into sparse-models","ad2d495b14521519ed481a0f3548dd7da50c50a5",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 17:11:31","BF: choice ==> choice(replace=False)","10a7219e9746a897d03603eab4db4021f5f40238",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 17:11:53","Merge branch 'master' into sparse-models","223aa5ae5b5c5e7226a6024172c319e50527bed2",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 17:22:20","import nibabel in datasets.py","b478c80a8e725cdaeb7677539c3e0910ae5ff0c9",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-13 17:32:28","more","335158518eb63f74475eb81fe43041a506b0d206",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-14 07:26:02","BF: basestring ==> _basestring (bkport for py3)","60d0eca5d7cbde8f1f772186d79dbcc1ffd81b77",""
"pull_request_commit","219","nilearn","nilearn","dohmatob","2015-07-14 07:26:37","Merge branch 'master' into sparse-models","99b9c5534d1318f2e5f64f69ba8620c73580f0f5",""
"pull_request_commit_comment","219","nilearn","nilearn","GaelVaroquaux","2014-10-14 11:53:40","You need to change this in the examples too.
","5c9b0a9488073252bf2f95fb419898a3b6b5ec54","(5, 401, u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-14 12:18:00","Done.

On Tue, Oct 14, 2014 at 1:53 PM, Gael Varoquaux notifications@github.com
wrote:

> You need to change this in the examples too.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/dohmatob/nilearn/commit/5c9b0a9488073252bf2f95fb419898a3b6b5ec54#commitcomment-8152357
> .
## 

DED
","5c9b0a9488073252bf2f95fb419898a3b6b5ec54","(5, 401, u'nilearn/decoding/space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-27 22:00:56","""slicer"" variable appears to be unused
","b830c1caed9cb42a53f1eac06aa038dac6330499","(None, None, None)"
"pull_request_commit_comment","219","nilearn","nilearn","banilo","2014-10-16 12:25:48","Calling y ""dependent variate"" might be confusing. Perhaps ""dependent variable"" or better ""response"", ""predicting target"", or even ""ground truth""?
","bf20ef34b7febffb5c9396f9e4fbc6e898328896","(32, 31, u'plot_haxby_space_net.py')"
"pull_request_commit_comment","219","nilearn","nilearn","dohmatob","2014-10-16 12:37:34","You're right.

On Thu, Oct 16, 2014 at 2:25 PM, Danilo Bzdok notifications@github.com
wrote:

> Calling y ""dependent variate"" might be confusing. Perhaps ""dependent
> variable"" or better ""response"", ""predicting target"", or even ""ground truth""?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/dohmatob/nilearn/commit/bf20ef34b7febffb5c9396f9e4fbc6e898328896#commitcomment-8186234
> .
## 

DED
","bf20ef34b7febffb5c9396f9e4fbc6e898328896","(32, 31, u'plot_haxby_space_net.py')"
