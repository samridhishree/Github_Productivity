"rectype","issueid","project_owner","project_name","actor","time","text","action","title"
"issue_title","100","nilearn","nilearn","pgervais","2013-08-29 11:59:45","This is an implementation of the algorithm described in 

_Jean Honorio and Dimitris Samaras ""Simultaneous and Group-Sparse Multi-Task Learning of Gaussian Graphical Models"", arXiv:1207.4255 (17 July 2012), http://arxiv.org/abs/1207.4255._

Given several sets of input signals, it computes several sparse precision matrices at once, using a common sparsity pattern.

The implementation itself is already fairly stable and optimized. It remains mainly some technical issues (like API, argument names, etc.). Namely:
- [x] Settle on names for functions and classes, the current ones may not be the best (group_sparse_covariance GroupSparseCovariance and GroupSparseCovarianceCV)
- [x] Clean up handling of stopping criteria (esp. the _group_sparse_covariance_costs function)
- [x] Separate in GroupSparseCovarianceCV the parameters for selection of alpha and parameters for final fit.
- [x] Automatically normalize input signals.
- [x] Clean up user messages, use the logging system of PR #84 (or similar). 
- [x] Write some documentation on the inner working of the CV version, namely the modified stopping criterion to stop optimization.
- [x] Rename the internal variable `Winv` into `W_inv`
- [x] Rename the regularization parameter `rho` to `alpha`, to match scikit-learn's convention. This implies renaming an internal variable already called `alpha`
- [x] add support for caching in GroupSparseCovariance|CV
","start issue","[ENH] Group-sparse precision estimation, by Honorio & Samaras."
"issue_closed","100","nilearn","nilearn","GaelVaroquaux","2013-09-18 16:24:24","","closed issue","[ENH] Group-sparse precision estimation, by Honorio & Samaras."
"pull_request_title","100","nilearn","nilearn","pgervais","2013-08-29 11:59:45","This is an implementation of the algorithm described in 

_Jean Honorio and Dimitris Samaras ""Simultaneous and Group-Sparse Multi-Task Learning of Gaussian Graphical Models"", arXiv:1207.4255 (17 July 2012), http://arxiv.org/abs/1207.4255._

Given several sets of input signals, it computes several sparse precision matrices at once, using a common sparsity pattern.

The implementation itself is already fairly stable and optimized. It remains mainly some technical issues (like API, argument names, etc.). Namely:
- [x] Settle on names for functions and classes, the current ones may not be the best (group_sparse_covariance GroupSparseCovariance and GroupSparseCovarianceCV)
- [x] Clean up handling of stopping criteria (esp. the _group_sparse_covariance_costs function)
- [x] Separate in GroupSparseCovarianceCV the parameters for selection of alpha and parameters for final fit.
- [x] Automatically normalize input signals.
- [x] Clean up user messages, use the logging system of PR #84 (or similar). 
- [x] Write some documentation on the inner working of the CV version, namely the modified stopping criterion to stop optimization.
- [x] Rename the internal variable `Winv` into `W_inv`
- [x] Rename the regularization parameter `rho` to `alpha`, to match scikit-learn's convention. This implies renaming an internal variable already called `alpha`
- [x] add support for caching in GroupSparseCovariance|CV
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","[ENH] Group-sparse precision estimation, by Honorio & Samaras."
"pull_request_merged","100","nilearn","nilearn","GaelVaroquaux","2013-09-18 16:24:24","[ENH] Group-sparse precision estimation, by Honorio & Samaras.","2dce8db3fc8e299af6c09272c98a3d9ba9d5246a","Pull request merge from pgervais/nilearn:honorio_samaras to nilearn/nilearn:master"
"issue_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:22:12","Travis failed... with what looks as a somewhat trivial bug.
","",""
"issue_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:43:03",">    I know that looking at the example is the simplest thing to do. And I also
>    know that there is many things to do, including those that you just
>    pointed out. What I really need is feedback on the core of the
>    algorithm...

I need to be able to run it to inspect it.
","",""
"issue_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 08:23:48","I just noticed that the group_sparse_covariance tests are very long running. We should work on making them faster.
","",""
"issue_comment","100","nilearn","nilearn","GaelVaroquaux","2013-09-05 16:37:24","> Wait. I already have the answer: compare the two algorithms.

Yup
","",""
"issue_comment","100","nilearn","nilearn","pgervais","2013-08-29 12:23:52","I didn't check the tests before creating the PR. I'll fix that soon.
","",""
"issue_comment","100","nilearn","nilearn","pgervais","2013-08-29 12:39:53","I know that looking at the example is the simplest thing to do. And I also know that there is many things to do, including those that you just pointed out. What I really need is feedback on the core of the algorithm...
","",""
"issue_comment","100","nilearn","nilearn","bthirion","2013-09-03 21:34:29","Tnx for the nice work, and sorry for bothering you with details. This PR should obviously get in. 
","",""
"issue_comment","100","nilearn","nilearn","pgervais","2013-09-05 15:46:32","I'm now starting to clean up the examples that show off GroupSparseCovarianceCV. There are two files that do almost the same thing: `plot_adhd_covariance.py` and `plot_adhd_covariance2.py`. The first loads data for one subject, and compute a CV'd graph lasso. The second loads data for several subjects and compute a CV'd group-sparse covariance. There are in practice only one thing that differs between them: the optimization algorithm. The code is mostly copy-pasted.

I think keeping only the group-sparse one is enough. What do you think?
","",""
"issue_comment","100","nilearn","nilearn","pgervais","2013-09-05 15:47:46","Wait. I already have the answer: compare the two algorithms. 
","",""
"issue_comment","100","nilearn","nilearn","pgervais","2013-09-11 13:12:54","In the last commits, I added two pages of documentation. One is about functional connectivity analysis with nilearn (in user's guide, under ""unsupervised learning""), the other is a technical description of the implementation of the group-sparse covariance algorithm. The latter is accessible via a link at the very bottom of the former. This link is rather hard to find, it has been done on purpose: the technical page is for developers or machine-learning guys.
","",""
"issue_comment","100","nilearn","nilearn","bthirion","2013-09-11 21:31:55","Besides a couple of typos, everything looks file. Thanks for the documentation.
","",""
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:32:55","This file need a better name and a better title.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'plot_adhd_covariance2.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:34:45","Why these commented lines?
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'plot_adhd_covariance2.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:35:13","We don't do 'if **name**'... in examples, they have to be as simple as possible.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'plot_adhd_covariance2.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:36:53","I don't think that this should be in a function, but should be in the top-level.

I also think that we should be demoing the object, and not the path function.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'plot_adhd_covariance2.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:38:15","The io module has been move to 'input_data'. You probably have some .pyc files that remain on your hard drive.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'plot_adhd_covariance2.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:40:44","The fact that we need this function is telling me that we have a broken API. It is the role of the objects in input_data to do this, and we _shouldn't_ have to define such functions.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'plot_adhd_covariance2.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:45:54","I think that you missing a whitespace before the "":"", elsewhere the docstring won't be rendered well by the numpy docstring scrapper (that's true for all docstrings).
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/tests/test_group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:46:25","I'd rather use np.empty here.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/tests/test_group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:47:32","I think that you mean 'n_samples, n_features'. Let's stick to the same vocabulary as with the scikit-learn.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/tests/test_group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:48:41","The last argument should be called 'random_state' as in the scikit-learn, and you should use check_random_state to be as flexible as the scikit-learn is with regards to the input argument.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/tests/test_group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:49:06","Capital ""G"" for ""Gaussian"".
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/tests/test_group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:51:44","I think that this function should be in the public API to be used in an example.

Also, I'd like the notion of 'task' to disappear from the codebase: it is machine learning jargon that will not talk to neuroimagers. We could for instance call this function ""generate_group_sparse_gaussian_graphs"", and rename ""n_tasks"" to ""n_subjects"".
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/tests/test_group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:55:02","Once again, I'd rather use np.empty here.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/signal.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:56:25","It seems to me that a lot of this could be done using sklearn.utils.gen_even_slices.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/signal.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 12:58:59","Here also, sklearn.utils.gen_even_slices might be useful.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/signal.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:00:16","I believe that it should be very fast compared to the rest.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","pgervais","2013-08-29 13:00:26","True, I realized that recently. There's more than this file to fix ...
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/tests/test_group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:00:54","I am thinking that this should be a callable.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(791, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:01:23","I think that I'd like the world 'callable' in the first line of the docstring.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:05:30","You should make it a dictionary, I believe, as it will make forward evolution easier.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:06:27","I believe that this is 'callable or None'. Note also the missing white-space before the semi-colon.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","pgervais","2013-08-29 13:06:30","This function is not called by the script. It is a remain of some experiment.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'plot_adhd_covariance2.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:09:14","By number of signals, I think that you mean ""n_features"" in machine learning speech. You should have both (say one in parenthesis): I understand that for a layman ""number of signals"" is more intuitive, but ""n_features"" is more precise for a scikit-learn user.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","pgervais","2013-08-29 13:11:03","That's possible, but is not the point addressed by this PR. Please open an issue.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'plot_adhd_covariance2.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:11:31","I feel that this should be a private function.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:11:52","This should also be private, I believe.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:12:34","""display"" should probably be renamed ""verbose"", to stick with scikit-learn conventions.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:14:21","Do you need to specify n_tasks and n_var? I believe that you can infer them from emp_covs, which would make the signature of this function lighter.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:16:06","I think that writing ""Honorio & Samaras"" would be easier to read for the non-expert (I do realize that this is a comment, by still...)
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:17:46","The returns is false: there are 3 return arguments.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:18:32","longer names would make it much easier to read for people.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:18:57","You should be using scipy.linalg, not np.linalg.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:21:29","Why do you have to copy omega?
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 13:22:59","It would probably help if you would give the shape of the arrays (and is the above an array)
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","pgervais","2013-08-29 13:30:11","I didn't know about gen_even_slices when I wrote this function. I'll fix it.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/signal.py')"
"pull_request_commit_comment","100","nilearn","nilearn","pgervais","2013-08-29 13:38:13","What about ""number of features""? Beside, who are we targetting here? The neuroscientist, or the machine learning user? It the former case, the ""task"" input argument should be renamed ""subjects"". 
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","pgervais","2013-08-29 13:51:18","To keep track of it along iteration. This has been superseded by the probe system. 
BTW this function is never called in the current code, because the stopping criterion relies on something else now. 
However, before dropping this function altogether, it may be interesting to know if the duality gap computation can be useful to someone.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","pgervais","2013-08-29 13:57:00","The name of the function is also highly annoying. I keep trying to write things like `rho_max = rho_max(emp_covs, n_samples)`. I think `get_rho_max` or `compute_rho_max` would be better. Is there a scikit-learn convention on this matter?
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 14:14:12","On Thu, Aug 29, 2013 at 06:38:14AM -0700, Philippe Gervais wrote:

>    What about ""number of features""? Beside, who are we targetting here? The
>    neuroscientist, or the machine learning user? It the former case, the
>    ""task"" input argument should be renamed ""subjects"".

A bit of both. However, feature is very standard, very tasks is less. It
might be useful to put in some places signals in parenthesis, just to
make things easier for people.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 14:18:58",">    To keep track of it along iteration. This has been superseded by the probe
>    system.

OK, but in the non CV estimator, isn't it used? I would expect that the
dual gap would be a good stopping criterion in this situation. I think
that we had a conversation on that. No use reminding me what the
conclusion were, just go ahead with what you think is right and drop it
if you feel that there is no use for it.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-29 14:19:54",">    The name of the function is also highly annoying. I keep trying to write
>    things like rho_max = rho_max(emp_covs, n_samples). I think get_rho_max or
>    compute_rho_max would be better. Is there a scikit-learn convention on
>    this matter?

No, but I think that compute_foo is a good name.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 10:08:15","n_var should be n_features
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 10:11:22","Please tell us in one line what this does (alpha_max may not be clear to somebody not knowing the problem well).
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 10:14:26","true_sub = np.empty_like(sub)
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 10:21:32","Honestly, in empirical_covariances, I would give up on the debug features. By definition, the formula gives us SPD matrices. I know that with numerical errors they _might_ not be, but how often have you seen this happen?
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 10:34:01","n_var -> n_features

Also, the shape description should be on line above.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 10:38:19","I think that you are returning too many things. n_subjects and n_var (which should be named n_features) can be trivially inferred from emp_covs. As a side note, I don't believe that you are using those return arguments in your code.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 10:40:17","I would use 'log_likelihood' or 'log_lik' in the name, rather than score, which is not very specific. Same thing about the 'score' word in the first line.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 10:41:00","I think that we need better names, for instance 'log_lik'.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 10:42:00","I believe that this is not the loss, but the objective function, which is the sum of a loss and a regularization.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 11:40:26","You should inherit from sklearn.base.BaseEstimator
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 11:43:13","self.cv_scores and self.cv_alphas should have a trailing underscore, as they are derived from data.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 11:47:15","I don't think that random_state is documented. Anyhow, it should be a more versatile input type, and accept also anything that can be a seed. For this, you need to rely on check_random_state from scikit-learn.

Also, your current implementation has a mutable in its function definition. I don't think that it works as you expect: if you call it twice in the same Python function, it won't give the same result.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/tests/test_group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","pgervais","2013-08-30 11:47:26","Granted. I wanted to avoid a call to .shape in an loop. This proved useless later on.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","pgervais","2013-08-30 11:50:14","This function both returns the log-likelihood, and the quantity optimized by the algorithm, which is the opposite of the log-likelihood, penalized. A good common word for these two unrelated quantities was ""score"". In the case of the log-likelihood, it matches scikit-learn's convention.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","pgervais","2013-08-30 11:50:53","This has been corrected.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-08-30 11:56:17",">    This function both returns the log-likelihood, and the quantity optimized
>    by the algorithm, which is the opposite of the log-likelihood, penalized.
>    A good common word for these two unrelated quantities was ""score"". In the
>    case of the log-likelihood, it matches scikit-learn's convention.

I'd call it ""..._scores"", in that case.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","pgervais","2013-08-30 12:48:54","I fixed this.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/tests/test_group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","bthirion","2013-09-03 17:17:31","Then shouln't it be removed ?
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/_utils/testing.py')"
"pull_request_commit_comment","100","nilearn","nilearn","bthirion","2013-09-03 17:24:37","len(subjects) = n_subjects. n_samples varies according to the subject
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/_utils/testing.py')"
"pull_request_commit_comment","100","nilearn","nilearn","bthirion","2013-09-03 17:28:53","So density controls the sparsity of the square root or of the underlying Autoregresive model. May be this could be made more precise.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(142, '', u'nilearn/_utils/testing.py')"
"pull_request_commit_comment","100","nilearn","nilearn","bthirion","2013-09-03 17:44:17"," dtype=np.float
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","GaelVaroquaux","2013-09-03 19:22:22","Agreed, if it's not used, it should be removed.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/_utils/testing.py')"
"pull_request_commit_comment","100","nilearn","nilearn","bthirion","2013-09-03 21:10:09","Maybe a comment on Fortan / C ordering somewhere ? Sorry if I missed it.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","bthirion","2013-09-03 21:15:22","You call them empirical covariance matrices in other docstrings.
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'nilearn/group_sparse_covariance.py')"
"pull_request_commit_comment","100","nilearn","nilearn","bthirion","2013-09-05 16:46:01","Could the title be improved to be more explicit (\alpha=...)? 

Otherwise, the example runs well on my box
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'plot_adhd_covariance2.py')"
"pull_request_commit_comment","100","nilearn","nilearn","bthirion","2013-09-11 21:19:22","convergence
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'doc/developers/group_sparse_covariance.rst')"
"pull_request_commit_comment","100","nilearn","nilearn","bthirion","2013-09-11 21:22:01","criterion
","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce","(None, '', u'doc/developers/group_sparse_covariance.rst')"
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-04 12:34:22","First steps on Honorio-Samaras algorithm","b2ebc1d32a4e98c6030c9a1ebf5374da3d0e2e77",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-05 07:54:54","Test signals generator

Code for main algorithm has been completed, but not tested (it has not
been run at all).","5d45f91146e9ee822ad3db651748d4bd1491fbc1",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-05 14:06:07","Corrected some bugs in Newton-Raphson step

Adapted the formula to enforce a positive solution.
The formula used for the second derivative contained an error (bad
sign).","bc18fac522e36fff3c84f1674dad7faafd7215e4",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-07 12:55:09","Algorithm works on test case.

In the current state, the algorithm converges for different test cases.

The function contains **much** code for testing and asserting, that slows
down things a lot.","a8a6866532a258c949fe7f258b0da38b4691fe85",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-07 14:18:30","Added a debugging flag

Removed some test code.","7d19b206fd66bffd2f47ae3ebc67b8f61c106a45",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-19 08:39:00","Working example of honorio-samaras algorithm

This example is really rough, and there is debugging code everywhere.
But this state works.","6ec0d156811feec1ac48c11471e874840e333e33",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-19 10:57:58","Code cleanup","cc6bfa3afdd7f61f1338a7e85874f7649e67d4f8",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-19 15:54:09","Optimized Newton-Raphson step.","a62c134e800b14666da14e7ffa82bb7f91f9b2cb",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-20 09:23:25","Reduced memory consumption in _detrend","cfff05b8fa02cf32bf0649a140e4674fe2bc836d",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-20 13:36:55","Reduced memory usage in _detrend, again

This memory usage reduction applies to all cases, instead of the previous
commit which improved a corner case.","ebb0b14b53bb09d9b56152bc6c41e905836b0dd2",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-20 14:12:49","Created _mean_of_squares

This function will replace a similar computation in high_variance_confounds()
which uses much more memory (execution time is similar, if not smaller).","5148cb88e7a79daf05ae38c99cdd9278031f191b",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-20 15:00:04","Lower memory usage for high_variance_confound","6aac7dc51949557a59cae7bac20f906b70eb4ccd",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-20 16:02:09","Improved performance of H-S algorithm implementation

Replaced a loop by a numpy operation. Around 15% speedup on
plot_adhd_covariance2.py with 40 subjects.","d355f7fa74f8dd03b2e431b80b1a59c3bae70740",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-21 15:02:14","Solved remaining problems after merging.","07a4b00b4260d856fb3d33bdfbcd7096531f1f77",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-24 11:30:27","Renamed honorio_samaras function

New name: group_spare_covariance()
Updated docstring.","97d75f47274b8d7e0e5b235346149da0bae9fc79",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-24 15:50:40","Added a test for group_sparse_covariance

Big code cleanup","a5491161e989fb340ae71b64e248c92d58545e43",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-24 16:31:22","Another optimization in group_sparse_covariance

The computation of an matrix inverse has been replaced by a computation
based on Sherman-Woodbury-Morrison formula. This improves the asymptotic
running time for a large number of variables.","f4b35bfb76d4bc1c0518f2a380883c463a6b03ad",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-25 11:43:00","group_sparse_covariance takes signals as input

The group_sparse_covariance() signature has been changed, to be
consistent with that of analogous functions in scikit-learn.

Added tests for group_sparse_covariance, adapted every example.","135137e76e96c27f6fa91ba642a08d5f5b781604",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-25 13:09:12","Created GroupSparseCovariance

Improved user messages in group_sparse_covariance
Moved LogMixin out of nifti_region.py.","78b5af702062b32f26b1ffbb37b0ba7b3534c682",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-26 13:55:28","Added duality gap computation

The return_costs keyword of group_sparse_covariance nows returns the
duality gap in addition to the primal problem cost.","38425e0d05eebb1d807329cf62ccc3cba2244dee",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-06-26 15:07:52","Small optimization of cost computation

duality gap and objective in group_sparse_covarianceÂ are now computed
inside a single function. This is faster than the previous two-function
implementation.","57674cc1f9f5940f0958c05166fa94427970b5af",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-01 12:26:16","Added duality gap and rho max computations

Duality gap is computed with return_costs=True.

rho max is the value of the regularization parameter above which the
resulting matrix is completely sparse (will be useful for cross-validated
estimator)","9d3c538a6505eb9911d589345cc803a9686bcf99",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-02 11:12:33","First implementation of GroupSparseCovarianceCV","3317621c983fd8b37044babf8bba56dc15eda8c5",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-02 11:49:44","Parallel computation in GroupSparseCovarianceCV","80e94afa47296efd32a720686eca3c6d327d95e3",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-03 08:10:46","Better duality gap computation

The computation now seems to always give positive duality gap.","d8a93d56b46b36cee1638689ded8689a1c2db647",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-03 08:39:56","Added tolerance-based stop criterion

The duality gap is compared to a tolerance value. Computation is
stopped if gap is smaller than tolerance.","75cb98341a947f3c98cbeaa84fba8e81bfbc4bb2",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-03 11:23:34","Added tests for GroupSparseCovariance(CV)

Corrected some bugs thanks to the tests.","f96449e777d8640f4a4890ec9a7350a32c72448b",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-03 14:06:09","Added a warning in group_sparse_covariance

A warning is issued when tolerance is not reached with the prescribed
number of iterations.","eb1e104bdb91ffa723b279b8b94b5d152b9924d4",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-03 14:30:02","Tiny optimization: saved some memory allocation.","47e695e2a7edbcbb995e7d65605b13cdb4fa63b0",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-03 15:28:15","Fixed a spurious warning in a test","df24ce4278d4e2073590fd44b72083711a4dfc58",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-03 16:08:55","Reorganized code

Split group_sparse_covariance in two functions: one that takes signals
as input (for users) and one that takes empirical covariances (for
internal usage). This prepares for some more optimization of the
cross-validation function.","e38173bfb42c91a5261c06dcfbe0b8a2389e1137",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-09 16:03:18","Improved GroupSparseCovarianceCV (faster)

Used restart value to (slightly) reduce convergence time.

Updated documentation
Added functions to generate test signals.","264cad6cd6e30cd28db7bb2dacc1990bf1451486",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-10 12:10:41","Duality gap is not inf anymore

In some cases where the empirical covariance matrices were singular,
the duality gap as computed previously could take infinite values. This
has been fixed by computing another feasible dual point in such cases.
The computed value is not a tight bound, but it is believed to be only
useful far from the optimum.

Added objective value in user messages during computation.","7e4088911a4bb4ef4616d0fce041e2f3de0e201a",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-10 16:00:55","Corrected a bug in signals generation","5c542cb39b113998487bbb4d12a5cd71b7298b24",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-18 14:57:01","Intermediate possibly buggy state","c8c2a2376b87e0f7f1cd7d6b83fbe07dcd3639d4",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-19 16:14:33","Updated generate_sparse_precision_matrix

Added an option to get an corresponding covariance matrix with
only ones on the diagonal.","8275f2a88a59029c36037c1370b13f28fdb88106",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-19 16:15:41","Optimized Newton-Raphson step.

Dropped dependence on scipy for Newton-Raphson.","e227e8217c6e52d1be6389f8b86f8a31e886e6f1",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-19 16:17:07","Code cleanup","b2f238ebd2cb4479908ff88224e193865291ab8f",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-31 13:11:40","Time is returned along with costs","e04fa90c00318a1d62d1695e9eb4a113872de63c",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-31 14:43:50","Changed stopping criterion

Iterations are now stopped when all coefficients in the estimated
precision matrix change less than the tolerance between two
iterations.
This gives kind of a guaranteed number of decimals in the estimated
precision matrices.","82441ec271984fe43d070a6bfca029f5bc5f3536",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-07-31 15:43:33","Minor bug fix.","4fd4d669dc5955258790c1bb809338894cfe0a16",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-01 09:18:33","Probing of group_sparse_covariance

Added keyword ""probe_function"" to group_sparse_covariance. This is a
way to provide a function that is called after each iteration. See
aspect-oriented programming.

Updated the standard message displayed for each iteration, to show the
max norm of the difference between two estimation of the precision
matrices.","a95ecb3bec35d74a08bf3ea83d1d1ac8b8fd26b3",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-01 12:24:47","Remove ""return_costs"" keyword

This keyword is useless because the same feature can be obtained using
a probe.","7fa05c75129a7250300ab4d7e701db2149fb9764",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-01 13:46:23","Code cleanup

Almost every ""dtype"" keyword argument has been removed.","cba4e0ecaa5a4ef3c879fd9319b4220dee85db64",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-01 15:08:27","Optimized group_sparse_covariance()

Gained 20% execution time on a particular case (mainly in the Newton-
Raphson part)","b1643a9132eda07ab638bdb12a9325d61af54010",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-01 15:23:22","One more small optimization in GSC.

GSC: group_sparse_covariance","2afa244b4444a6d424cbd546bbf11fe7a42a5381",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-26 15:22:57","Fixed an import bug.

Added some documentation to group_sparse_covariance()","f9119317c63e14a5c31aa10153691b869d380a04",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-28 07:54:06","Arguments passed to probe function are correct now","a4dd1f784b7e03823b547a41ea1adf54fc840578",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-29 11:23:25","Added early stopping to GroupSparseCovarianceCV

Early stopping also to run cross-validation code faster, by stopping
iteration before reaching convergence according to maximum norm.

group_sparse_score output has been modified: the first value is now
the log-likelihood on test set (instead of the opposite of it: the
sign was changed).

Updated docstrings.","d85db4f8a0abf49f9370202803c3922da05af6f4",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-29 14:00:57","Code cleanup","d5a32175fcb5d029c34fbfc7621ad2b025fc7616",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-29 15:06:32","Started renaming ""rho"" to ""alpha""

Got rid of conflicting usage of the ""alpha"" name.","544ceaee84578be6cef415d1272e226e54cdb5c4",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-29 16:21:16","Code refactoring

- Renamings: task->subject, rho->alpha.
- Fixed a lot of docstrings
- Removed some constructs not compatible with Python 3 (xrange)
- Refactored plot_adhd_covariance2 a lot. Now shorter and much clearer.
- Replaced numpy.linalg.inv by scipy.linalg.inv","363cb21af01cd3c86067882b98eb08e6f4a33a75",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-30 11:31:19","Renamed internal variable

Winv -> W_inv","91c9d2756f4287bb4293f27b7c1a348dcece419f",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-30 12:12:46","Code cleanup (doc, api)

group_sparse_score() renamed to group_sparse_scores() (added an S)","239f2fff548e15f5cb595c48f657427c6af96703",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-30 12:44:36","Code simplification and cleanup

Replace some custom code by sklearn.utils.gen_even_slices","5b3cf6f005d0461c770f5fedf5e546859cab4e71",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-30 15:08:14","Got rid of spurious warning in tests

Docstring and comments fixes.","adf52e0c61e74ac05e8c40f3f706edede4eb6926",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-08-30 15:18:27","generate_group_sparse_gaussian_graphs() in testing","ed8731343cf8374b9603e93f251abde4b6166139",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-02 13:09:58","Improved numerical stability

Added two lines to force symmetric at the end of _update_submatrix.

Some cleanup.","d8590f94ae7afaec4a6228c93d4c688d57965f38",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-02 13:32:01","Merge branch 'master' into honorio_samaras

Conflicts:
	nilearn/input_data/nifti_region.py
	nilearn/tests/__init__.py","0867b2b579f8622b31453078601e66afe21f3ae9",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-02 13:41:39","group_sparse_covariance uses logger.log()","544d3a9801c975462fa3bd6c8cf8d3472170487d",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-02 13:43:40","Remove LogMixin class.","160b8590da99f8b94b054e47e1e2ba01c0aee2b5",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-02 14:06:49","Code cleanup","a2a0f549159b18ad9e724c79b19494e8c7426447",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-04 07:42:00","Code cleanup","13bdc45a39182b911bdb7ebe848bf3f95085c070",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-04 16:28:42","Small docstring fixes","43c24627d31f22ea2f7c9e462ff23dd309dab2f0",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-05 10:54:13","Added a technical documentation page

The documentation is about the group sparse covariance algorithm.
No link to it has been added in index.rst yet.","fca0ea9168e6b3a14b6280f7adc11719f84f19af",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-06 09:20:30","Merged graph lasso and group-sparse examples","c2d820df13a3f91f6407b428cb67c0e1b18413f9",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-06 09:28:47","""standardize"" keyword for empirical_covariances

Standardizing time series is required for numerical stability of group-
sparse covariance optimization. This is technically useful for the
cross-validated version.","6ec7e60209b776aaef9dd52437a19d29aecf8f4b",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-06 12:48:46","[DOC] group-sparse covariance

Added information of parallelism, grid search and warm restart.","ac7c0dee779218197305780f866ef96802b0761d",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-06 13:58:46","Clean up duality gap computation

Move duality gap computation into group_sparse_scores().

Group-sparse covariance estimation is now always performed in
double precision.","c7af1f34d7cbecf347b3039f962b33980e226265",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-09 08:28:57","Separated convergence parameters in CV

In GroupSparseCovarianceCV, tolerance and maximum iteration number can
have different values for the alpha selection phase, and the final
optimization. Two keywords have been added: tol_cv and max_iter_cv.","5ee35b3def2d7b1e15b65b626d406b8da1e5682f",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-10 12:50:51","Example comparing sparse precision estimation","35c8a5d57ac877e47041d9e2e64c779bfae0de11",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-10 12:59:17","Removed ""assume_centered"" keyword in g.s.c.

The ""assume_centered"" keyword has been removed from every function
related to group-sparse covariance (useless).","63375299bad2c7374d372d1ce17228354f187a71",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-11 08:32:37","[DOC] fixed paragraph on data extraction

plot_adhd_covariance.py example had changed a lot, the rst file was
referencing lines that didn't exist anymore.","2b2a15c8e2b156b35fa66130876a5a77e87d4489",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-11 12:01:39","[BUG] compatibility with sklearn 0.10-0.14","8acb04971d8f06887521fbdc6a17589390003c79",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-11 12:38:11","[DOC] functional connectivity page

And a page on the technical aspects of the group-sparse covariance
implementation.

Also fixed some bugs.","47bddb156c7406532560b7c679e430606a162de0",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-11 13:06:11","Merge branch 'master' into honorio_samaras

Conflicts:
	doc/index.rst","33f19ea7355c58237cd07fa2ef4b48d09bca4850",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-11 13:34:35","[TST] Fix failing test

A test is failing on Travis (np.diff(objective) <= 0 in
test_group_sparse_covariance.py:51), that does not occur of the
developer's machine. np.diff(objective) may be hitting the numerical
noise floor (1e-14 or so). Some values may then be slightly positive.
This is most probably due to a different numerical library which leads to
a slightly different result.

The fix consists in decreasing the number of iterations so that optimization
stops before hitting the noise floor.","8647d78eff16492804d460e5666e91c9a8fb9ea5",""
"pull_request_commit","100","nilearn","nilearn","pgervais","2013-09-13 07:39:01","[DOC] fixed some typos","a895fded9e5cc73dd3ce7f03c1896e1fbf7d83ce",""
"pull_request_commit_comment","100","nilearn","nilearn","bthirion","2013-09-11 21:08:06","gives
","47bddb156c7406532560b7c679e430606a162de0","(152, 152, u'doc/functional_connectivity.rst')"
