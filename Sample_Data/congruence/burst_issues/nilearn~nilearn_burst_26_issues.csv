,rectype,issueid,project_owner,project_name,actor,time,text,action,title
0,rectype,issueid,project_owner,project_name,actor,time,text,action,title
1,issue_title,146,nilearn,nilearn,AlexandreAbraham,2013-12-06 15:37:51,"Add an image manipulation example and the corresponding documentation page.

This could be in conflict with the existing documentation `manipulating_mr_images` so we may merge them or rename the one created there.
",start issue,Image manipulation example
2,issue_closed,146,nilearn,nilearn,GaelVaroquaux,2013-12-11 02:44:29,,closed issue,Image manipulation example
3,pull_request_title,146,nilearn,nilearn,AlexandreAbraham,2013-12-06 15:37:51,"Add an image manipulation example and the corresponding documentation page.

This could be in conflict with the existing documentation `manipulating_mr_images` so we may merge them or rename the one created there.
",537beea50c0a311d3bf9e78c17ae67c275179973,Image manipulation example
4,pull_request_merged,146,nilearn,nilearn,GaelVaroquaux,2013-12-11 02:44:29,Image manipulation example,c59ddc2b3ca1fbcbd26b4a6605abe596f5fbb77c,Pull request merge from AlexandreAbraham/nilearn:course to nilearn/nilearn:master
5,issue_comment,146,nilearn,nilearn,GaelVaroquaux,2013-12-06 21:26:54,"I have been thinking about how this can be integrated in the current documentation and I think that a good option would be to add it at the end of the manipulating_mr_images.rst file. It will require reworking a bit wording, but I think that this is where is makes more sens.
",,
6,issue_comment,146,nilearn,nilearn,GaelVaroquaux,2013-12-11 01:34:06,"I'll be merging this guy in the plane tomorrow. Thanks a lot for the excellent work!
",,
7,pull_request_commit_comment,146,nilearn,nilearn,GaelVaroquaux,2013-12-06 16:00:04,"That link label seems wrong.
",537beea50c0a311d3bf9e78c17ae67c275179973,"(None, '', u'doc/image_manipulation.rst')"
8,pull_request_commit_comment,146,nilearn,nilearn,GaelVaroquaux,2013-12-06 16:00:49,"We do not want to use the word 'tutorial' anymore: let's talk about 'page', or 'section', as we are designing this to be part of a documentation.
",537beea50c0a311d3bf9e78c17ae67c275179973,"(None, '', u'doc/image_manipulation.rst')"
9,pull_request_commit,146,nilearn,nilearn,AlexandreAbraham,2013-12-06 08:58:58,Add image manipulation script and doc,b33d3899a4f803df1b6a0e988d1cb859cb49f9f9,
10,pull_request_commit,146,nilearn,nilearn,AlexandreAbraham,2013-12-06 15:35:07,Use smooth function. Integrate images in text.,7335eee62edc8670266a8a7889dac1d19f072701,
11,pull_request_commit,146,nilearn,nilearn,AlexandreAbraham,2013-12-06 15:41:04,Add title to figures in image manipulation example.,c1edcd06db88e40c4158af7feabfe23b7209f552,
12,pull_request_commit,146,nilearn,nilearn,AlexandreAbraham,2013-12-09 10:11:15,Fix typo in Analyze file format,6f270a1cf4a2ccf17cf79e713168acca06faa960,
13,pull_request_commit,146,nilearn,nilearn,AlexandreAbraham,2013-12-10 10:39:50,Merge image manipulation script into vizualisation,593949f19f688ad1f74d8f2758f955829c6cb8cd,
14,pull_request_commit,146,nilearn,nilearn,AlexandreAbraham,2013-12-10 10:40:06,Start merging documentation,2e881438f0846398bb09c3fc69685c33cce2f37a,
15,pull_request_commit,146,nilearn,nilearn,AlexandreAbraham,2013-12-10 13:34:09,Fix code inclusions,0fe4cb12ab87ee4e60bae1248d5ad5dacff6ea7a,
16,pull_request_commit,146,nilearn,nilearn,AlexandreAbraham,2013-12-10 13:34:49,Clean old files,537beea50c0a311d3bf9e78c17ae67c275179973,
0,rectype,issueid,project_owner,project_name,actor,time,text,action,title
1,issue_title,149,nilearn,nilearn,mekman,2013-12-13 16:06:46,"Hi all,

thanks for the great package. Nilearn looks really great. I'm sure many people will find it useful in the future. Very cool stuff!

Offending line:
https://github.com/nilearn/nilearn/blob/master/nilearn/decoding/searchlight.py#L166

I'll file a PR soon.

kind regards,
 Matthias
",start issue,BUG: searchlight scoring parameter not correctly passed on
2,issue_closed,149,nilearn,nilearn,GaelVaroquaux,2013-12-15 16:35:30,,closed issue,BUG: searchlight scoring parameter not correctly passed on
3,issue_comment,149,nilearn,nilearn,GaelVaroquaux,2013-12-15 16:35:28,"Thanks for the report. I am however closing this as a ""won't fix"", given that, if I understand things right, it works under more recent versions of scikit-learn.
",,
0,rectype,issueid,project_owner,project_name,actor,time,text,action,title
1,issue_title,144,nilearn,nilearn,eickenberg,2013-12-02 23:24:21,"Here is a start. Data loads, can be looked at and is processed according to the description in the paper.

Probably can be simplified a lot in many places, but it's a start.
",start issue,WIP: Haxby data processing example
2,issue_closed,144,nilearn,nilearn,GaelVaroquaux,2013-12-11 23:11:58,,closed issue,WIP: Haxby data processing example
3,pull_request_title,144,nilearn,nilearn,eickenberg,2013-12-02 23:24:21,"Here is a start. Data loads, can be looked at and is processed according to the description in the paper.

Probably can be simplified a lot in many places, but it's a start.
",197ac7da87011f8e96174a91861bef30addaa0d2,WIP: Haxby data processing example
4,issue_comment,144,nilearn,nilearn,fabianp,2013-12-03 08:30:34,"Would it be possible to reuse the maps in dataset_files.func instead of redoing the GLM? it seems to me that there are the response for the 8+1 categories
",,
5,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:06:41,"Can we rename the script to plot_....py, so that it is compiled during building of the webpage.
",,
6,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:27:52,"Indeed, I agree with Fabian that the GLM is confusing.

It poses a bit of a problem, because the data in .func are not beta maps, but raw EPIs. In general, it is never a good idea to use the raw EPIs. With the Haxby dataset, we do it anyhow, but it's not great.

Ideally, we would have an object to estimate the betas from conditions. We don't have it currently. I don't know what is our best option here.
",,
7,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-03 09:55:57,"The structure of the GLM is taken from the paper and is rather specific. Looks like this is what worked best after trying a bunch of stuff. I don't think one can make an easily understandable object that generates this type of design matrix. But of course there is potential to simplify the code generating the design.
I deliberately used sklearn.linear_model.LinearRegression in order to avoid using linalg.pinv etc
",,
8,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:59:04,"Thinking more about the issue of the GLM: let's do as in the original article. I don't think that they used a GLM.
",,
9,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 10:00:19,"Ok, I need more time to understand the problem (right now I am multitasking too much).
",,
10,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 10:01:40,"Also, it would be good to have @bthirion's opinion on that.
",,
11,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-03 11:54:06,"They used a GLM and then computed similarities between conditions between even and odd runs.

I am not completely sure yet how they did their testing, since this leaves relatively few values for doing statistics

There is a very simple but pretty effective SVM decoding routine in the script I wrote. It currently reaches a .8 classification average over 9 classes. It is learned on single TRs. But it is just ""a stub"" in the sense that I did the absolutely simplest thing possible, not even removing rest data, but predicting it as a label.
",,
12,issue_comment,144,nilearn,nilearn,bthirion,2013-12-03 21:38:29,"As discussed today, the GLM is just meant to create averages something like
X_avg = np.array([X[y==i].mean(0) for in in np.unique(y)])
should suffice.
But, more importantly, classifier-based experiments are more compelling 
than cross-session correlations, so we may just drop this part of 
Haxby's historical experiments.
",,
13,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-04 14:00:36,"All GLMs gone, two SVM experiments coded.
To be optimized and possibly extended with logistic regression etc
",,
14,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-05 15:30:35,"yup, kernel=linear helps ... a lot ... especially in full brain
",,
15,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-05 15:31:45,"I did look at the cimputed epi mask, and to me it looked just fine. Then again, I only looked at some slices. Where do you think is the problem? Is it too small?
",,
16,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-05 15:34:53,"Question: I know I am supposed to kick out the resting state intervals ... right now I am predicting them with the label ""rest"". If I do cut them out, I get the opportunity to do some hemodynamic shifting (I could do it without cutting, but still). Do I lag the data (TR=2.5s, 2TR=5s) or do I leave it as is?
",,
17,issue_comment,144,nilearn,nilearn,bthirion,2013-12-05 15:50:21,"Leave it as is.
",,
18,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-06 16:44:58,"This is getting closer and closer to plot_haxby_decoding.py

Which direction do we take? Merge this with plot_haxby_decoding or make it into something different / more elaborate?
",,
19,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-06 16:58:50,"I had in mind that this example would be a bit more elaborate than the plot_haxby_decoding and stick more the to spirit of the original article: working on masks rather than doing a full brain analysis with anova, and reporting prediction power with bar plots for different categories of objects and different parts of the brain (using the masks provided with the dataset).

Does this sound reasonable?
",,
20,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-06 18:07:34,"Added code to look at other ROIs.

Not sure what to put in box plot since as of now scores are global classification accuracy.

Am I supposed to look at per class scores in different regions?
",,
21,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-06 18:17:23,"Well, I think that to answer thé neuroscientific question that was asked in thé original paper, scores on différent types of stimuli must be reported.
",,
22,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-06 18:51:57,"Definitely,

however I am afraid of the implications on code readability: I cannot create a scorer that returns multiple scores -- this will be blocked by cross_val_score as of one of the recent sklearn releases.

I can do everything by hand -- but in that case switching to one v one may be more understandable in terms of the code
",,
23,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-06 18:58:58,"forget about the last statement about one v one.

What I could do is write a scoring function that for each class returns true (and maybe false) positive scores.

And do a list comprehension iterating over a cv=KFold instead of a cross_val_score.

Does that sound like a compromise?
",,
24,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-06 21:31:23,"I would indeed do a manual for loop. The question is whether we simply
want to do the 'one vs all' manually, in a for loop, without even calling
it one vs all. That should probably be quite simple and not require
notions that beginners do not understand.
",,
25,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-08 21:35:01,"The latest refactoring commit adds a rudimentary visualization of the f1_scores

https://github.com/eickenberg/nilearn/commit/c7c44e7010ee2e259d172ab7dcd91c3e99613f30

Performance is very bad except in mask_vt and mask_house. Very bad as in f1_score = 0.0

I have written (but not yet pushed) a cross-validation routine over the SVM C parameter. It looks like C > 1, e.g. C=1000 sometimes helps. The loop takes pretty long though. I was actually going to add a loop over classweights, since this had been discussed. But I think all this is very time consuming.

Any ideas on how to proceed? Let me know if I should push the crossvalidation routine
",,
26,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 21:46:17,"> Don't push the cross-validation routine.

To justify this: the goal is to have a very simple example, not the best
performing one. We want to be tutorial.
",,
27,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 21:46:26,"> Any ideas on how to proceed? Let me know if I should push the
> crossvalidation routine

Don't push the cross-validation routine. However, it is really good that
you have found that it helps a lot. I will use it in my tutorial to
stress the importance of cross-validation.
",,
28,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-08 22:33:46,"Ok. I was going to up the penalty to 100. (Not overfitting, of course -- I could have chosen this by chance, right? :))

But this makes the SVM have great difficulties treating the difficult stuff. I have the feeling it goes to max_iter before  converging. A waste of time that I won't push either.

The crossvalidation scheme will be on another branch once it is done.
",,
29,issue_comment,144,nilearn,nilearn,eickenberg,2013-12-09 21:37:18,"As an alternative I just made a one versus one scheme which shows scores for all couplings of labels in a matrix. One for each region. So if necessary I can also push that up here.
",,
30,issue_comment,144,nilearn,nilearn,fabianp,2013-12-11 08:53:17,"I've looked a bit into alternative classifiers. The mean of the scores_mean matrix is 0.26 using a LinearSVC as in the example and jumps to 0.30 if using a LogisticRegression with l1 penalty (default parameters). 

I plotted the difference is score in the confusion matrix (I can fix the colorbar and make it easier to read if you think it would be useful to show).

![difference](https://f.cloud.github.com/assets/277639/1722245/1e915b34-623d-11e3-9672-f4f80c3ef594.png)

so you win on almost all tasks (but not all!). Other stuff that I tried and did not get better results are Anova-SVM,  ensemble methods (RandomForestClassifier) and (surprisingly) a LogisticRegression with elasticnet penalty.

Please tell me if you would me to make a patch (in a single file or as an option for Michel's code?) for this or if you plan to say this in the oral/slides.
",,
31,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-11 19:27:57,"> Please tell me if you would me to make a patch (in a single file or as
> an option for Michel's code?)

That would be fantastic. As a single file, ie a new example. Thanks a
lot!
",,
32,issue_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-11 23:11:57,"Merged. Thank you
",,
33,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:10:14,"I think that we should maybe move this in another script. We are going to drown the beginning under too many things.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
34,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:10:56,"I would prefer if you used scipy.misc.imread, rather than the PIL
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
35,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:12:02,"We have decided to only use ""import matplotlib.pyplot as plt"" rather than ""pylab as pl""
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
36,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:13:57,"Ideally we want to use the NiftiMasker. We loose people when we go in these details.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
37,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:14:31,"Using strings are comments is going to confuse a lot beginners.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
38,pull_request_commit_comment,144,nilearn,nilearn,AlexandreAbraham,2013-12-03 09:18:13,"This is useful code anyway. The most important fact is that it points out that stimuli are not returned with absolute paths, which make them unusable for the end-user. I have fixed that and here is the same code to show them:

```
if take_a_look_at_stimuli:
    import Image
    import pylab as pl
    for stim_type, files in data_files.stimuli.items():
        if stim_type == ""controls"":
            # skip control images, there are too many
            continue
        pl.figure()
        for i in range(48):
            pl.subplot(6, 8, i + 1)
            try:
                pl.imshow(Image.open(files[i]), origin=""lower"")
                pl.gray()
                pl.axis(""off"")
            except:
                # just go to the next one if the file is not present
                continue
        pl.title(stim_type)
    pl.show()
```
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
39,pull_request_commit_comment,144,nilearn,nilearn,AlexandreAbraham,2013-12-03 09:25:27,"Awesome. `scipy.misc.imread` does not have the bug of random flipped images !
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
40,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:43:17,"> This is useful code anyway.

It is. I just think that it should not be there.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
41,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-03 09:45:51,"Cool! Do I update this section with Alex's code and scipy.misc.imread or do I remove it altogether?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
42,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-03 09:46:25,"OK, will correct and do this in the future
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
43,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-03 09:47:39,"> Cool! Do I update this section with Alex's code and scipy.misc.imread or do I
> remove it altogether?

Move it to a separate file and update it, please.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
44,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-03 09:51:35,"Two things:
1) When I tell NiftiMasker standardize=True, then the masked BOLD timecourses have neither zero mean nor unit variance. Not completely sure what is going on there. @AlexandreAbraham is looking into it I think. Otherwise I can do the same

2) Independently of this, the data seems to be naturally structured into 12 blocks of 121TR, which were probably acquired in different scanner runs. Detrending over the borders of these blocks makes no sense and neither does standardizing. If we want to use the NiftiMasker, we need to either break the file into these blocks and then mask or be able to pass breakpoints to the masker, such that it can pass them on using the bp kwarg to scipy.signal.detrend.

Seems overspecific but at the same time a rather recurrent phenomenon to me. Any thoughts?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
45,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-03 09:52:08,"Just trying to distinguish citations from comments. Will remove this.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
46,pull_request_commit_comment,144,nilearn,nilearn,AlexandreAbraham,2013-12-03 10:16:21,"I have tested and see no problem with the NiftiMasker.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
47,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-03 11:50:49,"Ok, do I gloss over boundary problems and just let the niftimasker do its job globally or do I take into account the structure of the data?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'haxby_full_analysis.py')"
48,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 13:27:47,"don't you want kernel='linear' ?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
49,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 13:28:32,"""ANOVA + Linear SVM C=1 on full brain""
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
50,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 13:29:27,"import matplotlib.pyplot as plt
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
51,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 15:34:06,"pep8 line too long
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
52,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-05 19:42:30,"Can you not do a function. I don't think that it is useful here, and it makes the example more complex.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
53,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 19:47:57,"the function is used in the other script
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
54,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:27:53,"matplotlib.pyplot
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
55,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:28:06,"remove these comments
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
56,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:33:50,"n_jobs=12 ???

use 1 by default please. For the poor people :)
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
57,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:34:30,"I would print the std rather than all the scores.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
58,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:36:32,"I get 0.69 mean score on full brain. Do you confirm?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
59,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-05 20:37:51,"import matplotlib.pyplot as plt
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
60,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-05 22:04:23,"I get .87 on full brain and .86 in the ROI.

this is without removing rest. Unfortunately I have uncommitted stuff on the other machine so I prefer continuing tomorrow
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
61,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-05 22:06:14,"I can remove the function call from the plot_haxby_full_analysis.py if it seems unncessary and change the stimulus visualization to script style. The reason for making a function was only this.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
62,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-05 22:07:10,"0.69 is what I got without kernel=""linear"" on full brain btw
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
63,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-06 06:23:51,"> the function is used in the other script

But I would rather if this would not be the case. It forces to explain
way too many things when all we are trying to teach people is basic
nilearn usage.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
64,pull_request_commit_comment,144,nilearn,nilearn,bthirion,2013-12-06 22:23:08,"cv=12 is supposed to amount to a leave-on-session-out cross-validation, I guess ? 
This is not obvious.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
65,pull_request_commit_comment,144,nilearn,nilearn,bthirion,2013-12-06 22:46:05,"Score: 0.67 +- 0.09 # on my laptop. I should run my experiments on your machine... what is you is number ?
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
66,pull_request_commit_comment,144,nilearn,nilearn,bthirion,2013-12-06 22:47:48,"Score: 0.60 +- 0.13 # here. After the tutorial, people will not only want to use nilearn, but use it on your box...
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
67,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-06 23:38:58,"is146139
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
68,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-06 23:39:49,"This is definitely weird...
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
69,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-06 23:41:40,"hmm, same on my laptop now. Will compare all software versions Monday
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
70,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-06 23:42:43,"got the same on my laptop.

I guess my computer is really powerful ...
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
71,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-07 18:34:17,"+1 for using LeaveOneLabelOut to make it explicit
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
72,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 01:17:43,"To make the example simpler, I would completely forgo the ANOVA + SVM part.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
73,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 01:19:34,"You need to give an rst title in the docstring, for rendering in the example gallery.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_stimuli.py')"
74,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 01:22:52,"Is there a reason that the vt mask is treated differently from the two others (eg face vs house)?

I think that the way that I would architecture that script is that I would do a loop over the masks names and a loop over the subjects inside that to produce the different values per subject and per mask, and end up with bar plots (or some other nice summary figure).
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
75,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-08 20:18:31,"No reason, except that I hadn't refactored yet, after adding the new functionality.

Restructuring done, but need a little more to make it nice.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
76,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-08 20:19:29,"I agree. 
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
77,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-08 20:22:02,"This may  be related to different scoring standards in possibly different versions of sklearn. 

Not sure though.

In any case, I will have to deal with a strong class imbalance problem while doing OvR for each label. Chance level for 0-1 scoring is at 87.5%
I am now going for f1_score. Any reason why I shouldn't?
If I go for AUC, I will have to dig into the classifier and move an offset around, which may not be ideal for a simple example
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
78,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-08 20:22:28,"yes
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
79,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-08 20:33:27,"F1_score is good. Thé standard accuracy score -by default- doesn't work if se do simple per-category scoring?eickenberg notifications@github.com a écrit :In plot_haxby_full_analysis.py:

> -    all_timecourses = brain_masker.fit_transform(data_files.func[subject_id])
> -    from sklearn.feature_selection import f_classif, SelectKBest
> -    from sklearn.pipeline import Pipeline
> -    feature_selection = SelectKBest(f_classif, 500)
> -    pipeline = Pipeline([(""Feature selection"", feature_selection),
> -                         (""Classifier"", classifier)])
>   +
> -    scores_anova_svm = cross_val_score(pipeline,
> -                                       all_timecourses[resting_state == False], 
> -                                       labels['labels'][resting_state == False],
> -                                       cv=12, n_jobs=1,
> -                                       verbose=True)
> -    # With resting removed state this scores at .91 (with RS: .87)
> -    print ""ANOVA + Linear SVM C=1 on full brain""
> -    print ""Score: %1.2f +- %1.2f"" % (scores_anova_svm.mean(),
> -                                     scores_anova_svm.std())
>   This may be related to different scoring standards in possibly different versions of sklearn. 

Not sure though.

In any case, I will have to deal with a strong class imbalance problem while doing OvR for each label. Chance level for 0-1 scoring is at 87.5%
I am now going for f1_score. Any reason why I shouldn't?
If I go for AUC, I will have to dig into the classifier and move an offset around, which may not be ideal for a simple example

—
Reply to this email directly or view it on GitHub.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
80,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-10 08:43:34,"same here you need title for rst rendering
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
81,pull_request_commit_comment,144,nilearn,nilearn,agramfort,2013-12-10 08:45:15,"the figure layout is not perfect. Text goes out on the figure on my box.

Also I think the colorbar should reflect where the chance level is. It looks too black I feel.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
82,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-10 15:08:09,"Fixed the figure layout.

The scoring is done using f1_score. Where it is black I get no recall at all. So the score is bad.

If I use 0/1 scoring, the one vs rest chance level is at 87.5 % ....
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
83,pull_request_commit_comment,144,nilearn,nilearn,eickenberg,2013-12-10 15:08:31,"was did
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
84,pull_request_commit_comment,144,nilearn,nilearn,GaelVaroquaux,2013-12-11 00:46:31,"Thanks a lot. I'll be in the plane tomorrow, I think that I'll merge this
in, and modify a bit the example when I am in the plane, as I need to
converge on the course.
",197ac7da87011f8e96174a91861bef30addaa0d2,"(None, '', u'plot_haxby_full_analysis.py')"
85,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-02 18:39:07,Most of the data loading and pre-treatment done,b372dfb21b577d3c10763630a30f6f324b418961,
86,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-02 20:22:00,Going the wrong way. Should be using beta maps. Will correct,345b63edfdaacc571561f00b947537c714c1899b,
87,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-02 23:22:22,Correlation Matrix and rudimentary svm in cross_val,59755c88e407fa106682b470f810a8b98db93c36,
88,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-03 10:35:01,commit before merge,cf3a150a84a7c74a6cf6cf05c2bf83376662a7f8,
89,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-03 10:35:05,Merge branch 'master' of github.com:nilearn/nilearn into haxby_full,8323dc8c5fb2aee08b637c4f502ab5299894089d,
90,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-03 11:44:56,Exported function for showing stimuli to another example file,52a558a88149852488d282667b68586428cbb787,
91,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-03 11:49:37,changed pl to plt everywhere,f266cf7609e4aba90a277038b415e02fa4af4ab0,
92,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-03 12:12:29,Removed comments in quotes. Replaced by frame of # signs,2d0150fc271fd0c1babadc0c930fcf1c58c1007b,
93,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-04 13:40:06,Removed @@@@@ Michael comments in source in favour of discussion in diff and cleaner code,4f9892220665bfca79511c642b015aa8f85739ac,
94,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-04 13:40:41,Merge branch 'master' of github.com:nilearn/nilearn into haxby_full,bacb46e75d704ed992f1a103bd12f17d47644b0d,
95,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-04 13:59:14,"Scrapped the whole GLM + similarity matrix stuff in favour of two decoding schemes, one in the given ventral temporal ROI, one on full brain. Both perform way above chance, but the ROI one is better",81d0aedfa135b33d8384eb74b7200634fc0c95a8,
96,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-05 15:15:42,why is the default kernel of that svm package the rbf???,834736dfad9eeac9ccdd02d554305e49aa3dc526,
97,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-05 15:17:17,changed prints and comments,9a28c07bd9a70395f2930e24c9c9b1bb603e4c33,
98,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-05 15:18:13,pylab -> matplotlib.pylab,e80c4f8ce0fb3bdffaa4ccdac1604e49517e369b,
99,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-05 15:19:04,"updated svm scores in comment - yes, linear does better",9464e6396511136199efe3b98142726755ef27c9,
100,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-05 15:35:23,PEP8,f3bcf6e62dd1298ae84726485fdfd4cf4b45bbbe,
101,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-05 15:37:27,removed unnecessary lines,a8d3d6d4846860229bea567620cbdf7db43e3119,
102,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-06 09:01:52,"Removed some comments, set n_jobs to 1, print mean score + stdev, changed pylab to pyplot",6b8dfe6fd528a65dcb3065d78baf309bb8df4da0,
103,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-06 09:04:55,turned haxby stimulus presentation back into script,f2fa47eee854163b1d421eaf16fcf6790f477c00,
104,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-06 16:41:36,"removed resting state blocks. Makes scores better on full brain, worse in ROI. scores are marked in code",e625b0cba00c2bdc87ec99ae0d9d87d5cf3abe2d,
105,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-06 17:55:45,"Scrapped detrending for the ROI, scores up to same level as full brain ...... is this scary?",7c97791383adb5c641492ca424c61d8028385e81,
106,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-06 18:06:20,taking a look at the other ROIs in the dataset,cdb90fe1c5dffa3611f785a5e016d4ea918eafc0,
107,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-08 20:16:30,"refactoring script. One cross-validation per label and mask. Changed scoring to f1 precision/recall, because 0-1 score chance level is at .875",f68b316c3ce64c37844fbfbe0bcac0597934af7f,
108,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-08 20:23:53,refactoring,99e57da01db8c5fa3760b63f693838f77b1481cb,
109,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-08 21:03:41,more refactoring,c7c44e7010ee2e259d172ab7dcd91c3e99613f30,
110,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-10 14:54:32,RST,690ad0243dfd3aa535d81006605f695062d289b9,
111,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-10 15:04:42,fixed plotting issue,3f57ab1fed0999aa6584931e96e4f68ef7a54c00,
112,pull_request_commit,144,nilearn,nilearn,eickenberg,2013-12-10 15:05:34,set SVM C back to 1,197ac7da87011f8e96174a91861bef30addaa0d2,
113,pull_request_commit_comment,144,nilearn,nilearn,AlexandreAbraham,2013-12-05 13:34:09,"Have you looked at the mask ? In my memories, `compute_epi_mask` has bad results on Haxby with default parameters.
",81d0aedfa135b33d8384eb74b7200634fc0c95a8,"(135, 103, u'plot_haxby_full_analysis.py')"
0,rectype,issueid,project_owner,project_name,actor,time,text,action,title
1,issue_title,145,nilearn,nilearn,rphlypo,2013-12-05 12:43:51,"Adding a mathematical closing operation after the mathematical opening to avoid spurious gaps due to the thresholding observed in some images.

At this moment the `closing` operator must be smaller than or equal to the  ̀opening` operator, any comments or suggestions are welcome.
",start issue,added a closing operation to the masker to avoid 'gaps' in the mask
2,issue_closed,145,nilearn,nilearn,GaelVaroquaux,2014-01-31 09:58:13,,closed issue,added a closing operation to the masker to avoid 'gaps' in the mask
3,pull_request_title,145,nilearn,nilearn,rphlypo,2013-12-05 12:43:51,"Adding a mathematical closing operation after the mathematical opening to avoid spurious gaps due to the thresholding observed in some images.

At this moment the `closing` operator must be smaller than or equal to the  ̀opening` operator, any comments or suggestions are welcome.
",4d2dc1fc8428b6310ec2356bd85816c154608e43,added a closing operation to the masker to avoid 'gaps' in the mask
4,pull_request_merged,145,nilearn,nilearn,GaelVaroquaux,2014-01-31 09:58:13,added a closing operation to the masker to avoid 'gaps' in the mask,f78748d0a36443b74a3d289b48bcd7eda74f1013,Pull request merge from rphlypo/nilearn:mask_closing to nilearn/nilearn:master
5,issue_comment,145,nilearn,nilearn,AlexandreAbraham,2013-12-05 13:05:28,"It happens to me that one or two voxels remains unselected so I'm +1 for this PR.
",,
6,issue_comment,145,nilearn,nilearn,AlexandreAbraham,2013-12-05 13:47:10,"As discussed, should we replace closing by a call to `fill_holes` ?
",,
7,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2013-12-05 15:07:06,"> As discussed, should we replace closing by a call to fill_holes ?

No, I think that I prefer the closing. Indeed, some of the inner
ventricles may appear as holes in a brain mask. That's a good thing.

+1 for the closing. Thanks Ronald.
",,
8,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2013-12-05 19:37:07,"From a very general standpoint, I am a bit worried by adding on more parameter, that non experts will not be able to set, especially since it is off by default.

This raises 2 questions:
- should it be on by default?
- can we merge it with the mask_opening, by saying that we do n openings, and (n-1) closings. Of course the name of the argument will have to be changed.
",,
9,issue_comment,145,nilearn,nilearn,rphlypo,2013-12-06 07:35:08,"I do agree with the additional parameter being a little cumbersome, and I do agree that we could set the number of openings and closings equal to each other, as such uniting these in a single operation.

The closings are on by default if a `NiftiMasker` object is initiated, but are off by default in the lower level function `compute_epi_mask` (this choice for backward compatibility). If we integrate both `opening` and `closing` into a single parameter/operation, I assume this question is automatically dealt with.

+1 for a simplified user interface with a single parameter, merging `opening` and `closing` into a single operation.
",,
10,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2013-12-06 15:32:19,"> I do agree that we could set the number of openings and closings equal to each
> other, as such uniting these in a single operation.

OK, then we need to find a name for this parameter :)

> The closings are on by default if a NiftiMasker object is initiated, but are
> off by default in the lower level function compute_epi_mask (this choice for
> backward compatibility).

Let's not bother about backward compatibility for a package that isn't
even released.
",,
11,issue_comment,145,nilearn,nilearn,bthirion,2013-12-09 22:26:18,"The PR is technically correct, though I'm worried that we keep fixing this function with an ever-increasing list of parameters. Obviously, it is not possible to reduce it if the closing has to be smaller than the opening. 
Actually, I would have thought that people would prefer to rely on anatomically derived masks, I order to keep only gm. Are there so many datasets without anatomy provided with functional data ? Or any other pb behind ?
",,
12,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2013-12-09 23:16:08,"We decided not to merge it with the current set of parameters, but to
contract the closing and opening in a single parameter.
",,
13,issue_comment,145,nilearn,nilearn,AlexandreAbraham,2014-01-30 15:18:43,"@rphlypo When addressing a comment, please reply so that people get notifications about it.
",,
14,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2014-01-30 15:58:28,"Small remark on the docstring: @rphlypo could you please check that I am not mistaken and correct. Beyond that, :+1: for merge.
",,
15,issue_comment,145,nilearn,nilearn,rphlypo,2014-01-30 16:36:20,"docstring has been updated as per the request of @GaelVaroquaux 
",,
16,issue_comment,145,nilearn,nilearn,rphlypo,2014-01-30 16:37:23,"file has undergone an autoPEP8 for full PEP8 compliance
",,
17,issue_comment,145,nilearn,nilearn,GaelVaroquaux,2014-01-30 16:40:26,":+1: for merge. Thanks!
",,
18,pull_request_commit_comment,145,nilearn,nilearn,GaelVaroquaux,2014-01-30 15:57:00,"The docstring isn't completely clear to me. However, I have the impression that this is not what the code does: the code does first 2*n dilations, followed by n erosions.
",4d2dc1fc8428b6310ec2356bd85816c154608e43,"(None, '', u'nilearn/masking.py')"
19,pull_request_commit,145,nilearn,nilearn,ronald.phlypo@inria.fr,2013-12-05 12:39:48,added a closing operation to the masker to avoid 'gaps' in the mask,c8e350c34c980ef70048439d1ee8606ccb52dfd0,
20,pull_request_commit,145,nilearn,nilearn,ronald.phlypo@inria.fr,2013-12-10 07:53:09,merged opening and closing into the single parameter opening,af535765ccd71496b5c6b2423a1320abfb7bab14,
21,pull_request_commit,145,nilearn,nilearn,ronald.phlypo@inria.fr,2013-12-10 08:03:51,changed parameters in nifti-masker to only keep opening,8697a8b06807448400ad2a31ba783f95ad510b8d,
22,pull_request_commit,145,nilearn,nilearn,ronald.phlypo@inria.fr,2013-12-10 08:07:34,changed parameters in nifti-masker to only keep opening & removed trailing tab-character on blank line,d028169402f12a79c1435958e6a0236e34c22ece,
23,pull_request_commit,145,nilearn,nilearn,ronald.phlypo@inria.fr,2014-01-22 07:44:13,Merge branch 'master' of https://github.com/nilearn/nilearn into mask_closing,e4e11e8ef1245820506741400d4d05a865ab5494,
24,pull_request_commit,145,nilearn,nilearn,ronald.phlypo@inria.fr,2014-01-30 16:34:10,clarified docstring as per the request of G. Varoquaux,4d2dc1fc8428b6310ec2356bd85816c154608e43,
25,pull_request_commit_comment,145,nilearn,nilearn,rphlypo,2013-12-10 08:01:06,"Kept `opening` as the single parameter, since it is the opening that is of major importance to estimate the largest connected set from which the mask is derived.
",af535765ccd71496b5c6b2423a1320abfb7bab14,"(None, None, None)"
0,rectype,issueid,project_owner,project_name,actor,time,text,action,title
1,issue_title,150,nilearn,nilearn,mekman,2013-12-13 16:07:52,"addressing issue #149
",start issue,FIX: scoring not passed on to cross_val_score 
2,issue_closed,150,nilearn,nilearn,GaelVaroquaux,2013-12-15 15:34:44,,closed issue,FIX: scoring not passed on to cross_val_score 
3,pull_request_title,150,nilearn,nilearn,mekman,2013-12-13 16:07:52,"addressing issue #149
",261ec19e9ffec8a87b73db93bdd51673a5b3e646,FIX: scoring not passed on to cross_val_score 
4,issue_comment,150,nilearn,nilearn,mekman,2013-12-13 16:37:08,"Sorry about that! Could someone please give some advise on how to deal with that? The tests assuming a recent sklearn version where scoring can be specified as string. However the documentation states that nilearn depends on `Scikit-learn >= 0.12.1`. 

Do you prefer reverting back to the deprecated sklearn `score_func` or rely on a more recent sklearn version?
",,
5,issue_comment,150,nilearn,nilearn,agramfort,2013-12-13 16:18:50,"travis failed due to old version of sklearn...
",,
6,issue_comment,150,nilearn,nilearn,agramfort,2013-12-13 20:18:57,"ping @GaelVaroquaux @AlexandreAbraham
",,
7,issue_comment,150,nilearn,nilearn,agramfort,2013-12-13 20:35:43,"how do you handle the travis issue?
",,
8,issue_comment,150,nilearn,nilearn,GaelVaroquaux,2013-12-13 20:34:37,"> ping @GaelVaroquaux @AlexandreAbraham

This feature will work only with a recent scikit-learn. Given that
nilearn is not even yet released, we don't want to start doing backports
and hacks.
",,
9,issue_comment,150,nilearn,nilearn,GaelVaroquaux,2013-12-13 20:48:44,"> how do you handle the travis issue?

Travis works fine in master. The pull request breaks Travis by adding an
option that is unsupported in older version of scikit-learn. In master,
this option is purposely ignored, and a warning is raised telling the
user to update scikit-learn.
",,
10,issue_comment,150,nilearn,nilearn,GaelVaroquaux,2013-12-15 15:34:38,"Closing this guy, as I don't think that we can merge it.
",,
11,pull_request_commit,150,nilearn,nilearn,mekman,2013-12-13 16:01:09,FIX: scoring not passed on to cross_val_score resulting always in default (accuracy) scoring method,261ec19e9ffec8a87b73db93bdd51673a5b3e646,
0,rectype,issueid,project_owner,project_name,actor,time,text,action,title
1,issue_title,148,nilearn,nilearn,fabianp,2013-12-12 11:31:30,"Same example as plot_haxby_full_analysis.py, but using l1-logistic
instead. I've made two small changes to plot_haxby_full_analysis.py
to make the comparison easier to interpret: fix the vmax so that
both are in the same scale and put a title in the plot.
",start issue,"Haxby example, this time with l1-logistic"
2,issue_closed,148,nilearn,nilearn,GaelVaroquaux,2013-12-18 16:08:34,,closed issue,"Haxby example, this time with l1-logistic"
3,pull_request_title,148,nilearn,nilearn,fabianp,2013-12-12 11:31:30,"Same example as plot_haxby_full_analysis.py, but using l1-logistic
instead. I've made two small changes to plot_haxby_full_analysis.py
to make the comparison easier to interpret: fix the vmax so that
both are in the same scale and put a title in the plot.
",80460ac1d79ac48abbc67c7d1dc2f46051efe09b,"Haxby example, this time with l1-logistic"
4,pull_request_merged,148,nilearn,nilearn,GaelVaroquaux,2013-12-18 16:08:34,"Haxby example, this time with l1-logistic",83ca2ac2e0fed367b658662ae7a8f017f4e705b1,Pull request merge from fabianp/nilearn:master to nilearn/nilearn:master
5,pull_request_commit,148,nilearn,nilearn,fabianp,2013-12-12 11:28:44,"Haxby example, this time with l1-logistic

Same example as plot_haxby_full_analysis.py, but using l1-logistic
instead. I've made two small changes to plot_haxby_full_analysis.py
to make the comparison easier to interpret: fix the vmax so that
both are in the same scale and put a title in the plot.",80460ac1d79ac48abbc67c7d1dc2f46051efe09b,
