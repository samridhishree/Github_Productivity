"rectype","issueid","project_owner","project_name","actor","time","text","action","title"
"issue_title","629","nilearn","nilearn","arthurmensch","2015-07-03 09:48:44","I think it would be great to have a score method added to CanICA (and future DictLearning), that would output explained variance ratio (based on `components_` and time serie covariance)
","start issue","Add score function to CanICA"
"issue_comment","629","nilearn","nilearn","GaelVaroquaux","2015-07-03 16:11:39","Like the one that is in the parietal-python code base? 

Sent from my phone. Please forgive brevity and mis spelling

On Jul 3, 2015, 11:48, at 11:48, Arthur Mensch notifications@github.com wrote:

> I think it would be great to have a score method added to CanICA (and
> future DictLearning), that would output explained variance ratio (based
> on `components_` and time serie covariance)
> 
> ---
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/nilearn/nilearn/issues/629
","",""
"issue_comment","629","nilearn","nilearn","arthurmensch","2015-07-03 17:04:20","Yes, except it could be cleaned using sklearn.covariance
On 3 Jul 2015 18:11, ""Gael Varoquaux"" notifications@github.com wrote:

> Like the one that is in the parietal-python code base?
> 
> Sent from my phone. Please forgive brevity and mis spelling
> 
> On Jul 3, 2015, 11:48, at 11:48, Arthur Mensch notifications@github.com
> wrote:
> 
> > I think it would be great to have a score method added to CanICA (and
> > future DictLearning), that would output explained variance ratio (based
> > on `components_` and time serie covariance)
> > 
> > ---
> > 
> > Reply to this email directly or view it on GitHub:
> > https://github.com/nilearn/nilearn/issues/629
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/nilearn/nilearn/issues/629#issuecomment-118384748.
","",""
"issue_comment","629","nilearn","nilearn","banilo","2015-07-03 22:33:09","> score

Why would you call it a `score` method? Would it not be better to stick with scikit learn conventions, such as `sklearn.decomposition.PCA.explained_variance_`
","",""
"issue_comment","629","nilearn","nilearn","arthurmensch","2015-07-06 14:30:35","It could be used to score test sets decomposed over fitted maps. I was more thinking about `log_likelihood` of a Gaussian fit.

We could also provide a `explained_variance_` attribute that would hold the explained variance of the maps over the training set
","",""
"issue_comment","629","nilearn","nilearn","bthirion","2015-07-06 19:44:47","Note that ICA does not change the Gaussian likelihood of the model (it works on making it non-Gaussian, while holding mean/covariance fixed) and does not consider explained variance (unlike most DL formuations). So I don't think that it makes sense to use this criterion. Sorry if I misunderstood.
","",""
"issue_comment","629","nilearn","nilearn","arthurmensch","2015-07-07 06:30:49","This is true, doesn't it make sense to consider explained variance (and Gaussian log likelihood) for hierarchical PCA ?
","",""
"issue_comment","629","nilearn","nilearn","bthirion","2015-07-07 07:52:30","It does. Note that explained variance is easy to compute, while the Gaussian lig likelihood is not always. The explained variance remains well defined even if your data are not Gaussian distributed. 
","",""
